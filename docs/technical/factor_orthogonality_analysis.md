# Factor Orthogonality Analysis

**File:** `analysis/factor_orthogonality_analysis.py`

## Purpose
Quantitatively assess the orthogonality (independence) of scoring factors used in the early token detection pipeline. Ensures that each factor contributes unique predictive value, 
minimizing redundancy and overfitting in the gem scoring model.

## What It Does
- Loads exported factor data from `analysis_results/factor_data.csv` (generated by the detection pipeline).
- Computes the Pearson correlation matrix for all numeric scoring factors.
- Visualizes the correlation matrix as a heatmap and saves it to `analysis_results/factor_correlation_heatmap.png`.
- Identifies highly correlated factor pairs (|r| > 0.7) and summarizes them.

---

## Context & Quantitative Rationale

In systematic trading and quantitative research, **factor orthogonality** is essential for building robust, interpretable, and high-performing models. In the context of the Virtuoso early token detection pipeline, each factor (liquidity, age, concentration, price change, volume, momentum, behavioral, volatility, etc.) is designed to capture a distinct dimension of token quality or risk. If factors are highly correlated, the model may overweight certain risks or signals, leading to overfitting, spurious results, or reduced out-of-sample performance.

**Orthogonal (uncorrelated) factors** ensure that each input adds unique predictive value, maximizing the information content of the gem score and improving the reliability of alerts. This is a core principle in factor investing, risk modeling, and all forms of systematic signal construction.

---

## What This Analysis Does
- **Loads** exported factor data from `analysis_results/factor_data.csv` (generated by the detection pipeline).
- **Computes** the Pearson correlation matrix for all numeric scoring factors.
- **Visualizes** the correlation matrix as a heatmap and saves it to `analysis_results/factor_correlation_heatmap.png`.
- **Identifies** highly correlated factor pairs (|r| > 0.7) and summarizes them.
- **Sends a Telegram alert** with:
  - A summary of the orthogonality analysis (highlighting any problematic correlations).
  - The correlation heatmap image for rapid review by quants and ops.

---

## Theoretical Background
- **Why orthogonality?**
  - Redundant factors inflate model complexity and can create false confidence in backtests.
  - Orthogonal factors improve signal-to-noise ratio, reduce multicollinearity, and make model attribution clearer.
  - In production, orthogonality helps ensure that regime shifts or data errors in one factor do not unduly impact the overall score.
- **Pearson correlation** is used for linear relationships; for non-linear dependencies, consider Spearman or mutual information.

---

## Usage Instructions

### Prerequisites
- Python environment with `pandas`, `matplotlib`, `seaborn`, and all project dependencies installed.
- Factor data exported to `analysis_results/factor_data.csv` (run the early token detection pipeline first).
- Telegram bot token and chat ID set as environment variables:
  ```bash
  export TELEGRAM_BOT_TOKEN=your_bot_token
  export TELEGRAM_CHAT_ID=your_chat_id
  ```

### Manual Run
```bash
python analysis/factor_orthogonality_analysis.py
```

### Automate via Cron
Example: run daily at 7am
```bash
0 7 * * * cd /path/to/virtuoso_gem && /usr/local/bin/python3 analysis/factor_orthogonality_analysis.py
```

---

## Output
- **Telegram:**
  - Message summarizing factor orthogonality (lists any highly correlated pairs, or confirms all are orthogonal).
  - Attached heatmap image of the correlation matrix for visual inspection.
- **Filesystem:**
  - `analysis_results/factor_correlation_heatmap.png` (updated each run).

---

## Quantitative Best Practices & Interpretation
- **Checklist for Robust Factor Research:**
  - [ ] Are any factor pairs |correlation| > 0.7? If so, review for redundancy.
  - [ ] Are all factors theoretically justified and empirically validated?
  - [ ] Is the factor set stable across different market regimes?
  - [ ] Are weights recalibrated after any factor changes?
  - [ ] Is the analysis rerun after major code/data updates?
- **If any factor pairs have |correlation| > 0.7:**
  - Review these factors for redundancy or overlap in economic meaning.
  - Consider removing, transforming, or combining highly correlated factors.
  - Recalibrate scoring weights and backtest the new model.
- **If all |correlation| ‚â§ 0.7:**
  - Your factor set is robust and each factor adds unique value.
- **Repeat this analysis regularly** as your data and scoring logic evolve.
- **Document all changes** to factor definitions, weights, and analysis results for auditability.

---

## Example Telegram Alert
```
üìä <b>Factor Orthogonality Analysis</b>
‚ö†Ô∏è <b>Highly correlated factor pairs (|r| > 0.7):</b>
‚Ä¢ <b>liquidity</b> & <b>volume_24h</b>: r = 0.85
Heatmap saved: <code>analysis_results/factor_correlation_heatmap.png</code>
```
(plus the heatmap image)

---

## Maintenance & Extension
- **Rerun** after any major change to factor definitions, scoring logic, or data regime.
- **To extend:** Add new factors to the export and update the `factor_cols` list in the script.
- **For advanced analysis:** Consider adding Spearman correlation, rolling window analysis, or automated reporting to the quant/dev team.
- **Integrate** with CI/CD or workflow automation for continuous monitoring.

---

## Contact
For quant/dev questions, operational issues, or to propose improvements, contact the project maintainer or quant research lead. Maintain a log of all analysis runs and decisions for full transparency and operational rigor. 