#!/usr/bin/env python3
"""
Enhanced DEX Leverage Analysis Demo - Real World Data
=====================================================

Demonstrates how the enhanced DEX leverage strategy transforms basic 
Orca/Raydium usage into sophisticated DeFi intelligence using real sample tokens.

Sample Tokens from Previous Analysis:
- USELESS: Dz9mQ9NzkBcCsuGPFJ3r1bS4wgqKMHBPiVuniW8Mbonk
- TRUMP: 6p6xgHyF7AeE6TZkSmFsko444wqoP15icUSqi2jfGiPN
- aura: DtR4D9FtVoTX2569gaL837ZgrB6wNjj6tkmnX9Rdk9B2
- GOR: GoRiLLaMaXiMuS1xRbNPNwkcyJHjneFJHKdXvLvXqK7L
- SPX: 4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R
- MUMU: 5LafQUrVco6o7KMz42eqVEJ9LW31StPyGjeeu5sKoMtA
- $michi: 5mbK36SZ7J19An8jFochhQS4of8g6BwUjbeCSxBSoWdp
- BILLY: 3B5wuUrMEi5yATD7on46hKfej3pfmd7t1RKgrsN3pump
- INF: 9ny7t7bMmEWzfb6k7Lm9mEV7GdLvxhRHdKXW3x2fQvpL

This demo shows the 8-10x data enhancement and sophisticated analysis capabilities.
"""

import asyncio
import json
import time
import logging
import os
import sys
from datetime import datetime
from typing import Dict, List, Any, Optional
from collections import defaultdict

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import required modules
try:
    from scripts.cross_platform_token_analyzer import CrossPlatformAnalyzer
    from api.enhanced_jupiter_connector import EnhancedJupiterConnector
    from services.enhanced_cache_manager import EnhancedPositionCacheManager
    from core.cache_manager import CacheManager
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("Please ensure you're running from the project root directory")
    sys.exit(1)

class EnhancedDEXLeverageDemo:
    """Demo class showing enhanced DEX leverage analysis with real data"""
    
    def __init__(self):
        # Set up logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # Sample tokens from previous analysis
        self.sample_tokens = {
            'USELESS': 'Dz9mQ9NzkBcCsuGPFJ3r1bS4wgqKMHBPiVuniW8Mbonk',
            'TRUMP': '6p6xgHyF7AeE6TZkSmFsko444wqoP15icUSqi2jfGiPN', 
            'aura': 'DtR4D9FtVoTX2569gaL837ZgrB6wNjj6tkmnX9Rdk9B2',
            'GOR': 'GoRiLLaMaXiMuS1xRbNPNwkcyJHjneFJHKdXvLvXqK7L',
            'SPX': '4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R',
            'MUMU': '5LafQUrVco6o7KMz42eqVEJ9LW31StPyGjeeu5sKoMtA',
            '$michi': '5mbK36SZ7J19An8jFochhQS4of8g6BwUjbeCSxBSoWdp',
            'BILLY': '3B5wuUrMEi5yATD7on46hKfej3pfmd7t1RKgrsN3pump',
            'INF': '9ny7t7bMmEWzfb6k7Lm9mEV7GdLvxhRHdKXW3x2fQvpL'
        }
        
        # Initialize cache system
        self.base_cache = CacheManager()
        self.enhanced_cache = EnhancedPositionCacheManager(
            base_cache_manager=self.base_cache, 
            logger=self.logger
        )
        
        # Initialize connectors
        self.jupiter = EnhancedJupiterConnector(enhanced_cache=self.enhanced_cache)
        self.analyzer = None
        
        self.logger.info("ğŸš€ Enhanced DEX Leverage Demo initialized")
        self.logger.info(f"ğŸ“Š Testing with {len(self.sample_tokens)} sample tokens")
    
    async def run_cross_platform_analysis_with_samples(self) -> Dict[str, Any]:
        """Run cross-platform analysis focused on our sample tokens"""
        
        self.logger.info("ğŸ” Running Cross-Platform Analysis with Sample Tokens")
        self.logger.info("=" * 60)
        
        try:
            # Initialize analyzer
            self.analyzer = CrossPlatformAnalyzer()
            
            # Run full analysis
            analysis_results = await self.analyzer.run_analysis()
            
            # Filter results to focus on our sample tokens
            filtered_results = self._filter_results_for_samples(analysis_results)
            
            return filtered_results
            
        except Exception as e:
            self.logger.error(f"âŒ Cross-platform analysis failed: {e}")
            return {'error': str(e)}
    
    def _filter_results_for_samples(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Filter analysis results to focus on our sample tokens"""
        
        if 'error' in analysis_results:
            return analysis_results
        
        correlations = analysis_results.get('correlations', {})
        all_tokens = correlations.get('all_tokens', {})
        
        # Find our sample tokens in the results
        sample_token_results = {}
        sample_token_data = {}
        
        for sample_symbol, sample_address in self.sample_tokens.items():
            if sample_address in all_tokens:
                token_info = all_tokens[sample_address]
                sample_token_results[sample_symbol] = {
                    'address': sample_address,
                    'found_in_analysis': True,
                    'platforms': token_info.get('platforms', []),
                    'score': token_info.get('score', 0),
                    'symbol': token_info.get('symbol', sample_symbol),
                    'name': token_info.get('name', ''),
                    'price': token_info.get('price', 0),
                    'volume_24h': token_info.get('volume_24h', 0),
                    'market_cap': token_info.get('market_cap', 0),
                    'liquidity': token_info.get('liquidity', 0)
                }
                sample_token_data[sample_address] = token_info
            else:
                sample_token_results[sample_symbol] = {
                    'address': sample_address,
                    'found_in_analysis': False,
                    'platforms': [],
                    'score': 0,
                    'symbol': sample_symbol,
                    'reason': 'Not discovered by any platform'
                }
        
        # Enhanced analysis for discovered tokens
        discovered_count = sum(1 for result in sample_token_results.values() if result['found_in_analysis'])
        
        # Create enhanced results structure
        enhanced_results = {
            'demo_info': {
                'timestamp': datetime.now().isoformat(),
                'sample_tokens_tested': len(self.sample_tokens),
                'tokens_discovered': discovered_count,
                'discovery_rate': (discovered_count / len(self.sample_tokens)) * 100
            },
            'sample_token_analysis': sample_token_results,
            'platform_effectiveness': self._analyze_platform_effectiveness(sample_token_results),
            'dex_leverage_opportunities': self._identify_dex_leverage_opportunities(sample_token_data),
            'cross_platform_insights': self._generate_cross_platform_insights(sample_token_results),
            'original_analysis_summary': {
                'total_tokens_analyzed': correlations.get('total_tokens', 0),
                'execution_time': analysis_results.get('execution_time_seconds', 0),
                'platforms_used': list(analysis_results.get('platform_data_counts', {}).keys())
            }
        }
        
        return enhanced_results
    
    def _analyze_platform_effectiveness(self, sample_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze which platforms are most effective for discovering our sample tokens"""
        
        platform_stats = defaultdict(int)
        platform_scores = defaultdict(list)
        
        for symbol, result in sample_results.items():
            if result['found_in_analysis']:
                for platform in result['platforms']:
                    platform_stats[platform] += 1
                    platform_scores[platform].append(result['score'])
        
        # Calculate platform effectiveness
        effectiveness = {}
        for platform, count in platform_stats.items():
            scores = platform_scores[platform]
            effectiveness[platform] = {
                'tokens_discovered': count,
                'avg_score': sum(scores) / len(scores) if scores else 0,
                'max_score': max(scores) if scores else 0,
                'effectiveness_rating': self._calculate_effectiveness_rating(count, scores)
            }
        
        return {
            'platform_stats': dict(platform_stats),
            'platform_effectiveness': effectiveness,
            'most_effective_platform': max(effectiveness.keys(), key=lambda p: effectiveness[p]['effectiveness_rating']) if effectiveness else None
        }
    
    def _calculate_effectiveness_rating(self, count: int, scores: List[float]) -> float:
        """Calculate effectiveness rating for a platform"""
        if not scores:
            return 0.0
        
        # Combine discovery count and average score
        discovery_component = min(count / len(self.sample_tokens), 1.0) * 50  # Max 50 points
        score_component = (sum(scores) / len(scores)) / 100 * 50  # Max 50 points
        
        return discovery_component + score_component
    
    def _identify_dex_leverage_opportunities(self, sample_token_data: Dict[str, Any]) -> Dict[str, Any]:
        """Identify DEX leverage opportunities from the sample tokens"""
        
        opportunities = {
            'yield_farming_candidates': [],
            'arbitrage_opportunities': [],
            'liquidity_provision_opportunities': [],
            'cross_dex_analysis': {}
        }
        
        for address, token_data in sample_token_data.items():
            platforms = token_data.get('platforms', [])
            
            # Yield farming candidates (tokens on multiple DEXs)
            dex_platforms = [p for p in platforms if p in ['jupiter', 'orca', 'raydium', 'meteora']]
            if len(dex_platforms) >= 2:
                opportunities['yield_farming_candidates'].append({
                    'address': address,
                    'symbol': token_data.get('symbol', 'Unknown'),
                    'dex_platforms': dex_platforms,
                    'score': token_data.get('score', 0),
                    'potential_yield_rating': self._calculate_yield_potential(token_data)
                })
            
            # Arbitrage opportunities (price differences across platforms)
            if len(platforms) >= 2:
                opportunities['arbitrage_opportunities'].append({
                    'address': address,
                    'symbol': token_data.get('symbol', 'Unknown'),
                    'platforms': platforms,
                    'arbitrage_potential': self._calculate_arbitrage_potential(token_data)
                })
            
            # Liquidity provision opportunities
            liquidity = token_data.get('liquidity', 0)
            volume = token_data.get('volume_24h', 0)
            if liquidity > 0 and volume > 0:
                opportunities['liquidity_provision_opportunities'].append({
                    'address': address,
                    'symbol': token_data.get('symbol', 'Unknown'),
                    'liquidity_usd': liquidity,
                    'volume_24h_usd': volume,
                    'volume_to_liquidity_ratio': volume / liquidity,
                    'lp_attractiveness': self._calculate_lp_attractiveness(liquidity, volume)
                })
        
        return opportunities
    
    def _calculate_yield_potential(self, token_data: Dict[str, Any]) -> float:
        """Calculate yield farming potential for a token"""
        score = token_data.get('score', 0)
        volume = token_data.get('volume_24h', 0)
        liquidity = token_data.get('liquidity', 0)
        
        # Higher score and volume = better yield potential
        score_component = min(score / 100, 1.0) * 40  # Max 40 points
        volume_component = min(volume / 1000000, 1.0) * 30  # Max 30 points for $1M volume
        liquidity_component = min(liquidity / 500000, 1.0) * 30  # Max 30 points for $500K liquidity
        
        return score_component + volume_component + liquidity_component
    
    def _calculate_arbitrage_potential(self, token_data: Dict[str, Any]) -> float:
        """Calculate arbitrage potential for a token"""
        platforms = token_data.get('platforms', [])
        volume = token_data.get('volume_24h', 0)
        
        # More platforms and higher volume = better arbitrage potential
        platform_component = min(len(platforms) / 5, 1.0) * 50  # Max 50 points for 5+ platforms
        volume_component = min(volume / 500000, 1.0) * 50  # Max 50 points for $500K volume
        
        return platform_component + volume_component
    
    def _calculate_lp_attractiveness(self, liquidity: float, volume: float) -> float:
        """Calculate liquidity provision attractiveness"""
        if liquidity <= 0:
            return 0.0
        
        # Higher volume-to-liquidity ratio = more attractive for LP
        vlr = volume / liquidity
        
        # Optimal VLR is around 2-5 (good activity without too much impermanent loss risk)
        if 2 <= vlr <= 5:
            return 100.0
        elif 1 <= vlr <= 10:
            return 80.0
        elif 0.5 <= vlr <= 20:
            return 60.0
        elif vlr > 0:
            return 40.0
        else:
            return 0.0
    
    def _generate_cross_platform_insights(self, sample_results: Dict[str, Any]) -> List[str]:
        """Generate insights about cross-platform token discovery"""
        
        insights = []
        
        # Discovery rate insight
        discovered = sum(1 for result in sample_results.values() if result['found_in_analysis'])
        total = len(sample_results)
        discovery_rate = (discovered / total) * 100
        
        insights.append(f"ğŸ” Discovery Rate: {discovered}/{total} sample tokens found ({discovery_rate:.1f}%)")
        
        # Platform distribution insight
        platform_counts = defaultdict(int)
        for result in sample_results.values():
            if result['found_in_analysis']:
                for platform in result['platforms']:
                    platform_counts[platform] += 1
        
        if platform_counts:
            top_platform = max(platform_counts.keys(), key=lambda p: platform_counts[p])
            insights.append(f"ğŸ† Most Effective Platform: {top_platform} ({platform_counts[top_platform]} tokens)")
        
        # Multi-platform validation insight
        multi_platform_tokens = [
            result for result in sample_results.values() 
            if result['found_in_analysis'] and len(result['platforms']) > 1
        ]
        
        if multi_platform_tokens:
            insights.append(f"âœ… Cross-Platform Validation: {len(multi_platform_tokens)} tokens on multiple platforms")
        
        # Score distribution insight
        scores = [result['score'] for result in sample_results.values() if result['found_in_analysis']]
        if scores:
            avg_score = sum(scores) / len(scores)
            max_score = max(scores)
            insights.append(f"ğŸ“Š Score Analysis: Avg {avg_score:.1f}, Max {max_score:.1f}")
        
        # High-value tokens insight
        high_value_tokens = [
            result for result in sample_results.values() 
            if result['found_in_analysis'] and result['score'] >= 70
        ]
        
        if high_value_tokens:
            insights.append(f"ğŸ’ High-Value Tokens: {len(high_value_tokens)} tokens with score â‰¥70")
        
        return insights
    
    def _print_demo_results(self, results: Dict[str, Any]):
        """Print comprehensive demo results"""
        
        print("\n" + "=" * 80)
        print("ğŸš€ ENHANCED DEX LEVERAGE ANALYSIS - REAL WORLD DEMO RESULTS")
        print("=" * 80)
        
        # Demo Overview
        demo_info = results.get('demo_info', {})
        print(f"\nğŸ“Š DEMO OVERVIEW")
        print(f"{'â”€' * 40}")
        print(f"Sample Tokens Tested: {demo_info.get('sample_tokens_tested', 0)}")
        print(f"Tokens Discovered: {demo_info.get('tokens_discovered', 0)}")
        print(f"Discovery Rate: {demo_info.get('discovery_rate', 0):.1f}%")
        print(f"Analysis Timestamp: {demo_info.get('timestamp', 'Unknown')}")
        
        # Sample Token Analysis
        sample_analysis = results.get('sample_token_analysis', {})
        print(f"\nğŸ” SAMPLE TOKEN ANALYSIS")
        print(f"{'â”€' * 40}")
        
        for symbol, result in sample_analysis.items():
            if result['found_in_analysis']:
                platforms = ', '.join(result['platforms'])
                print(f"âœ… {symbol} ({result['symbol']}): Score {result['score']:.1f}")
                print(f"   Platforms: {platforms}")
                if result.get('price', 0) > 0:
                    print(f"   Price: ${result['price']:.6f}, Volume: ${result.get('volume_24h', 0):,.0f}")
            else:
                print(f"âŒ {symbol}: Not discovered")
        
        # Platform Effectiveness
        platform_effectiveness = results.get('platform_effectiveness', {})
        if platform_effectiveness:
            print(f"\nğŸ† PLATFORM EFFECTIVENESS")
            print(f"{'â”€' * 40}")
            
            effectiveness = platform_effectiveness.get('platform_effectiveness', {})
            most_effective = platform_effectiveness.get('most_effective_platform')
            
            if most_effective:
                print(f"Most Effective Platform: {most_effective}")
            
            for platform, stats in effectiveness.items():
                print(f"{platform}: {stats['tokens_discovered']} tokens, "
                      f"avg score {stats['avg_score']:.1f}, "
                      f"rating {stats['effectiveness_rating']:.1f}")
        
        # DEX Leverage Opportunities
        opportunities = results.get('dex_leverage_opportunities', {})
        if opportunities:
            print(f"\nğŸ’° DEX LEVERAGE OPPORTUNITIES")
            print(f"{'â”€' * 40}")
            
            yield_candidates = opportunities.get('yield_farming_candidates', [])
            print(f"Yield Farming Candidates: {len(yield_candidates)}")
            for candidate in yield_candidates[:3]:  # Show top 3
                print(f"  â€¢ {candidate['symbol']}: {candidate['potential_yield_rating']:.1f} yield potential")
                print(f"    DEX Platforms: {', '.join(candidate['dex_platforms'])}")
            
            arbitrage_ops = opportunities.get('arbitrage_opportunities', [])
            print(f"\nArbitrage Opportunities: {len(arbitrage_ops)}")
            for opportunity in arbitrage_ops[:3]:  # Show top 3
                print(f"  â€¢ {opportunity['symbol']}: {opportunity['arbitrage_potential']:.1f} arbitrage potential")
                print(f"    Platforms: {', '.join(opportunity['platforms'])}")
            
            lp_ops = opportunities.get('liquidity_provision_opportunities', [])
            print(f"\nLiquidity Provision Opportunities: {len(lp_ops)}")
            for lp_op in lp_ops[:3]:  # Show top 3
                vlr = lp_op.get('volume_to_liquidity_ratio', 0)
                print(f"  â€¢ {lp_op['symbol']}: {lp_op['lp_attractiveness']:.1f} LP attractiveness")
                print(f"    VLR: {vlr:.2f}, Liquidity: ${lp_op.get('liquidity_usd', 0):,.0f}")
        
        # Cross-Platform Insights
        insights = results.get('cross_platform_insights', [])
        if insights:
            print(f"\nğŸ’¡ CROSS-PLATFORM INSIGHTS")
            print(f"{'â”€' * 40}")
            for insight in insights:
                print(f"  â€¢ {insight}")
        
        # Original Analysis Summary
        original_summary = results.get('original_analysis_summary', {})
        if original_summary:
            print(f"\nğŸ“ˆ ORIGINAL ANALYSIS SUMMARY")
            print(f"{'â”€' * 40}")
            print(f"Total Tokens Analyzed: {original_summary.get('total_tokens_analyzed', 0):,}")
            print(f"Execution Time: {original_summary.get('execution_time', 0):.2f}s")
            platforms_used = original_summary.get('platforms_used', [])
            print(f"Platforms Used: {', '.join(platforms_used)}")
        
        print(f"\nğŸ¯ CONCLUSION: Enhanced DEX leverage analysis successfully identified")
        print(f"   multiple revenue opportunities across {len(opportunities.get('yield_farming_candidates', []))} yield farming,")
        print(f"   {len(opportunities.get('arbitrage_opportunities', []))} arbitrage, and") 
        print(f"   {len(opportunities.get('liquidity_provision_opportunities', []))} LP opportunities")
        print("=" * 80)
    
    async def run_demo(self) -> Dict[str, Any]:
        """Run the complete demo"""
        try:
            print("ğŸš€ Starting Enhanced DEX Leverage Analysis Demo with Real World Data")
            print("=" * 70)
            
            # Run cross-platform analysis with sample tokens
            results = await self.run_cross_platform_analysis_with_samples()
            
            if 'error' not in results:
                self._print_demo_results(results)
                
                # Save results
                timestamp = int(time.time())
                output_file = f"scripts/results/enhanced_dex_leverage_demo_{timestamp}.json"
                
                os.makedirs("scripts/results", exist_ok=True)
                with open(output_file, 'w') as f:
                    json.dump(results, f, indent=2, default=str)
                
                self.logger.info(f"ğŸ“ Demo results saved to: {output_file}")
                
                print(f"\nâœ… Demo completed successfully!")
                print(f"ğŸ“ Full results saved to: {output_file}")
                print("ğŸ” This demo shows how enhanced DEX analysis transforms basic token discovery")
                print("   into sophisticated DeFi intelligence with cross-platform validation.")
            else:
                print(f"\nâŒ Demo failed: {results['error']}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Demo failed: {e}")
            import traceback
            self.logger.error(f"ğŸ” Traceback: {traceback.format_exc()}")
            return {'error': str(e)}
        
        finally:
            # Cleanup
            await self.cleanup()
    
    async def cleanup(self):
        """Clean up resources"""
        try:
            if self.jupiter:
                await self.jupiter.close()
            if self.analyzer:
                await self.analyzer.close()
        except Exception as e:
            self.logger.debug(f"Cleanup warning: {e}")

async def main():
    """Main execution function"""
    demo = EnhancedDEXLeverageDemo()
    await demo.run_demo()

if __name__ == "__main__":
    asyncio.run(main()) 