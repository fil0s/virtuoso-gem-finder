#!/usr/bin/env python3
"""
High Conviction Token Detector with Enhanced Reporting

This script runs cross-platform analysis to identify high-conviction tokens,
then performs detailed Birdeye analysis for comprehensive alerts.

Enhanced Features:
- Cross-platform analysis for cost-effective initial filtering
- Detailed Birdeye analysis for high-conviction tokens
- Whale and holder analysis
- Community and boosting information
- Duplicate alert prevention
- Comprehensive Telegram alerts
- Optimized API usage to minimize costs
- Enhanced API usage tracking and cost analysis
- Performance monitoring and bottleneck identification
- Comprehensive session reporting
- Error pattern analysis and recovery metrics
- Formatted table outputs using prettytable
"""

import asyncio
import json
import os
import sys
import time
import psutil
from datetime import datetime, timedelta
from typing import Dict, List, Set, Optional, Any
from pathlib import Path
from collections import defaultdict
import logging
import argparse

# Add prettytable for formatted output
try:
    from prettytable import PrettyTable
except ImportError:
    print("❌ prettytable not found. Installing...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "prettytable"])
    from prettytable import PrettyTable

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from scripts.cross_platform_token_analyzer import CrossPlatformAnalyzer
    from services.telegram_alerter import TelegramAlerter, MinimalTokenMetrics
    from api.birdeye_connector import BirdeyeAPI
    from core.cache_manager import CacheManager
    from services.rate_limiter_service import RateLimiterService
    from utils.logger_setup import LoggerSetup
    from core.config_manager import ConfigManager
except ImportError as e:
    print(f"❌ Import error: {e}")
    print("💡 Make sure you're running from the project root directory")
    print("💡 Or set PYTHONPATH: export PYTHONPATH=$PWD:$PYTHONPATH")
    sys.exit(1)

class HighConvictionTokenDetector:
    """
    High-conviction token detector that combines cross-platform analysis
    with detailed Birdeye analysis for comprehensive token evaluation.
    Enhanced with comprehensive reporting and monitoring capabilities.
    """
    
    def __init__(self, config_path: str = "config/config.yaml", debug_mode: bool = False):
        """Initialize the High Conviction Token Detector with enhanced reporting capabilities"""
        self.debug_mode = debug_mode
        self.compact_mode = False  # New: Enable compact formatting
        self.use_colors = True     # New: Enable ANSI color output
        
        # Session tracking
        self.session_id = f"hc_detector_{int(time.time())}"
        self.session_start_time = datetime.now()
        self.cycle_count = 0
        self.successful_cycles = 0
        
        self.config_manager = ConfigManager(config_path)
        self.config = self.config_manager.get_config()
        
        # Initialize logging
        self.logger_setup = LoggerSetup("HighConvictionDetector")
        self.logger = self.logger_setup.logger
        
        # Enhanced debug mode - check config setting
        config_debug_mode = self.config.get('DEVELOPMENT', {}).get('debug_mode', False)
        self.effective_debug_mode = debug_mode or config_debug_mode  # Track effective debug mode
        if self.effective_debug_mode:  # Enable if requested via CLI or config
            self._setup_debug_logging()
        
        # Initialize core services
        self.cache_manager = CacheManager()
        self.rate_limiter = RateLimiterService()
        
        # Initialize APIs
        self._init_apis()
        
        # Initialize Telegram alerter
        self._init_telegram()
        
        # State management
        self.alerted_tokens_file = "data/alerted_tokens.json"
        self.alerted_tokens: Set[str] = self._load_alerted_tokens()
        
        # Configuration - Updated for 0-100 scale
        scoring_config = self.config.get('SCORING', {})
        self.high_conviction_threshold = scoring_config.get('high_conviction_threshold', 70.0)
        self.alert_threshold = scoring_config.get('alert_threshold', 35.0)
        
        # FIX: Read min_candidate_score from root level, not nested path
        self.min_cross_platform_score = scoring_config.get('min_candidate_score', 30.0)
        
        self.logger.info(f"📊 Threshold Configuration Loaded:")
        self.logger.info(f"  • Min Candidate Score: {self.min_cross_platform_score}")
        self.logger.info(f"  • High Conviction Threshold: {self.high_conviction_threshold}")
        self.logger.info(f"  • Alert Threshold: {self.alert_threshold}")
        
        # Enhanced session statistics and reporting
        self.process = psutil.Process()
        
        # ANSI Color codes for optimized formatting
        self.colors = {
            'RED': '\033[91m',
            'GREEN': '\033[92m',
            'YELLOW': '\033[93m',
            'BLUE': '\033[94m',
            'MAGENTA': '\033[95m',
            'CYAN': '\033[96m',
            'WHITE': '\033[97m',
            'BOLD': '\033[1m',
            'RESET': '\033[0m'
        } if self.use_colors else {k: '' for k in ['RED', 'GREEN', 'YELLOW', 'BLUE', 'MAGENTA', 'CYAN', 'WHITE', 'BOLD', 'RESET']}
        
        # Enhanced Token Registry System
        self.session_token_registry = {
            "all_tokens_by_scan": {},  # scan_number -> list of tokens
            "unique_tokens_discovered": {},  # address -> token details
            "token_sources": {},  # address -> list of sources (rugcheck, dexscreener, birdeye)
            "token_scores": {},  # address -> score progression over time
            "high_conviction_tokens": {},  # address -> detailed analysis
            "cross_platform_validated_tokens": {},  # address -> multi-platform token data
            "session_summary": {
                "total_unique_tokens": 0,
                "tokens_by_source": {
                    "rugcheck": 0,
                    "dexscreener": 0,
                    "birdeye": 0
                },
                "score_distribution": {
                    "0-2": 0,
                    "2-4": 0,
                    "4-6": 0,
                    "6-8": 0,
                    "8-10": 0
                },
                "multi_platform_tokens": 0,
                "score_progression_analysis": {}
            }
        }
        
        # Comprehensive session statistics
        self.session_stats = {
            'start_time': self.session_start_time.isoformat(),
            'session_id': self.session_id,
            'debug_mode': self.debug_mode,
            'detector_config': {
                'high_conviction_threshold': self.high_conviction_threshold,
                'min_cross_platform_score': self.min_cross_platform_score
            },
            'detection_cycles': [],
            'tokens_discovered': {},
            
            # Enhanced Service-Based API tracking
            'api_usage_by_service': {
                'rugcheck': {
                    'service_name': 'RugCheck API',
                    'total_calls': 0,
                    'successful_calls': 0,
                    'failed_calls': 0,
                    'total_response_time_ms': 0,
                    'avg_response_time_ms': 0,
                    'endpoints': ['RugCheck Trending API'],
                    'endpoint_stats': defaultdict(lambda: {
                        'calls': 0, 'successes': 0, 'failures': 0, 
                        'total_time_ms': 0, 'avg_time_ms': 0
                    }),
                    'estimated_cost_usd': 0.0,
                    'health_status': 'unknown',
                    'last_error': None,
                    'consecutive_failures': 0
                },
                'dexscreener': {
                    'service_name': 'DexScreener API',
                    'total_calls': 0,
                    'successful_calls': 0,
                    'failed_calls': 0,
                    'total_response_time_ms': 0,
                    'avg_response_time_ms': 0,
                    'endpoints': ['DexScreener Boosted API', 'DexScreener Top Boosted API'],
                    'endpoint_stats': defaultdict(lambda: {
                        'calls': 0, 'successes': 0, 'failures': 0,
                        'total_time_ms': 0, 'avg_time_ms': 0
                    }),
                    'estimated_cost_usd': 0.0,
                    'health_status': 'unknown',
                    'last_error': None,
                    'consecutive_failures': 0
                },
                'birdeye': {
                    'service_name': 'Birdeye API',
                    'total_calls': 0,
                    'successful_calls': 0,
                    'failed_calls': 0,
                    'total_response_time_ms': 0,
                    'avg_response_time_ms': 0,
                    'endpoints': [],
                    'endpoint_stats': defaultdict(lambda: {
                        'calls': 0, 'successes': 0, 'failures': 0, 
                        'total_time_ms': 0, 'avg_time_ms': 0
                    }),
                    'rate_limit_hits': 0,
                    'estimated_cost_usd': 0.0,
                    'health_status': 'unknown',
                    'last_error': None,
                    'consecutive_failures': 0
                }
            },
            
            # Enhanced Debug Information
            'debug_analysis': {
                'api_errors_detected': 0,
                'none_type_errors_detected': 0,
                'address_filtering_events': 0,
                'successful_api_calls': 0,
                'error_patterns': [],
                'recovery_events': [],
                'performance_warnings': []
            },
            
            # Real-Time Health Monitoring
            'health_monitoring': {
                'overall_health_status': 'unknown',
                'service_health_scores': {},
                'performance_alerts': [],
                'optimization_opportunities': [],
                'system_stability_score': 0.0,
                'api_reliability_score': 0.0
            },
            
            # Cost analysis and optimization
            'cost_analysis': {
                'total_estimated_cost_usd': 0.0,
                'cost_per_cycle_avg': 0.0,
                'cost_per_token_discovered': 0.0,
                'cost_per_high_conviction_token': 0.0,
                'cost_breakdown_by_service': {
                    'rugcheck': 0.0,
                    'dexscreener': 0.0,
                    'birdeye_cross_platform': 0.0,
                    'birdeye_detailed_analysis': 0.0,
                    'birdeye_whale_analysis': 0.0,
                    'birdeye_volume_analysis': 0.0,
                    'birdeye_security_analysis': 0.0,
                    'birdeye_community_analysis': 0.0
                },
                'optimization_recommendations': []
            },
            
            # Performance bottleneck identification
            'performance_analysis': {
                'avg_cycle_duration': 0,
                'pipeline_stage_durations': {
                    'cross_platform_analysis_ms': [],
                    'detailed_analysis_ms': [],
                    'whale_analysis_ms': [],
                    'volume_analysis_ms': [],
                    'security_analysis_ms': [],
                    'community_analysis_ms': [],
                    'scoring_calculation_ms': [],
                    'alert_generation_ms': []
                },
                'bottlenecks_identified': [],
                'system_resource_usage': {
                    'peak_memory_mb': 0,
                    'avg_cpu_percent': 0,
                    'disk_io_mb': 0
                },
                'slowest_cycles': [],
                'fastest_cycles': []
            },
            
            # Enhanced error pattern analysis
            'error_analysis': {
                'total_errors': 0,
                'errors_by_service': defaultdict(int),
                'errors_by_endpoint': defaultdict(int),
                'errors_by_type': defaultdict(int),
                'error_patterns': [],
                'recovery_success_rate': 0.0,
                'consecutive_failures': 0,
                'max_consecutive_failures': 0,
                'error_timeline': [],
                'critical_errors': [],
                'warning_errors': []
            },
            
            # Enhanced performance metrics
            'performance_metrics': {
                'total_cycles': 0,
                'successful_cycles': 0,
                'avg_cycle_duration': 0,
                'total_tokens_found': 0,
                'unique_tokens': 0,
                'high_conviction_tokens': 0,
                'cross_platform_validated_tokens': 0,
                'tokens_per_hour': 0,
                'high_conviction_rate': 0.0,
                'cycle_success_rate': 0.0,
                'api_efficiency_score': 0.0,
                'total_alerts_sent': 0,
                'token_discovery_rate': 0.0,
                'score_improvement_rate': 0.0
            },
            
            # Detailed token analysis preservation
            'detailed_token_analyses': {}
        }
        
        # Ensure data directory exists
        os.makedirs("data", exist_ok=True)
        
        self.logger.info("🚀 High Conviction Token Detector with Enhanced Monitoring initialized")
        self.logger.info(f"📊 High conviction threshold: {self.high_conviction_threshold}")
        self.logger.info(f"🔍 Cross-platform minimum score: {self.min_cross_platform_score}")
        self.logger.info(f"🆔 Session ID: {self.session_id}")
        if self.is_debug_enabled():
            self.logger.info("🐛 Enhanced debug mode enabled")
    
    def is_debug_enabled(self) -> bool:
        """Check if debug mode is enabled via CLI argument or config file"""
        config_debug_mode = self.config.get('DEVELOPMENT', {}).get('debug_mode', False)
        return self.debug_mode or config_debug_mode
    
    def _setup_debug_logging(self):
        """Setup enhanced debug logging with pattern recognition"""
        # Get the root logger and set to DEBUG level
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.DEBUG)
        
        # Create debug handler if not exists
        debug_handler = None
        for handler in root_logger.handlers:
            if hasattr(handler, 'baseFilename') and 'debug' in str(handler.baseFilename):
                debug_handler = handler
                break
                
        if not debug_handler:
            debug_handler = logging.FileHandler('debug_session.log')
            debug_handler.setLevel(logging.DEBUG)
            formatter = logging.Formatter(
                '%(asctime)s [%(levelname)s] %(name)s - %(message)s'
            )
            debug_handler.setFormatter(formatter)
            root_logger.addHandler(debug_handler)
            
        self.logger.info("🐛 ENHANCED DEBUG MODE ENABLED - Comprehensive API data logging active")
        self.logger.info("📝 Debug logs will be saved to: debug_session.log")
        self.logger.info("🔍 API responses will be logged in full detail for investigation")
        
    def _init_apis(self):
        """Initialize API connections"""
        try:
            # Initialize cross-platform analyzer
            birdeye_api_key = os.getenv('BIRDEYE_API_KEY')
            if not birdeye_api_key:
                self.logger.warning("⚠️ BIRDEYE_API_KEY not found in environment")
                
            self.cross_platform_analyzer = CrossPlatformAnalyzer(
                config=self.config,
                logger=self.logger
            )
            
            # Initialize detailed Birdeye API for enhanced analysis
            if birdeye_api_key:
                birdeye_config = self.config.get('BIRDEYE_API', {})
                birdeye_config['api_key'] = birdeye_api_key
                
                self.birdeye_api = BirdeyeAPI(
                    config=birdeye_config,
                    logger=self.logger,
                    cache_manager=self.cache_manager,
                    rate_limiter=self.rate_limiter
                )
                self.logger.info("✅ Birdeye API initialized for detailed analysis")
            else:
                self.birdeye_api = None
                self.logger.warning("⚠️ Birdeye API not available for detailed analysis")
                
        except Exception as e:
            self.logger.error(f"❌ Error initializing APIs: {e}")
            raise
            
    def _init_telegram(self):
        """Initialize Telegram alerter"""
        try:
            telegram_config = self.config.get('TELEGRAM', {})
            
            if not telegram_config.get('enabled', False):
                self.logger.warning("⚠️ Telegram alerts disabled in configuration")
                self.telegram_alerter = None
                return
                
            bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
            chat_id = os.getenv('TELEGRAM_CHAT_ID')
            
            if not bot_token or not chat_id:
                self.logger.warning("⚠️ Telegram credentials not found in environment")
                self.telegram_alerter = None
                return
                
            self.telegram_alerter = TelegramAlerter(
                bot_token=bot_token,
                chat_id=chat_id,
                config=telegram_config,
                logger_setup=self.logger_setup
            )
            
            self.logger.info("✅ Telegram alerter initialized")
            
        except Exception as e:
            self.logger.error(f"❌ Error initializing Telegram: {e}")
            self.telegram_alerter = None
            
    def _load_alerted_tokens(self) -> Set[str]:
        """Load previously alerted tokens to avoid duplicates"""
        try:
            if os.path.exists(self.alerted_tokens_file):
                with open(self.alerted_tokens_file, 'r') as f:
                    data = json.load(f)
                    # Clean up old entries (older than 7 days)
                    cutoff_time = time.time() - (7 * 24 * 60 * 60)
                    cleaned_data = {
                        token: timestamp for token, timestamp in data.items()
                        if timestamp > cutoff_time
                    }
                    # Save cleaned data back
                    with open(self.alerted_tokens_file, 'w') as f:
                        json.dump(cleaned_data, f, indent=2)
                    
                    tokens = set(cleaned_data.keys())
                    self.logger.info(f"📋 Loaded {len(tokens)} previously alerted tokens")
                    return tokens
        except Exception as e:
            self.logger.error(f"❌ Error loading alerted tokens: {e}")
            
        return set()
        
    def _save_alerted_tokens(self):
        """Save alerted tokens to file"""
        try:
            # Convert set to dict with timestamps
            data = {}
            current_time = time.time()
            
            # Load existing data to preserve timestamps
            if os.path.exists(self.alerted_tokens_file):
                try:
                    with open(self.alerted_tokens_file, 'r') as f:
                        existing_data = json.load(f)
                        data.update(existing_data)
                except:
                    pass
                    
            # Add new tokens with current timestamp
            for token in self.alerted_tokens:
                if token not in data:
                    data[token] = current_time
                    
            with open(self.alerted_tokens_file, 'w') as f:
                json.dump(data, f, indent=2)
                
        except Exception as e:
            self.logger.error(f"❌ Error saving alerted tokens: {e}")
            
    # ==================== ENHANCED REPORTING METHODS ====================
    
    def _capture_api_usage_stats(self):
        """Capture API usage statistics from detector components"""
        try:
            # Capture Birdeye API stats
            if hasattr(self, 'birdeye_api') and self.birdeye_api:
                birdeye_stats = self._get_birdeye_api_stats()
                self._update_api_stats('birdeye', birdeye_stats)
                
            # Capture cross-platform analyzer stats
            if hasattr(self, 'cross_platform_analyzer'):
                cross_platform_stats = self._get_cross_platform_api_stats()
                self._update_api_stats('dexscreener', cross_platform_stats.get('dexscreener', {}))
                self._update_api_stats('rugcheck', cross_platform_stats.get('rugcheck', {}))
                
        except Exception as e:
            self.logger.error(f"❌ Error capturing API usage stats: {e}")
            self._record_error('api_stats_capture', str(e), 'system')
            
    def _get_birdeye_api_stats(self) -> Dict[str, Any]:
        """Extract Birdeye API usage statistics from the actual API tracker"""
        stats = {
            'calls': 0,
            'successes': 0,
            'failures': 0,
            'total_time_ms': 0,
            'endpoints': {},
            'estimated_cost': 0.0
        }
        
        try:
            # Get stats directly from the Birdeye API's comprehensive statistics tracker
            if hasattr(self, 'birdeye_api') and self.birdeye_api:
                birdeye_stats = self.birdeye_api.get_api_call_statistics()
                
                # Map the comprehensive stats to our expected format
                stats['calls'] = birdeye_stats.get('total_api_calls', 0)
                stats['successes'] = birdeye_stats.get('successful_api_calls', 0)
                stats['failures'] = birdeye_stats.get('failed_api_calls', 0)
                stats['total_time_ms'] = birdeye_stats.get('total_response_time_ms', 0)
                
                # Extract endpoint breakdown
                top_endpoints = birdeye_stats.get('top_endpoints', [])
                for endpoint_data in top_endpoints:
                    endpoint_name = endpoint_data.get('endpoint', 'unknown')
                    stats['endpoints'][endpoint_name] = {
                        'calls': endpoint_data.get('total_calls', 0),
                        'successes': endpoint_data.get('successful_calls', 0),
                        'failures': endpoint_data.get('failed_calls', 0),
                        'avg_response_time_ms': endpoint_data.get('avg_response_time_ms', 0)
                    }
                
                # Get cost tracking data if available
                cost_tracking = birdeye_stats.get('cost_tracking', {})
                if cost_tracking and isinstance(cost_tracking, dict):
                    stats['estimated_cost'] = cost_tracking.get('total_cost_usd', 0.0)
                else:
                    # Fallback cost estimation
                    stats['estimated_cost'] = stats['calls'] * 0.001
                
                self.logger.debug(f"📊 Birdeye API stats extracted: {stats['calls']} calls, {stats['successes']} successes")
                
            else:
                self.logger.warning("⚠️ Birdeye API not available for stats extraction")
                    
        except Exception as e:
            self.logger.error(f"❌ Error getting Birdeye API stats: {e}")
            
        return stats
        
    def _get_cross_platform_api_stats(self) -> Dict[str, Dict[str, Any]]:
        """Extract cross-platform API usage statistics"""
        stats = {
            'dexscreener': {
                'calls': 0,
                'successes': 0,
                'failures': 0,
                'total_time_ms': 0,
                'estimated_cost': 0.0
            },
            'rugcheck': {
                'calls': 0,
                'successes': 0,
                'failures': 0,
                'total_time_ms': 0,
                'estimated_cost': 0.0
            }
        }
        
        try:
            analyzer = self.cross_platform_analyzer
            
            # Try to extract stats from the analyzer
            if hasattr(analyzer, 'get_api_stats'):
                analyzer_stats = analyzer.get_api_stats()
                if analyzer_stats:
                    stats.update(analyzer_stats)
            
            # Estimate costs (DexScreener is free, RugCheck has rate limits)
            stats['dexscreener']['estimated_cost'] = 0.0  # Free API
            stats['rugcheck']['estimated_cost'] = stats['rugcheck']['calls'] * 0.0001  # Minimal cost
            
        except Exception as e:
            self.logger.error(f"❌ Error getting cross-platform API stats: {e}")
            
        return stats
        
    def _update_api_stats(self, provider: str, new_stats: Dict[str, Any]):
        """Update API statistics for a provider"""
        if provider not in self.session_stats['api_usage_by_service']:
            return
            
        provider_stats = self.session_stats['api_usage_by_service'][provider]
        
        # Update totals (incremental)
        calls_delta = new_stats.get('calls', 0) - provider_stats.get('last_calls', 0)
        successes_delta = new_stats.get('successes', 0) - provider_stats.get('last_successes', 0)
        failures_delta = new_stats.get('failures', 0) - provider_stats.get('last_failures', 0)
        time_delta = new_stats.get('total_time_ms', 0) - provider_stats.get('last_total_time_ms', 0)
        
        provider_stats['total_calls'] += calls_delta
        provider_stats['successful_calls'] += successes_delta
        provider_stats['failed_calls'] += failures_delta
        provider_stats['total_response_time_ms'] += time_delta
        
        # Calculate averages
        if provider_stats['total_calls'] > 0:
            provider_stats['avg_response_time_ms'] = provider_stats['total_response_time_ms'] / provider_stats['total_calls']
            
        # Update cost estimates
        provider_stats['estimated_cost_usd'] += new_stats.get('estimated_cost', 0.0)
        
        # Store last values for next delta calculation
        provider_stats['last_calls'] = new_stats.get('calls', 0)
        provider_stats['last_successes'] = new_stats.get('successes', 0)
        provider_stats['last_failures'] = new_stats.get('failures', 0)
        provider_stats['last_total_time_ms'] = new_stats.get('total_time_ms', 0)
        
    def _capture_system_performance(self):
        """Capture system resource usage"""
        try:
            # Memory usage
            memory_info = self.process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            
            if memory_mb > self.session_stats['performance_analysis']['system_resource_usage']['peak_memory_mb']:
                self.session_stats['performance_analysis']['system_resource_usage']['peak_memory_mb'] = memory_mb
                
            # CPU usage
            cpu_percent = self.process.cpu_percent()
            current_avg = self.session_stats['performance_analysis']['system_resource_usage']['avg_cpu_percent']
            cycle_count = self.session_stats['performance_metrics']['total_cycles'] + 1
            new_avg = (current_avg * (cycle_count - 1) + cpu_percent) / cycle_count
            self.session_stats['performance_analysis']['system_resource_usage']['avg_cpu_percent'] = new_avg
            
        except Exception as e:
            self.logger.error(f"❌ Error capturing system performance: {e}")
            
    def _record_error(self, error_type: str, error_message: str, provider: str = 'unknown', endpoint: str = 'unknown'):
        """Record error for pattern analysis"""
        try:
            error_analysis = self.session_stats['error_analysis']
            
            # Update counters
            error_analysis['total_errors'] += 1
            error_analysis['errors_by_service'][provider] += 1
            error_analysis['errors_by_endpoint'][endpoint] += 1
            error_analysis['errors_by_type'][error_type] += 1
            
            # Track consecutive failures
            error_analysis['consecutive_failures'] += 1
            if error_analysis['consecutive_failures'] > error_analysis['max_consecutive_failures']:
                error_analysis['max_consecutive_failures'] = error_analysis['consecutive_failures']
                
            # Record error with timestamp
            error_record = {
                'timestamp': datetime.now().isoformat(),
                'type': error_type,
                'message': error_message,
                'provider': provider,
                'endpoint': endpoint,
                'cycle_number': self.session_stats['performance_metrics']['total_cycles'] + 1
            }
            error_analysis['error_timeline'].append(error_record)
            
            # Keep only last 100 errors to prevent memory bloat
            if len(error_analysis['error_timeline']) > 100:
                error_analysis['error_timeline'] = error_analysis['error_timeline'][-100:]
                
        except Exception as e:
            self.logger.error(f"❌ Error recording error: {e}")
            
    def _record_successful_cycle(self):
        """Record successful cycle to reset consecutive failure counter"""
        self.session_stats['error_analysis']['consecutive_failures'] = 0
        
    def _measure_pipeline_performance(self, cycle_results: Dict[str, Any], cycle_duration: float):
        """Measure and record pipeline performance metrics"""
        try:
            performance_analysis = self.session_stats['performance_analysis']
            
            # Record overall cycle duration
            cycle_duration_ms = cycle_duration * 1000
            
            # Track slowest and fastest cycles
            cycle_record = {
                'cycle_number': self.session_stats['performance_metrics']['total_cycles'],
                'duration_ms': cycle_duration_ms,
                'timestamp': datetime.now().isoformat(),
                'tokens_found': cycle_results.get('new_candidates', 0),
                'alerts_sent': cycle_results.get('alerts_sent', 0)
            }
            
            # Update slowest cycles (keep top 5)
            performance_analysis['slowest_cycles'].append(cycle_record)
            performance_analysis['slowest_cycles'].sort(key=lambda x: x['duration_ms'], reverse=True)
            performance_analysis['slowest_cycles'] = performance_analysis['slowest_cycles'][:5]
            
            # Update fastest cycles (keep top 5)
            performance_analysis['fastest_cycles'].append(cycle_record)
            performance_analysis['fastest_cycles'].sort(key=lambda x: x['duration_ms'])
            performance_analysis['fastest_cycles'] = performance_analysis['fastest_cycles'][:5]
            
            # Identify bottlenecks (cycles taking >5 minutes)
            if cycle_duration > 300:
                bottleneck = {
                    'cycle_number': self.session_stats['performance_metrics']['total_cycles'],
                    'duration_seconds': cycle_duration,
                    'timestamp': datetime.now().isoformat(),
                    'potential_cause': 'Unknown - investigate API response times'
                }
                performance_analysis['bottlenecks_identified'].append(bottleneck)
                
        except Exception as e:
            self.logger.error(f"❌ Error measuring pipeline performance: {e}")
            self._record_error('performance_measurement', str(e), 'system')
            
    def _update_cost_analysis(self):
        """Update cost analysis based on current API usage"""
        try:
            cost_analysis = self.session_stats['cost_analysis']
            
            # Calculate total cost from all providers
            total_cost = 0.0
            for provider, stats in self.session_stats['api_usage_by_service'].items():
                total_cost += stats.get('estimated_cost_usd', 0.0)
                
            cost_analysis['total_estimated_cost_usd'] = total_cost
            
            # Estimate cost breakdown by stage (rough approximation)
            # Cross-platform analysis uses DexScreener (free) and RugCheck (minimal)
            cost_analysis['cost_breakdown_by_service']['rugcheck'] = (
                self.session_stats['api_usage_by_service']['rugcheck'].get('estimated_cost_usd', 0.0)
            )
            cost_analysis['cost_breakdown_by_service']['dexscreener'] = (
                self.session_stats['api_usage_by_service']['dexscreener'].get('estimated_cost_usd', 0.0)
            )
            cost_analysis['cost_breakdown_by_service']['birdeye_cross_platform'] = (
                self.session_stats['api_usage_by_service']['birdeye'].get('estimated_cost_usd', 0.0)
            )
            
            # Detailed Birdeye analysis
            birdeye_cost = self.session_stats['api_usage_by_service']['birdeye'].get('estimated_cost_usd', 0.0)
            cost_analysis['cost_breakdown_by_service']['birdeye_detailed_analysis'] = birdeye_cost * 0.6
            cost_analysis['cost_breakdown_by_service']['birdeye_whale_analysis'] = birdeye_cost * 0.15
            cost_analysis['cost_breakdown_by_service']['birdeye_volume_analysis'] = birdeye_cost * 0.15
            cost_analysis['cost_breakdown_by_service']['birdeye_security_analysis'] = birdeye_cost * 0.05
            cost_analysis['cost_breakdown_by_service']['birdeye_community_analysis'] = birdeye_cost * 0.05
            
        except Exception as e:
            self.logger.error(f"❌ Error updating cost analysis: {e}")
            self._record_error('cost_analysis_update', str(e), 'system')
            
    def _preserve_detailed_token_analysis(self, cycle_results: Dict[str, Any]):
        """Preserve detailed token analysis results"""
        try:
            if not isinstance(cycle_results, dict):
                return
                
            # Extract detailed analyses if available
            detailed_analyses = cycle_results.get('detailed_analyses', [])
            
            if not detailed_analyses or not isinstance(detailed_analyses, list):
                return
                
            # Store detailed analysis for each token
            for analysis in detailed_analyses:
                if not isinstance(analysis, dict) or 'candidate' not in analysis:
                    continue
                    
                candidate = analysis['candidate']
                token_address = candidate.get('address')
                if not token_address:
                    continue
                    
                # Create comprehensive token analysis record
                token_analysis = {
                    'last_analyzed': datetime.now().isoformat(),
                    'cycle_number': self.session_stats['performance_metrics']['total_cycles'],
                    'basic_info': {
                        'symbol': candidate.get('symbol', 'Unknown'),
                        'name': candidate.get('name', ''),
                        'address': token_address
                    },
                    'scores': {
                        'final_score': analysis.get('final_score', 0),
                        'cross_platform_score': candidate.get('cross_platform_score', 0)
                    },
                    'analysis_results': {
                        'overview_data': analysis.get('overview_data', {}),
                        'whale_analysis': analysis.get('whale_analysis', {}),
                        'volume_price_analysis': analysis.get('volume_price_analysis', {}),
                        'community_boost_analysis': analysis.get('community_boost_analysis', {}),
                        'security_analysis': analysis.get('security_analysis', {}),
                        'trading_activity': analysis.get('trading_activity', {})
                    },
                    'platforms': candidate.get('platforms', []),
                    'discovery_method': candidate.get('discovery_method', 'unknown')
                }
                
                # Store or update the analysis
                if token_address in self.session_stats['detailed_token_analyses']:
                    # Update existing analysis
                    existing = self.session_stats['detailed_token_analyses'][token_address]
                    existing['analysis_count'] = existing.get('analysis_count', 1) + 1
                    existing['last_analyzed'] = token_analysis['last_analyzed']
                    existing['cycle_number'] = token_analysis['cycle_number']
                    
                    # Update scores if better
                    if token_analysis['scores']['final_score'] > existing['scores']['final_score']:
                        existing['scores'] = token_analysis['scores']
                        existing['analysis_results'] = token_analysis['analysis_results']
                else:
                    # New token analysis
                    token_analysis['analysis_count'] = 1
                    token_analysis['first_discovered'] = token_analysis['last_analyzed']
                    self.session_stats['detailed_token_analyses'][token_address] = token_analysis
                    
        except Exception as e:
            self.logger.error(f"❌ Error preserving detailed token analysis: {e}")
            self._record_error('token_analysis_preservation', str(e), 'system')
            
    def _calculate_final_metrics(self):
        """Calculate final performance and efficiency metrics"""
        if self.session_stats['performance_metrics']['total_cycles'] == 0:
            return
            
        # Calculate averages - FIX: Use 'duration_seconds' instead of 'cycle_duration_seconds'
        total_duration = sum(c.get('duration_seconds', 0) for c in self.session_stats['detection_cycles'])
        self.session_stats['performance_metrics']['avg_cycle_duration'] = total_duration / self.session_stats['performance_metrics']['total_cycles']
        
        # Calculate rates
        total_tokens = self.session_stats['performance_metrics']['total_tokens_found']
        high_conviction = self.session_stats['performance_metrics']['high_conviction_tokens']
        
        if total_tokens > 0:
            self.session_stats['performance_metrics']['high_conviction_rate'] = (high_conviction / total_tokens) * 100
            
        # Calculate tokens per hour
        duration_hours = (datetime.now() - self.session_start_time).total_seconds() / 3600
        if duration_hours > 0:
            self.session_stats['performance_metrics']['tokens_per_hour'] = total_tokens / duration_hours
            
        # Calculate success rate
        successful_cycles = self.session_stats['performance_metrics']['successful_cycles']
        total_cycles = self.session_stats['performance_metrics']['total_cycles']
        self.session_stats['performance_metrics']['cycle_success_rate'] = (successful_cycles / total_cycles) * 100
        
        # Calculate API efficiency score
        self._calculate_api_efficiency_score()
        
        # Update cost per metrics
        total_cost = self.session_stats['cost_analysis']['total_estimated_cost_usd']
        if total_cycles > 0:
            self.session_stats['cost_analysis']['cost_per_cycle_avg'] = total_cost / total_cycles
        if total_tokens > 0:
            self.session_stats['cost_analysis']['cost_per_token_discovered'] = total_cost / total_tokens
        if high_conviction > 0:
            self.session_stats['cost_analysis']['cost_per_high_conviction_token'] = total_cost / high_conviction
            
    def _calculate_api_efficiency_score(self):
        """Calculate API efficiency score based on success rates and response times"""
        total_score = 0
        provider_count = 0
        
        for provider, stats in self.session_stats['api_usage_by_service'].items():
            if stats['total_calls'] > 0:
                success_rate = stats['successful_calls'] / stats['total_calls']
                response_time_score = max(0, (2000 - stats['avg_response_time_ms']) / 2000)  # Normalize to 0-1
                provider_score = (success_rate * 0.7 + response_time_score * 0.3) * 100
                total_score += provider_score
                provider_count += 1
                
        if provider_count > 0:
            self.session_stats['performance_metrics']['api_efficiency_score'] = total_score / provider_count
        else:
            self.session_stats['performance_metrics']['api_efficiency_score'] = 0
            
    def _generate_optimization_recommendations(self):
        """Generate optimization recommendations based on collected data"""
        recommendations = []
        
        # API efficiency recommendations
        for provider, stats in self.session_stats['api_usage_by_service'].items():
            if stats['total_calls'] > 0:
                success_rate = stats['successful_calls'] / stats['total_calls']
                if success_rate < 0.95:
                    recommendations.append(f"Improve {provider} API reliability (current: {success_rate*100:.1f}%)")
                    
                if stats['avg_response_time_ms'] > 1000:
                    recommendations.append(f"Optimize {provider} API response times (current: {stats['avg_response_time_ms']:.0f}ms)")
        
        # Cost optimization
        total_cost = self.session_stats['cost_analysis']['total_estimated_cost_usd']
        if total_cost > 50:  # Arbitrary threshold
            recommendations.append("Consider implementing more aggressive caching to reduce API costs")
            
        # Performance optimization
        avg_duration = self.session_stats['performance_metrics']['avg_cycle_duration']
        if avg_duration > 300:  # 5 minutes
            recommendations.append("Investigate cycle duration bottlenecks - cycles taking too long")
            
        # Error pattern recommendations
        if self.session_stats['error_analysis']['total_errors'] > self.session_stats['performance_metrics']['total_cycles'] * 0.1:
            recommendations.append("High error rate detected - implement better error handling")
            
        self.session_stats['cost_analysis']['optimization_recommendations'] = recommendations
        
    def _save_session_results(self):
        """Save comprehensive session results to file"""
        try:
            results_dir = Path("data") / "session_reports"
            results_dir.mkdir(exist_ok=True)
            
            results_file = results_dir / f"hc_detector_session_{self.session_id}.json"
            
            # Update final stats
            self.session_stats['end_time'] = datetime.now().isoformat()
            self.session_stats['actual_duration_minutes'] = (datetime.now() - self.session_start_time).total_seconds() / 60
            
            # Calculate final performance metrics
            self._calculate_final_metrics()
            
            # Generate optimization recommendations
            self._generate_optimization_recommendations()
            
            # Convert defaultdict to regular dict for JSON serialization
            self._convert_defaultdicts_for_json()
            
            with open(results_file, 'w') as f:
                json.dump(self.session_stats, f, indent=2, default=str)
                
            self.logger.info(f"📊 Enhanced session results saved to: {results_file}")
            
            # Also save a summary report
            self._save_summary_report(results_dir)
            
        except Exception as e:
            self.logger.error(f"❌ Error saving session results: {e}")
            
    def _convert_defaultdicts_for_json(self):
        """Convert defaultdict objects to regular dicts for JSON serialization"""
        def convert_defaultdict(obj):
            if isinstance(obj, defaultdict):
                return dict(obj)
            elif isinstance(obj, dict):
                return {k: convert_defaultdict(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_defaultdict(item) for item in obj]
            else:
                return obj
        
        self.session_stats = convert_defaultdict(self.session_stats)
        
    def _save_summary_report(self, results_dir: Path):
        """Save a human-readable summary report"""
        summary_file = results_dir / f"hc_detector_summary_{self.session_id}.txt"
        
        with open(summary_file, 'w') as f:
            f.write("="*80 + "\n")
            f.write("HIGH CONVICTION DETECTOR SESSION SUMMARY\n")
            f.write("="*80 + "\n\n")
            
            # Basic stats
            f.write(f"Session ID: {self.session_id}\n")
            f.write(f"Duration: {self.session_stats.get('actual_duration_minutes', 0):.1f} minutes\n")
            f.write(f"Total Cycles: {self.session_stats['performance_metrics']['total_cycles']}\n")
            f.write(f"Successful Cycles: {self.session_stats['performance_metrics']['successful_cycles']}\n")
            f.write(f"Success Rate: {self.session_stats['performance_metrics']['cycle_success_rate']:.1f}%\n\n")
            
            # Token discovery
            f.write("TOKEN DISCOVERY:\n")
            f.write(f"- Total Tokens Found: {self.session_stats['performance_metrics']['total_tokens_found']}\n")
            f.write(f"- Unique Tokens: {self.session_stats['performance_metrics']['unique_tokens']}\n")
            f.write(f"- High Conviction Tokens: {self.session_stats['performance_metrics']['high_conviction_tokens']}\n")
            f.write(f"- High Conviction Rate: {self.session_stats['performance_metrics']['high_conviction_rate']:.1f}%\n")
            f.write(f"- Total Alerts Sent: {self.session_stats['performance_metrics']['total_alerts_sent']}\n\n")
            
            # API usage
            f.write("API USAGE SUMMARY:\n")
            for provider, stats in self.session_stats['api_usage_by_service'].items():
                f.write(f"- {provider.upper()}:\n")
                f.write(f"  Total Calls: {stats['total_calls']}\n")
                f.write(f"  Success Rate: {(stats['successful_calls']/max(stats['total_calls'], 1)*100):.1f}%\n")
                f.write(f"  Avg Response Time: {stats['avg_response_time_ms']:.0f}ms\n")
                f.write(f"  Estimated Cost: ${stats['estimated_cost_usd']:.4f}\n\n")
            
            # Cost analysis
            f.write("COST ANALYSIS:\n")
            f.write(f"- Total Estimated Cost: ${self.session_stats['cost_analysis']['total_estimated_cost_usd']:.4f}\n")
            f.write(f"- Cost per Cycle: ${self.session_stats['cost_analysis']['cost_per_cycle_avg']:.4f}\n")
            f.write(f"- Cost per Token: ${self.session_stats['cost_analysis']['cost_per_token_discovered']:.4f}\n\n")
            
            # Performance
            f.write("PERFORMANCE ANALYSIS:\n")
            f.write(f"- Average Cycle Duration: {self.session_stats['performance_metrics']['avg_cycle_duration']:.1f}s\n")
            f.write(f"- Peak Memory Usage: {self.session_stats['performance_analysis']['system_resource_usage']['peak_memory_mb']:.1f}MB\n")
            f.write(f"- API Efficiency Score: {self.session_stats['performance_metrics']['api_efficiency_score']:.1f}/100\n\n")
            
            # Errors
            f.write("ERROR ANALYSIS:\n")
            f.write(f"- Total Errors: {self.session_stats['error_analysis']['total_errors']}\n")
            f.write(f"- Recovery Success Rate: {self.session_stats['error_analysis']['recovery_success_rate']:.1f}%\n")
            f.write(f"- Max Consecutive Failures: {self.session_stats['error_analysis']['max_consecutive_failures']}\n\n")
            
            # Optimization recommendations
            f.write("OPTIMIZATION RECOMMENDATIONS:\n")
            for i, rec in enumerate(self.session_stats['cost_analysis']['optimization_recommendations'], 1):
                f.write(f"{i}. {rec}\n")
                
        self.logger.info(f"📋 Summary report saved to: {summary_file}")
        
    def _print_cycle_summary(self, cycle_results: Dict[str, Any]):
        """Print summary of current cycle results"""
        try:
            status = cycle_results.get('status', 'unknown')
            alerts_sent = cycle_results.get('alerts_sent', 0)
            new_candidates = cycle_results.get('new_candidates', 0)
            total_analyzed = cycle_results.get('total_analyzed', 0)
            duration = cycle_results.get('cycle_duration_seconds', 0)
            
            self.logger.info(f"📊 Cycle Summary - Status: {status}")
            self.logger.info(f"🔍 Analyzed: {total_analyzed} tokens | New Candidates: {new_candidates}")
            self.logger.info(f"📱 Alerts Sent: {alerts_sent} | Duration: {duration:.1f}s")
            
            # API efficiency update
            total_cost = self.session_stats['cost_analysis']['total_estimated_cost_usd']
            self.logger.info(f"💰 Session Cost: ${total_cost:.4f}")
            
        except Exception as e:
            self.logger.error(f"❌ Error printing cycle summary: {e}")
            
    # ==================== END ENHANCED REPORTING METHODS ====================

    async def run_detection_cycle(self) -> Dict[str, Any]:
        """
        Run a complete detection cycle with enhanced reporting:
        1. Cross-platform analysis for initial filtering
        2. Detailed Birdeye analysis for high-conviction tokens
        3. Send alerts for new high-conviction tokens
        4. Comprehensive API usage and performance tracking
        """
        cycle_start_time = time.time()
        scan_id = f"hc_scan_{int(cycle_start_time)}"
        
        self.logger.info(f"🔍 Starting enhanced detection cycle - {scan_id}")
        
        try:
            # Update cycle counter
            self.session_stats['performance_metrics']['total_cycles'] += 1
            
            # Capture system performance at start
            self._capture_system_performance()
            
            # Step 1: Cross-platform analysis for cost-effective filtering
            self.logger.info("📊 Step 1: Running cross-platform analysis...")
            cross_platform_start = time.time()
            
            cross_platform_results = await self.cross_platform_analyzer.run_analysis()
            cross_platform_duration = (time.time() - cross_platform_start) * 1000
            self.session_stats['performance_analysis']['pipeline_stage_durations']['cross_platform_analysis_ms'].append(cross_platform_duration)
            
            if not cross_platform_results:
                self.logger.warning("⚠️ No results from cross-platform analysis")
                self._record_error('cross_platform_analysis', 'No results returned', 'cross_platform_analyzer')
                return {"status": "no_results", "scan_id": scan_id}
                
            # Extract high-conviction candidates
            high_conviction_candidates = self._extract_high_conviction_candidates(cross_platform_results)
            
            if not high_conviction_candidates:
                self.logger.info("📊 No high-conviction candidates found in cross-platform analysis")
                cycle_duration = time.time() - cycle_start_time
                
                # Update session stats even for cycles with no candidates
                self._update_session_stats_for_cycle({
                    "status": "no_high_conviction",
                    "scan_id": scan_id,
                    "total_analyzed": len(cross_platform_results.get('correlations', {}).get('all_tokens', {})),
                    "candidates": 0,
                    "cycle_duration_seconds": cycle_duration
                })
                
                return {
                    "status": "no_high_conviction",
                    "scan_id": scan_id,
                    "total_analyzed": len(cross_platform_results.get('correlations', {}).get('all_tokens', {})),
                    "candidates": 0,
                    "cycle_duration_seconds": cycle_duration
                }
                
            self.logger.info(f"🎯 Found {len(high_conviction_candidates)} high-conviction candidates")
            
            # Step 2: Filter out already alerted tokens
            new_candidates = [
                token for token in high_conviction_candidates
                if token['address'] not in self.alerted_tokens
            ]
            
            if not new_candidates:
                self.logger.info("📊 All high-conviction candidates were already alerted")
                cycle_duration = time.time() - cycle_start_time
                
                self._update_session_stats_for_cycle({
                    "status": "all_already_alerted",
                    "scan_id": scan_id,
                    "total_candidates": len(high_conviction_candidates),
                    "new_candidates": 0,
                    "cycle_duration_seconds": cycle_duration
                })
                
                return {
                    "status": "all_already_alerted",
                    "scan_id": scan_id,
                    "total_candidates": len(high_conviction_candidates),
                    "new_candidates": 0,
                    "cycle_duration_seconds": cycle_duration
                }
                
            self.logger.info(f"🆕 Found {len(new_candidates)} new high-conviction candidates")
            
            # Step 3: Detailed Birdeye analysis for new candidates
            detailed_results = []
            alerts_sent = 0
            
            for candidate in new_candidates:
                try:
                    self.logger.info(f"🔬 Analyzing candidate: {candidate['symbol']} ({candidate['address']})")
                    
                    # Perform detailed analysis with timing
                    detailed_start = time.time()
                    detailed_analysis = await self._perform_detailed_analysis(candidate, scan_id)
                    detailed_duration = (time.time() - detailed_start) * 1000
                    self.session_stats['performance_analysis']['pipeline_stage_durations']['detailed_analysis_ms'].append(detailed_duration)
                    
                    if detailed_analysis:
                        detailed_results.append(detailed_analysis)
                        
                        # Update session registry with corrected symbol/name from detailed analysis
                        self._update_session_registry_with_detailed_analysis(detailed_analysis)
                        
                        # Send alert if score is high enough
                        if detailed_analysis['final_score'] >= self.high_conviction_threshold:
                            alert_start = time.time()
                            success = await self._send_detailed_alert(detailed_analysis, scan_id)
                            alert_duration = (time.time() - alert_start) * 1000
                            self.session_stats['performance_analysis']['pipeline_stage_durations']['alert_generation_ms'].append(alert_duration)
                            
                            if success:
                                alerts_sent += 1
                                self.alerted_tokens.add(candidate['address'])
                                
                    # Add delay between detailed analyses to respect rate limits
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    self.logger.error(f"❌ Error analyzing candidate {candidate['address']}: {e}")
                    self._record_error('candidate_analysis', str(e), 'birdeye', 'detailed_analysis')
                    continue
                    
            # Save updated alerted tokens
            self._save_alerted_tokens()
            
            # Capture final API usage stats
            self._capture_api_usage_stats()
            
            # Calculate cycle duration
            cycle_duration = time.time() - cycle_start_time
            
            # Prepare result
            result = {
                "status": "completed",
                "scan_id": scan_id,
                "cycle_duration_seconds": cycle_duration,
                "total_analyzed": len(cross_platform_results.get('correlations', {}).get('all_tokens', {})),
                "high_conviction_candidates": len(high_conviction_candidates),
                "new_candidates": len(new_candidates),
                "detailed_analyses": len(detailed_results),
                "alerts_sent": alerts_sent,
                "timestamp": datetime.now().isoformat(),
                # Add actual data for token discovery tracking
                "detailed_analyses_data": detailed_results,
                "high_conviction_candidates_data": high_conviction_candidates
            }
            
            # Update session statistics
            self._update_session_stats_for_cycle(result)
            
            # Enhanced token registry tracking - MOVED AFTER detailed analysis completion
            cycle_number = self.session_stats['performance_metrics']['total_cycles']
            self._record_scan_tokens(result, cycle_number)
            
            # Update health monitoring
            if hasattr(self, '_update_health_monitoring'):
                self._update_health_monitoring()
            
            # Record successful cycle
            self._record_successful_cycle()
            
            # Print optimized scan summary (falls back to comprehensive if needed)
            self._print_optimized_scan_summary(result)
            
            self.logger.info(f"✅ Detection cycle completed in {cycle_duration:.1f}s")
            self.logger.info(f"📊 Results: {alerts_sent} alerts sent from {len(new_candidates)} new candidates")
            
            # Save session results periodically (every 10 cycles)
            if self.session_stats['performance_metrics']['total_cycles'] % 10 == 0:
                self._save_session_results()
                self.logger.info(f"💾 Session results saved (Cycle {self.session_stats['performance_metrics']['total_cycles']})")
            
            return result
            
        except Exception as e:
            cycle_duration = time.time() - cycle_start_time
            self.logger.error(f"❌ Error in detection cycle: {e}")
            self._record_error('detection_cycle', str(e), 'detector', 'run_detection_cycle')
            
            error_result = {
                "status": "error",
                "scan_id": scan_id,
                "error": str(e),
                "cycle_duration_seconds": cycle_duration,
                "timestamp": datetime.now().isoformat()
            }
            
            # Update stats for failed cycle
            self._update_session_stats_for_cycle(error_result)
            
            return error_result

    def _update_session_stats_for_cycle(self, cycle_results: Dict[str, Any]):
        """Update comprehensive session statistics for a cycle"""
        try:
            # Record cycle details
            cycle_record = {
                'cycle_number': self.session_stats['performance_metrics']['total_cycles'],
                'timestamp': datetime.now().isoformat(),
                'duration_seconds': cycle_results.get('cycle_duration_seconds', 0),
                'status': cycle_results.get('status', 'unknown'),
                'tokens_analyzed': cycle_results.get('total_analyzed', 0),
                'high_conviction_candidates': cycle_results.get('high_conviction_candidates', 0),
                'new_candidates': cycle_results.get('new_candidates', 0),
                'detailed_analyses': cycle_results.get('detailed_analyses', 0),
                'alerts_sent': cycle_results.get('alerts_sent', 0),
                'scan_id': cycle_results.get('scan_id', '')
            }
            
            self.session_stats['detection_cycles'].append(cycle_record)
            
            # Update performance metrics
            if cycle_results.get('status') == 'completed':
                self.session_stats['performance_metrics']['successful_cycles'] += 1
                
            # Update token counts
            new_candidates = cycle_results.get('new_candidates', 0)
            alerts_sent = cycle_results.get('alerts_sent', 0)
            
            self.session_stats['performance_metrics']['total_tokens_found'] += new_candidates
            self.session_stats['performance_metrics']['total_alerts_sent'] += alerts_sent
            
            # Update token discovery tracking
            self._update_token_discovery_tracking(cycle_results)
            
            # Update unique tokens count
            self.session_stats['performance_metrics']['unique_tokens'] = len(self.session_stats['tokens_discovered'])
            
            # Update high conviction tokens count
            high_conviction_count = len([
                t for t in self.session_stats['tokens_discovered'].values() 
                if t.get('best_conviction_score', 0) >= self.high_conviction_threshold
            ])
            self.session_stats['performance_metrics']['high_conviction_tokens'] = high_conviction_count
            
            # Capture system performance
            self._capture_system_performance()
            
            # Measure pipeline performance
            self._measure_pipeline_performance(cycle_results, cycle_results.get('cycle_duration_seconds', 0))
            
            # Update cost analysis
            self._update_cost_analysis()
            
            # Preserve detailed token analysis if available
            if 'detailed_analyses_data' in cycle_results and isinstance(cycle_results['detailed_analyses_data'], list):
                self._preserve_detailed_token_analysis({'detailed_analyses': cycle_results['detailed_analyses_data']})
                
        except Exception as e:
            self.logger.error(f"❌ Error updating session stats: {e}")
            self._record_error('session_stats_update', str(e), 'system')

    def _update_token_discovery_tracking(self, cycle_results: Dict[str, Any]):
        """Update token discovery tracking with detailed analysis data"""
        try:
            current_time = datetime.now().isoformat()
            
            # Process detailed analysis data if available
            detailed_analyses = cycle_results.get('detailed_analyses_data', [])
            
            for analysis in detailed_analyses:
                # FIX: Extract address from the candidate object within the analysis
                candidate = analysis.get('candidate', {})
                address = candidate.get('address')
                if not address:
                    continue
                    
                # FIX: Extract symbol and name from the candidate (which has been updated with Birdeye data)
                symbol = candidate.get('symbol', 'Unknown')
                name = candidate.get('name', '')
                final_score = analysis.get('final_score', 0)
                cross_platform_score = candidate.get('cross_platform_score', 0)
                
                # Update or create token discovery record
                if address in self.session_stats['tokens_discovered']:
                    # Update existing record
                    token_record = self.session_stats['tokens_discovered'][address]
                    token_record['times_detected'] += 1
                    token_record['last_seen'] = current_time
                    
                    # FIX: Update symbol and name if we have better data
                    if symbol != 'Unknown' and symbol != token_record.get('symbol', ''):
                        token_record['symbol'] = symbol
                        self.logger.debug(f"🏷️ Updated symbol for {address}: {symbol}")
                    
                    if name and name != token_record.get('name', ''):
                        token_record['name'] = name
                        self.logger.debug(f"🏷️ Updated name for {address}: {name}")
                    
                    # Update best scores if improved
                    if final_score > token_record.get('best_conviction_score', 0):
                        token_record['best_conviction_score'] = final_score
                        token_record['best_analysis_data'] = analysis
                    
                    if cross_platform_score > token_record.get('best_cross_platform_score', 0):
                        token_record['best_cross_platform_score'] = cross_platform_score
                        
                else:
                    # Create new token record
                    self.session_stats['tokens_discovered'][address] = {
                        'symbol': symbol,
                        'name': name,
                        'address': address,
                        'first_seen': current_time,
                        'last_seen': current_time,
                        'times_detected': 1,
                        'best_conviction_score': final_score,
                        'best_cross_platform_score': cross_platform_score,
                        'best_analysis_data': analysis,
                        'discovery_method': candidate.get('discovery_method', 'unknown'),
                        'platforms': candidate.get('platforms', []),
                        'alert_sent': analysis.get('alert_sent', False),
                        'analysis_history': [analysis]
                    }
            
            # Also process high conviction candidates if no detailed analyses available
            if not detailed_analyses:
                candidates = cycle_results.get('high_conviction_candidates_data', [])
                for candidate in candidates:
                    address = candidate.get('address')
                    if not address:
                        continue
                        
                    symbol = candidate.get('symbol', 'Unknown')
                    cross_platform_score = candidate.get('cross_platform_score', 0)
                    
                    if address not in self.session_stats['tokens_discovered']:
                        self.session_stats['tokens_discovered'][address] = {
                            'symbol': symbol,
                            'name': candidate.get('name', ''),
                            'address': address,
                            'first_seen': current_time,
                            'last_seen': current_time,
                            'times_detected': 1,
                            'best_conviction_score': 0,
                            'best_cross_platform_score': cross_platform_score,
                            'best_analysis_data': candidate,
                            'discovery_method': candidate.get('discovery_method', 'cross_platform'),
                            'platforms': candidate.get('platforms', []),
                            'alert_sent': False,
                            'analysis_history': [candidate]
                        }
                        
        except Exception as e:
            self.logger.error(f"❌ Error updating token discovery tracking: {e}")
            self._record_error('token_discovery_tracking', str(e), 'system')

    def _extract_high_conviction_candidates(self, cross_platform_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract high-conviction candidates from enhanced cross-platform analysis results"""
        candidates = []
        
        try:
            correlations = cross_platform_results.get('correlations', {})
            
            # Debug logging to understand the structure
            if self.is_debug_enabled():
                self.logger.debug(f"🔍 Correlations keys: {list(correlations.keys())}")
                self.logger.debug(f"🔍 Multi-platform tokens count: {len(correlations.get('multi_platform_tokens', []))}")
                self.logger.debug(f"🔍 All tokens count: {len(correlations.get('all_tokens', {}))}")
            
            # Handle both old and new data structures - prioritize new structure
            multi_platform_tokens = correlations.get('multi_platform_tokens', [])
            
            if multi_platform_tokens:
                # New enhanced structure
                self.logger.info(f"📊 Processing {len(multi_platform_tokens)} tokens from new 'multi_platform_tokens' structure")
                
                for token_info in multi_platform_tokens:
                    score = token_info.get('score', 0)
                    address = token_info.get('address', '')
                    
                    if score >= self.min_cross_platform_score and address:
                        # Get additional data from the token platforms
                        platforms = token_info.get('platforms', [])
                        
                        # Extract enhanced metadata
                        enhanced_data = self._extract_enhanced_metadata(address, cross_platform_results)
                        
                        candidate = {
                            'address': address,
                            'symbol': enhanced_data.get('symbol', 'Unknown'),
                            'name': enhanced_data.get('name', ''),
                            'cross_platform_score': score,
                            'platforms': platforms,
                            'price': enhanced_data.get('price', 0),
                            'volume_24h': enhanced_data.get('volume_24h', 0),
                            'market_cap': enhanced_data.get('market_cap', 0),
                            'liquidity': enhanced_data.get('liquidity', 0),
                            'boost_data': enhanced_data.get('boost_data', {}),
                            'community_data': enhanced_data.get('community_data', {}),
                            'social_data': enhanced_data.get('social_data', {}),
                            'narrative_data': enhanced_data.get('narrative_data', {}),
                            'liquidity_analysis': enhanced_data.get('liquidity_analysis', {}),
                            'discovery_method': enhanced_data.get('discovery_method', 'cross_platform')
                        }
                        candidates.append(candidate)
                        
                        if self.is_debug_enabled():  # Enable debug output
                            self.logger.debug(f"✅ Added candidate: {candidate['symbol']} ({address[:8]}...) - Score: {score}")
            else:
                # Fallback for old structure
                all_tokens = correlations.get('all_tokens', {})
                self.logger.info(f"📊 Processing {len(all_tokens)} tokens from old 'all_tokens' structure")
                
                for address, token_data in all_tokens.items():
                    score = token_data.get('score', 0)
                    
                    if score >= self.min_cross_platform_score:
                        candidate = {
                            'address': address,
                            'symbol': token_data.get('symbol', 'Unknown'),
                            'name': token_data.get('name', ''),
                            'cross_platform_score': score,
                            'platforms': token_data.get('platforms', []),
                            'price': token_data.get('price', 0),
                            'volume_24h': token_data.get('volume_24h', 0),
                            'market_cap': token_data.get('market_cap', 0),
                            'liquidity': token_data.get('liquidity', 0),
                            'boost_data': token_data.get('boost_data', {}),
                            'community_data': token_data.get('community_data', {}),
                            'social_data': {},
                            'narrative_data': {},
                            'liquidity_analysis': {},
                            'discovery_method': 'cross_platform'
                        }
                        candidates.append(candidate)
                        
                        if self.is_debug_enabled():  # Enable debug output
                            self.logger.debug(f"✅ Added candidate: {candidate['symbol']} ({address[:8]}...) - Score: {score}")
                    
            # Sort by score (highest first)
            candidates.sort(key=lambda x: x['cross_platform_score'], reverse=True)
            
            self.logger.info(f"🎯 Successfully extracted {len(candidates)} high-conviction candidates (min score: {self.min_cross_platform_score})")
            
        except Exception as e:
            self.logger.error(f"❌ Error extracting candidates: {e}")
            if self.is_debug_enabled():  # Enable debug output
                import traceback
                self.logger.debug(f"🔍 Full traceback: {traceback.format_exc()}")
            
        return candidates

    def _extract_enhanced_metadata(self, address: str, cross_platform_results: Dict[str, Any]) -> Dict[str, Any]:
        """Extract enhanced metadata for a token from cross-platform results"""
        enhanced_data = {
            'symbol': 'Unknown',
            'name': '',
            'price': 0,
            'volume_24h': 0,
            'market_cap': 0,
            'liquidity': 0,
            'boost_data': {},
            'community_data': {},
            'social_data': {},
            'narrative_data': {},
            'liquidity_analysis': {},
            'discovery_method': 'cross_platform'
        }
        
        try:
            # Look for the token in the raw platform data
            platform_data = cross_platform_results.get('platform_data', {})
            
            # Check DexScreener data
            for ds_tokens in [platform_data.get('dexscreener_boosted', []), 
                             platform_data.get('dexscreener_top', [])]:
                for token in ds_tokens:
                    if token.get('tokenAddress') == address:
                        enhanced_data['boost_data'] = {
                            'amount': token.get('amount', 0),
                            'totalAmount': token.get('totalAmount', 0),
                            'description': token.get('description', ''),
                            'links': token.get('links', [])
                        }
                        break
            
            # Check DexScreener profiles
            for profile in platform_data.get('dexscreener_profiles', []):
                if profile.get('address') == address:
                    enhanced_data['social_data'] = {
                        'social_score': profile.get('social_score', 0),
                        'narrative_strength': profile.get('narrative_strength', 0),
                        'website': profile.get('website'),
                        'twitter': profile.get('twitter'),
                        'telegram': profile.get('telegram'),
                        'description': profile.get('description', '')
                    }
                    break
            
            # Check narrative discovery
            narrative_data = platform_data.get('dexscreener_narratives', {})
            narratives_found = []
            for narrative, tokens in narrative_data.items():
                for token in tokens:
                    if token.get('address') == address:
                        narratives_found.append({
                            'narrative': narrative,
                            'symbol': token.get('symbol', ''),
                            'name': token.get('name', ''),
                            'price_usd': token.get('price_usd', 0),
                            'volume_24h': token.get('volume_24h', 0),
                            'market_cap': token.get('market_cap', 0)
                        })
                        # Update basic data from narrative discovery
                        # Prioritize actual token symbol over 'Unknown'
                        token_symbol = token.get('symbol', '')
                        if token_symbol and token_symbol != 'Unknown':
                            enhanced_data['symbol'] = token_symbol
                        elif not enhanced_data['symbol'] or enhanced_data['symbol'] == 'Unknown':
                            enhanced_data['symbol'] = token.get('symbol', 'Unknown')
                        if not enhanced_data['name']:
                            enhanced_data['name'] = token.get('name', '')
                        if not enhanced_data['price']:
                            enhanced_data['price'] = token.get('price_usd', 0)
                        if not enhanced_data['volume_24h']:
                            enhanced_data['volume_24h'] = token.get('volume_24h', 0)
                        if not enhanced_data['market_cap']:
                            enhanced_data['market_cap'] = token.get('market_cap', 0)
            
            if narratives_found:
                enhanced_data['narrative_data'] = {
                    'narratives': narratives_found,
                    'narrative_count': len(narratives_found),
                    'discovery_method': 'narrative_discovery'
                }
                enhanced_data['discovery_method'] = 'narrative_discovery'
            
            # Check Birdeye data
            for token in platform_data.get('birdeye_trending', []):
                if token.get('address') == address:
                    enhanced_data.update({
                        'symbol': token.get('symbol', enhanced_data['symbol']),
                        'name': token.get('name', enhanced_data['name']),
                        'volume_24h': token.get('v24hUSD', enhanced_data['volume_24h']),
                        'price': token.get('price', enhanced_data['price']),
                        'market_cap': token.get('mc', enhanced_data['market_cap']),
                        'liquidity': token.get('liquidity', enhanced_data['liquidity'])
                    })
                    break
            
            # Check RugCheck data
            for token in platform_data.get('rugcheck_trending', []):
                if token.get('mint') == address:
                    enhanced_data['community_data'] = {
                        'vote_count': token.get('vote_count', 0),
                        'up_count': token.get('up_count', 0),
                        'sentiment_score': token.get('up_count', 0) / max(token.get('vote_count', 1), 1)
                    }
                    break
            
        except Exception as e:
            self.logger.error(f"❌ Error extracting enhanced metadata for {address}: {e}")
        
        return enhanced_data
        
    async def _perform_detailed_analysis(self, candidate: Dict[str, Any], scan_id: str) -> Optional[Dict[str, Any]]:
        """Perform detailed Birdeye analysis for a high-conviction candidate with enhanced error logging"""
        if not self.birdeye_api:
            self.logger.warning("⚠️ Birdeye API not available for detailed analysis")
            self._record_error('birdeye_api_unavailable', 'Birdeye API not initialized', 'birdeye', 'detailed_analysis')
            return None
            
        address = candidate['address']
        symbol = candidate.get('symbol', 'Unknown')
        
        self.logger.info(f"🔬 Starting detailed analysis for {symbol} ({address})")
        
        # Enhanced error tracking for each analysis step
        analysis_results = {
            'overview_data': {},
            'whale_analysis': {},
            'volume_price_analysis': {},
            'community_boost_analysis': {},
            'security_analysis': {},
            'trading_activity': {}
        }
        
        analysis_errors = {
            'overview_data': None,
            'whale_analysis': None,
            'volume_price_analysis': None,
            'community_boost_analysis': None,
            'security_analysis': None,
            'trading_activity': None
        }
        
        successful_analyses = 0
        total_analyses = 6
        
        try:
            # Test each analysis function individually with detailed error logging
            
            # 1. Token Overview Analysis
            try:
                self.logger.info(f"📊 Step 1/6: Getting token overview for {symbol}")
                overview_data = await self._get_token_overview_data_enhanced(address, scan_id)
                analysis_results['overview_data'] = overview_data
                if overview_data:
                    successful_analyses += 1
                    self.logger.info(f"✅ Token overview successful for {symbol}")
                else:
                    self.logger.warning(f"⚠️ Token overview returned empty for {symbol}")
            except Exception as e:
                error_msg = f"Token overview failed: {str(e)}"
                analysis_errors['overview_data'] = error_msg
                self.logger.error(f"❌ Token overview failed for {symbol}: {e}")
                self._record_error('token_overview_analysis', str(e), 'birdeye', 'get_token_overview')
                
            # 2. Whale Holder Analysis
            try:
                self.logger.info(f"🐋 Step 2/6: Getting whale analysis for {symbol}")
                whale_analysis = await self._get_whale_holder_analysis_enhanced(address, scan_id)
                analysis_results['whale_analysis'] = whale_analysis
                if whale_analysis:
                    successful_analyses += 1
                    self.logger.info(f"✅ Whale analysis successful for {symbol}")
                else:
                    self.logger.warning(f"⚠️ Whale analysis returned empty for {symbol}")
            except Exception as e:
                error_msg = f"Whale analysis failed: {str(e)}"
                analysis_errors['whale_analysis'] = error_msg
                self.logger.error(f"❌ Whale analysis failed for {symbol}: {e}")
                self._record_error('whale_holder_analysis', str(e), 'birdeye', 'get_token_holders')
                
            # 3. Volume Price Analysis
            try:
                self.logger.info(f"📈 Step 3/6: Getting volume/price analysis for {symbol}")
                volume_price_analysis = await self._get_volume_price_analysis_enhanced(address, scan_id)
                analysis_results['volume_price_analysis'] = volume_price_analysis
                if volume_price_analysis:
                    successful_analyses += 1
                    self.logger.info(f"✅ Volume/price analysis successful for {symbol}")
                else:
                    self.logger.warning(f"⚠️ Volume/price analysis returned empty for {symbol}")
            except Exception as e:
                error_msg = f"Volume/price analysis failed: {str(e)}"
                analysis_errors['volume_price_analysis'] = error_msg
                self.logger.error(f"❌ Volume/price analysis failed for {symbol}: {e}")
                self._record_error('volume_price_analysis', str(e), 'birdeye', 'get_ohlcv_data')
                
            # 4. Community Boost Analysis
            try:
                self.logger.info(f"👥 Step 4/6: Getting community analysis for {symbol}")
                community_boost_analysis = await self._get_community_boost_analysis_enhanced(address, scan_id)
                analysis_results['community_boost_analysis'] = community_boost_analysis
                if community_boost_analysis:
                    successful_analyses += 1
                    self.logger.info(f"✅ Community analysis successful for {symbol}")
                else:
                    self.logger.warning(f"⚠️ Community analysis returned empty for {symbol}")
            except Exception as e:
                error_msg = f"Community analysis failed: {str(e)}"
                analysis_errors['community_boost_analysis'] = error_msg
                self.logger.error(f"❌ Community analysis failed for {symbol}: {e}")
                self._record_error('community_boost_analysis', str(e), 'birdeye', 'get_token_overview')
                
            # 5. Security Analysis
            try:
                self.logger.info(f"🔒 Step 5/6: Getting security analysis for {symbol}")
                security_analysis = await self._get_security_analysis_enhanced(address, scan_id)
                analysis_results['security_analysis'] = security_analysis
                if security_analysis:
                    successful_analyses += 1
                    self.logger.info(f"✅ Security analysis successful for {symbol}")
                else:
                    self.logger.warning(f"⚠️ Security analysis returned empty for {symbol}")
            except Exception as e:
                error_msg = f"Security analysis failed: {str(e)}"
                analysis_errors['security_analysis'] = error_msg
                self.logger.error(f"❌ Security analysis failed for {symbol}: {e}")
                self._record_error('security_analysis', str(e), 'birdeye', 'security_check')
                
            # 6. Trading Activity Analysis
            try:
                self.logger.info(f"💹 Step 6/6: Getting trading activity for {symbol}")
                trading_activity = await self._get_trading_activity_analysis_enhanced(address, scan_id)
                analysis_results['trading_activity'] = trading_activity
                if trading_activity:
                    successful_analyses += 1
                    self.logger.info(f"✅ Trading activity successful for {symbol}")
                else:
                    self.logger.warning(f"⚠️ Trading activity returned empty for {symbol}")
            except Exception as e:
                error_msg = f"Trading activity failed: {str(e)}"
                analysis_errors['trading_activity'] = error_msg
                self.logger.error(f"❌ Trading activity failed for {symbol}: {e}")
                self._record_error('trading_activity_analysis', str(e), 'birdeye', 'get_token_transactions')
            
            # Log comprehensive analysis summary
            self.logger.info(f"📊 DETAILED ANALYSIS SUMMARY for {symbol}:")
            self.logger.info(f"  ✅ Successful analyses: {successful_analyses}/{total_analyses}")
            self.logger.info(f"  📊 Success rate: {(successful_analyses/total_analyses)*100:.1f}%")
            
            # Log any errors encountered
            failed_analyses = [name for name, error in analysis_errors.items() if error is not None]
            if failed_analyses:
                self.logger.warning(f"  ❌ Failed analyses: {', '.join(failed_analyses)}")
                for analysis_name, error in analysis_errors.items():
                    if error:
                        self.logger.warning(f"    • {analysis_name}: {error}")
            
            # Update candidate with fresh symbol/name from Birdeye API if available
            overview_data = analysis_results['overview_data']
            if overview_data and overview_data.get('symbol', 'Unknown') != 'Unknown':
                candidate['symbol'] = overview_data['symbol']
                self.logger.info(f"🏷️ Updated symbol from Birdeye: {candidate['symbol']}")
            if overview_data and overview_data.get('name'):
                candidate['name'] = overview_data['name']
                self.logger.info(f"🏷️ Updated name from Birdeye: {candidate['name']}")
            
            # Calculate final score even with partial data
            final_score, scoring_breakdown = self._calculate_final_score(
                candidate, 
                analysis_results['overview_data'], 
                analysis_results['whale_analysis'], 
                analysis_results['volume_price_analysis'],
                analysis_results['community_boost_analysis'], 
                analysis_results['security_analysis'], 
                analysis_results['trading_activity']
            )
            
            # Create detailed analysis result
            detailed_analysis = {
                'candidate': candidate,
                'final_score': final_score,
                'scoring_breakdown': scoring_breakdown,
                'overview_data': analysis_results['overview_data'],
                'whale_analysis': analysis_results['whale_analysis'],
                'volume_price_analysis': analysis_results['volume_price_analysis'],
                'community_boost_analysis': analysis_results['community_boost_analysis'],
                'security_analysis': analysis_results['security_analysis'],
                'trading_activity': analysis_results['trading_activity'],
                'analysis_timestamp': datetime.now().isoformat(),
                'scan_id': scan_id,
                'analysis_success_rate': (successful_analyses/total_analyses)*100,
                'analysis_errors': analysis_errors,
                'successful_analyses': successful_analyses,
                'total_analyses': total_analyses
            }
            
            # Log final result
            if successful_analyses >= 3:  # At least half successful
                self.logger.info(f"✅ Detailed analysis completed for {symbol} - Final score: {final_score:.1f}")
            else:
                self.logger.warning(f"⚠️ Detailed analysis partially failed for {symbol} - Final score: {final_score:.1f} (only {successful_analyses}/{total_analyses} analyses successful)")
            
            return detailed_analysis
            
        except Exception as e:
            self.logger.error(f"❌ Critical error in detailed analysis for {symbol} ({address}): {e}")
            self._record_error('detailed_analysis_critical', str(e), 'birdeye', 'perform_detailed_analysis')
            
            # Return partial analysis if any data was collected
            if successful_analyses > 0:
                self.logger.info(f"🔄 Returning partial analysis for {symbol} with {successful_analyses} successful analyses")
                final_score, scoring_breakdown = self._calculate_final_score(
                    candidate, 
                    analysis_results['overview_data'], 
                    analysis_results['whale_analysis'], 
                    analysis_results['volume_price_analysis'],
                    analysis_results['community_boost_analysis'], 
                    analysis_results['security_analysis'], 
                    analysis_results['trading_activity']
                )
                
                return {
                    'candidate': candidate,
                    'final_score': final_score,
                'scoring_breakdown': scoring_breakdown,
                    'overview_data': analysis_results['overview_data'],
                    'whale_analysis': analysis_results['whale_analysis'],
                    'volume_price_analysis': analysis_results['volume_price_analysis'],
                    'community_boost_analysis': analysis_results['community_boost_analysis'],
                    'security_analysis': analysis_results['security_analysis'],
                    'trading_activity': analysis_results['trading_activity'],
                    'analysis_timestamp': datetime.now().isoformat(),
                    'scan_id': scan_id,
                    'analysis_success_rate': (successful_analyses/total_analyses)*100,
                    'analysis_errors': analysis_errors,
                    'successful_analyses': successful_analyses,
                    'total_analyses': total_analyses,
                    'critical_error': str(e)
                }
            
            return None

    async def _get_token_overview_data_enhanced(self, address: str, scan_id: Optional[str] = None) -> Dict[str, Any]:
        """Get comprehensive token overview data with enhanced error logging"""
        try:
            self.logger.debug(f"🔍 Calling Birdeye get_token_overview for {address}")
            
            # Test the API call with detailed logging
            overview = await self.birdeye_api.get_token_overview(address)
            
            if overview is None:
                self.logger.warning(f"⚠️ get_token_overview returned None for {address}")
                self._record_error('token_overview_null', 'API returned None', 'birdeye', 'get_token_overview')
                return {}
            
            if not isinstance(overview, dict):
                self.logger.warning(f"⚠️ get_token_overview returned non-dict for {address}: {type(overview)}")
                self._record_error('token_overview_invalid_type', f'Expected dict, got {type(overview)}', 'birdeye', 'get_token_overview')
                return {}
            
            self.logger.debug(f"✅ get_token_overview returned {len(overview)} fields for {address}")
            
            # Extract and validate data with detailed logging
            extracted_data = {
                'symbol': overview.get('symbol', 'Unknown'),
                'name': overview.get('name', ''),
                'price': overview.get('price', 0),
                'market_cap': overview.get('marketCap', 0),
                'liquidity': overview.get('liquidity', 0),
                'volume_24h': overview.get('volume', {}).get('h24', 0) if isinstance(overview.get('volume'), dict) else 0,
                'price_change_24h': overview.get('priceChange24h', 0),
                'price_change_1h': overview.get('priceChange1h', 0),
                'price_change_5m': overview.get('priceChange5m', 0),
                'holders': overview.get('holders', 0),
                'transactions_24h': overview.get('transactions', {}).get('h24', 0) if isinstance(overview.get('transactions'), dict) else 0,
                'unique_wallets_24h': overview.get('uniqueWallets', {}).get('h24', 0) if isinstance(overview.get('uniqueWallets'), dict) else 0
            }
            
            # Log extracted values for debugging
            if self.is_debug_enabled():  # Enable debug output
                self.logger.debug(f"🔍 Extracted overview data for {address}:")
                for key, value in extracted_data.items():
                    self.logger.debug(f"  • {key}: {value}")
            
            return extracted_data
            
        except Exception as e:
            self.logger.error(f"❌ Error getting token overview for {address}: {e}")
            self._record_error('token_overview_exception', str(e), 'birdeye', 'get_token_overview')
            
            # Try to provide more specific error information
            if "401" in str(e) or "Unauthorized" in str(e):
                self.logger.error(f"🚫 Authentication error in token overview for {address}")
            elif "429" in str(e) or "rate limit" in str(e).lower():
                self.logger.error(f"🚫 Rate limit error in token overview for {address}")
            elif "timeout" in str(e).lower():
                self.logger.error(f"⏰ Timeout error in token overview for {address}")
            
            return {}
        
    async def _get_whale_holder_analysis_enhanced(self, address: str, scan_id: Optional[str] = None) -> Dict[str, Any]:
        """Get whale and holder analysis with enhanced error logging"""
        analysis = {
            'total_holders': 0,
            'whale_concentration': 0,
            'top_10_concentration': 0,
            'smart_money_detected': False,
            'top_traders_count': 0,
            'whale_activity_score': 0
        }
        
        try:
            self.logger.debug(f"🔍 Calling Birdeye get_token_holders for {address}")
            
            holders_data = await self.birdeye_api.get_token_holders(address, limit=50)
            
            if holders_data is None:
                self.logger.warning(f"⚠️ get_token_holders returned None for {address}")
                self._record_error('token_holders_null', 'API returned None', 'birdeye', 'get_token_holders')
                return analysis
            
            if not isinstance(holders_data, dict):
                self.logger.warning(f"⚠️ get_token_holders returned non-dict for {address}: {type(holders_data)}")
                self._record_error('token_holders_invalid_type', f'Expected dict, got {type(holders_data)}', 'birdeye', 'get_token_holders')
                return analysis
            
            self.logger.debug(f"✅ get_token_holders returned data for {address}")
            
            if 'items' in holders_data and isinstance(holders_data['items'], list):
                holders = holders_data['items']
                analysis['total_holders'] = holders_data.get('total', len(holders))
                
                if holders:
                    self.logger.debug(f"📊 Processing {len(holders)} holders for {address}")
                    
                    try:
                        total_supply = sum(float(h.get('uiAmount', 0)) for h in holders)
                        if total_supply > 0:
                            top_10_amount = sum(float(h.get('uiAmount', 0)) for h in holders[:10])
                            analysis['top_10_concentration'] = (top_10_amount / total_supply) * 100
                            
                            whale_amount = sum(
                                float(h.get('uiAmount', 0)) for h in holders
                                if (float(h.get('uiAmount', 0)) / total_supply) > 0.01
                            )
                            analysis['whale_concentration'] = (whale_amount / total_supply) * 100
                            
                            self.logger.debug(f"📊 Whale analysis for {address}: top_10={analysis['top_10_concentration']:.2f}%, whale={analysis['whale_concentration']:.2f}%")
                    except (ValueError, TypeError, ZeroDivisionError) as calc_error:
                        self.logger.warning(f"⚠️ Error calculating whale concentrations for {address}: {calc_error}")
                        self._record_error('whale_calculation_error', str(calc_error), 'birdeye', 'whale_concentration_calc')
                else:
                    self.logger.debug(f"📊 No holders found for {address}")
            else:
                self.logger.warning(f"⚠️ Invalid holders data structure for {address}: missing 'items' field")
                self._record_error('token_holders_invalid_structure', 'Missing items field', 'birdeye', 'get_token_holders')
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"❌ Error in whale/holder analysis for {address}: {e}")
            self._record_error('whale_holder_analysis_exception', str(e), 'birdeye', 'get_token_holders')
            return analysis
            
    async def _get_volume_price_analysis_enhanced(self, address: str, scan_id: Optional[str] = None) -> Dict[str, Any]:
        """Get detailed volume and price analysis with enhanced error logging"""
        analysis = {
            'volume_trend': 'unknown',
            'price_momentum': 'neutral',
            'volatility_score': 0,
            'volume_consistency': 0,
            'recent_volume_spike': False,
            'price_stability': 0
        }
        
        try:
            self.logger.debug(f"🔍 Attempting OHLCV data fetch for {address}")
            
            # Try to get OHLCV data for volume/price analysis
            ohlcv_data = await self.birdeye_api.get_ohlcv_data(address, time_frame='1h')
            
            if ohlcv_data is None:
                self.logger.debug(f"⚠️ OHLCV data returned None for {address}")
                self._record_error('ohlcv_data_null', 'OHLCV API returned None', 'birdeye', 'get_ohlcv_data')
                return analysis
            
            if not isinstance(ohlcv_data, list):
                self.logger.debug(f"⚠️ OHLCV data returned non-list for {address}: {type(ohlcv_data)}")
                self._record_error('ohlcv_data_invalid_type', f'Expected list, got {type(ohlcv_data)}', 'birdeye', 'get_ohlcv_data')
                return analysis
            
            if len(ohlcv_data) == 0:
                self.logger.debug(f"⚠️ OHLCV data returned empty list for {address}")
                return analysis
            
            self.logger.debug(f"✅ OHLCV data fetched for {address}: {len(ohlcv_data)} candles")
            
            # Analyze volume trend (simplified)
            if len(ohlcv_data) >= 2:
                recent_volume = ohlcv_data[-1].get('v', 0)
                previous_volume = ohlcv_data[-2].get('v', 0)
                
                if recent_volume > previous_volume * 1.5:
                    analysis['volume_trend'] = 'increasing'
                    analysis['recent_volume_spike'] = True
                elif recent_volume < previous_volume * 0.5:
                    analysis['volume_trend'] = 'decreasing'
                else:
                    analysis['volume_trend'] = 'stable'
                
                # Price momentum analysis
                recent_close = ohlcv_data[-1].get('c', 0)
                previous_close = ohlcv_data[-2].get('c', 0)
                
                if recent_close > previous_close * 1.02:
                    analysis['price_momentum'] = 'bullish'
                elif recent_close < previous_close * 0.98:
                    analysis['price_momentum'] = 'bearish'
                else:
                    analysis['price_momentum'] = 'neutral'
                
                self.logger.debug(f"📈 Volume/price analysis for {address}: trend={analysis['volume_trend']}, momentum={analysis['price_momentum']}")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"❌ Error in volume/price analysis for {address}: {e}")
            self._record_error('volume_price_analysis_exception', str(e), 'birdeye', 'get_ohlcv_data')
            return analysis
            
    async def _get_community_boost_analysis_enhanced(self, address: str, scan_id: Optional[str] = None) -> Dict[str, Any]:
        """Get community and boosting analysis with enhanced error logging"""
        analysis = {
            'has_website': False,
            'has_twitter': False,
            'has_telegram': False,
            'social_score': 0,
            'community_strength': 'unknown',
            'boost_status': 'none',
            'marketing_activity': 'low'
        }
        
        try:
            self.logger.debug(f"🔍 Getting community data via token overview for {address}")
            
            overview = await self.birdeye_api.get_token_overview(address)
            
            if overview is None:
                self.logger.debug(f"⚠️ Token overview returned None for community analysis of {address}")
                self._record_error('community_overview_null', 'Token overview returned None', 'birdeye', 'get_token_overview')
                return analysis
            
            if not isinstance(overview, dict):
                self.logger.debug(f"⚠️ Token overview returned non-dict for community analysis of {address}")
                self._record_error('community_overview_invalid_type', f'Expected dict, got {type(overview)}', 'birdeye', 'get_token_overview')
                return analysis
            
            self.logger.debug(f"✅ Token overview fetched for community analysis of {address}")
            
            extensions = overview.get('extensions', {})
            if extensions and isinstance(extensions, dict):
                if extensions.get('website'):
                    analysis['has_website'] = True
                    analysis['social_score'] += 2
                    self.logger.debug(f"🌐 Website found for {address}")
                    
                if extensions.get('twitter'):
                    analysis['has_twitter'] = True
                    analysis['social_score'] += 3
                    self.logger.debug(f"🐦 Twitter found for {address}")
                    
                if extensions.get('telegram'):
                    analysis['has_telegram'] = True
                    analysis['social_score'] += 2
                    self.logger.debug(f"📱 Telegram found for {address}")
                    
                if analysis['social_score'] >= 5:
                    analysis['community_strength'] = 'strong'
                elif analysis['social_score'] >= 3:
                    analysis['community_strength'] = 'moderate'
                else:
                    analysis['community_strength'] = 'weak'
                    
                self.logger.debug(f"👥 Community analysis for {address}: score={analysis['social_score']}, strength={analysis['community_strength']}")
            else:
                self.logger.debug(f"📊 No extensions data found for {address}")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"❌ Error in community/boost analysis for {address}: {e}")
            self._record_error('community_boost_analysis_exception', str(e), 'birdeye', 'get_token_overview')
            return analysis
            
    async def _get_security_analysis_enhanced(self, address: str, scan_id: Optional[str] = None) -> Dict[str, Any]:
        """Get security analysis with enhanced error logging"""
        analysis = {
            'is_scam': False,
            'is_risky': False,
            'security_score': 100,
            'risk_factors': [],
            'mint_authority': 'unknown',
            'freeze_authority': 'unknown'
        }
        
        try:
            self.logger.debug(f"🔍 Performing security analysis for {address}")
            
            # Basic security checks (placeholder for now)
            # In a real implementation, this would call security-specific endpoints
            
            # For now, just return default safe values
            self.logger.debug(f"🔒 Security analysis completed for {address} (placeholder)")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"❌ Error in security analysis for {address}: {e}")
            self._record_error('security_analysis_exception', str(e), 'birdeye', 'security_check')
            return analysis
            
    async def _get_trading_activity_analysis_enhanced(self, address: str, scan_id: Optional[str] = None) -> Dict[str, Any]:
        """Get trading activity analysis with enhanced error logging"""
        analysis = {
            'total_transactions': 0,
            'buy_sell_ratio': 0,
            'unique_traders': 0,
            'average_trade_size': 0,
            'trading_frequency': 'low',
            'recent_activity_score': 0
        }
        
        try:
            self.logger.debug(f"🔍 Getting trading activity for {address}")
            
            # Try to get recent transactions
            transactions = await self.birdeye_api.get_token_transactions(address, limit=20)
            
            if transactions is None:
                self.logger.debug(f"⚠️ Token transactions returned None for {address}")
                self._record_error('token_transactions_null', 'Transactions API returned None', 'birdeye', 'get_token_transactions')
                return analysis
            
            if not isinstance(transactions, list):
                self.logger.debug(f"⚠️ Token transactions returned non-list for {address}: {type(transactions)}")
                self._record_error('token_transactions_invalid_type', f'Expected list, got {type(transactions)}', 'birdeye', 'get_token_transactions')
                return analysis
            
            if len(transactions) == 0:
                self.logger.debug(f"📊 No recent transactions found for {address}")
                return analysis
            
            self.logger.debug(f"✅ Trading activity fetched for {address}: {len(transactions)} transactions")
            
            # Analyze transactions
            analysis['total_transactions'] = len(transactions)
            
            buy_count = sum(1 for tx in transactions if tx.get('side') == 'buy')
            sell_count = sum(1 for tx in transactions if tx.get('side') == 'sell')
            
            if sell_count > 0:
                analysis['buy_sell_ratio'] = buy_count / sell_count
            else:
                analysis['buy_sell_ratio'] = float('inf') if buy_count > 0 else 0
            
            # Trading frequency assessment
            if len(transactions) >= 15:
                analysis['trading_frequency'] = 'high'
            elif len(transactions) >= 5:
                analysis['trading_frequency'] = 'medium'
            else:
                analysis['trading_frequency'] = 'low'
            
            analysis['recent_activity_score'] = min(100, len(transactions) * 5)
            
            self.logger.debug(f"💹 Trading analysis for {address}: {len(transactions)} txs, frequency={analysis['trading_frequency']}")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"❌ Error in trading activity analysis for {address}: {e}")
            self._record_error('trading_activity_analysis_exception', str(e), 'birdeye', 'get_token_transactions')
            return analysis

    def _calculate_final_score(self, candidate: Dict[str, Any], overview_data: Dict[str, Any], 
                             whale_analysis: Dict[str, Any], volume_price_analysis: Dict[str, Any],
                             community_boost_analysis: Dict[str, Any], security_analysis: Dict[str, Any], 
                             trading_activity: Dict[str, Any]) -> tuple[float, Dict[str, Any]]:
        """Calculate comprehensive final score based on all analysis components"""
        try:
            # Start with the cross-platform score as base
            base_score = candidate.get('score', 0)
            
            # Initialize component scores
            overview_score = 0
            whale_score = 0
            volume_score = 0
            community_score = 0
            security_score = 0
            trading_score = 0
            
            # Track detailed breakdown for each component
            scoring_breakdown = {
                'base_score': base_score,
                'cross_platform_validation': {
                    'platforms': candidate.get('platforms', []),
                    'platform_count': len(candidate.get('platforms', [])),
                    'validation_bonus': base_score
                },
                'overview_analysis': {
                    'market_cap': overview_data.get('market_cap', 0) if overview_data else 0,
                    'liquidity': overview_data.get('liquidity', 0) if overview_data else 0,
                    'price_change_1h': overview_data.get('price_change_1h', 0) if overview_data else 0,
                    'price_change_24h': overview_data.get('price_change_24h', 0) if overview_data else 0,
                    'holders': overview_data.get('holders', 0) if overview_data else 0,
                    'score': 0,
                    'max_score': 20
                },
                'whale_analysis': {
                    'whale_concentration': whale_analysis.get('whale_concentration', 0) if whale_analysis else 0,
                    'smart_money_detected': whale_analysis.get('smart_money_detected', False) if whale_analysis else False,
                    'score': 0,
                    'max_score': 15
                },
                'volume_price_analysis': {
                    'volume_trend': volume_price_analysis.get('volume_trend', 'unknown') if volume_price_analysis else 'unknown',
                    'price_momentum': volume_price_analysis.get('price_momentum', 'unknown') if volume_price_analysis else 'unknown',
                    'score': 0,
                    'max_score': 15
                },
                'community_analysis': {
                    'social_score': community_boost_analysis.get('social_score', 0) if community_boost_analysis else 0,
                    'community_strength': community_boost_analysis.get('community_strength', 'unknown') if community_boost_analysis else 'unknown',
                    'score': 0,
                    'max_score': 10
                },
                'security_analysis': {
                    'security_score_raw': security_analysis.get('security_score', 100) if security_analysis else 100,
                    'risk_factors': security_analysis.get('risk_factors', []) if security_analysis else [],
                    'risk_factor_count': len(security_analysis.get('risk_factors', [])) if security_analysis else 0,
                    'score': 0,
                    'max_score': 10
                },
                'trading_activity': {
                    'recent_activity_score': trading_activity.get('recent_activity_score', 0) if trading_activity else 0,
                    'buy_sell_ratio': trading_activity.get('buy_sell_ratio', 0) if trading_activity else 0,
                    'transaction_count': trading_activity.get('transaction_count', 0) if trading_activity else 0,
                    'score': 0,
                    'max_score': 10
                }
            }
            
            # Overview data scoring (0-20 points)
            if overview_data:
                # Market cap scoring
                market_cap = overview_data.get('market_cap', 0)
                if market_cap > 1000000:  # > $1M
                    overview_score += 5
                elif market_cap > 100000:  # > $100K
                    overview_score += 3
                elif market_cap > 10000:   # > $10K
                    overview_score += 1
                
                # Liquidity scoring
                liquidity = overview_data.get('liquidity', 0)
                if liquidity > 500000:  # > $500K
                    overview_score += 5
                elif liquidity > 100000:  # > $100K
                    overview_score += 3
                elif liquidity > 10000:   # > $10K
                    overview_score += 1
                
                # Price momentum scoring
                price_change_1h = overview_data.get('price_change_1h', 0)
                price_change_24h = overview_data.get('price_change_24h', 0)
                
                if price_change_1h > 10:
                    overview_score += 3
                elif price_change_1h > 5:
                    overview_score += 2
                elif price_change_1h > 0:
                    overview_score += 1
                
                if price_change_24h > 20:
                    overview_score += 3
                elif price_change_24h > 10:
                    overview_score += 2
                elif price_change_24h > 0:
                    overview_score += 1
                
                # Holders scoring
                holders = overview_data.get('holders', 0)
                if holders > 1000:
                    overview_score += 4
                elif holders > 100:
                    overview_score += 2
                elif holders > 10:
                    overview_score += 1
            
            scoring_breakdown['overview_analysis']['score'] = overview_score
            
            # Whale analysis scoring (0-15 points)
            if whale_analysis:
                # Healthy whale concentration (not too concentrated)
                whale_concentration = whale_analysis.get('whale_concentration', 0)
                if 20 <= whale_concentration <= 60:  # Sweet spot
                    whale_score += 8
                elif 10 <= whale_concentration <= 80:  # Acceptable
                    whale_score += 5
                elif whale_concentration > 0:
                    whale_score += 2
                
                # Smart money detection
                if whale_analysis.get('smart_money_detected', False):
                    whale_score += 7
            
            scoring_breakdown['whale_analysis']['score'] = whale_score
            
            # Volume/Price analysis scoring (0-15 points)
            if volume_price_analysis:
                # Volume trend
                volume_trend = volume_price_analysis.get('volume_trend', 'stable')
                if volume_trend == 'increasing':
                    volume_score += 8
                elif volume_trend == 'stable':
                    volume_score += 4
                
                # Price momentum
                price_momentum = volume_price_analysis.get('price_momentum', 'neutral')
                if price_momentum == 'bullish':
                    volume_score += 7
                elif price_momentum == 'neutral':
                    volume_score += 3
            
            scoring_breakdown['volume_price_analysis']['score'] = volume_score
            
            # Community analysis scoring (0-10 points)
            if community_boost_analysis:
                social_score = community_boost_analysis.get('social_score', 0)
                community_score = min(10, social_score * 1.5)  # Scale to max 10
            
            scoring_breakdown['community_analysis']['score'] = community_score
            
            # Security analysis scoring (0-10 points)
            if security_analysis:
                security_score_raw = security_analysis.get('security_score', 100)
                security_score = (security_score_raw / 100) * 10  # Scale to 0-10
                
                # Deduct for risk factors
                risk_factors = security_analysis.get('risk_factors', [])
                security_score -= len(risk_factors) * 2
                security_score = max(0, security_score)  # Don't go below 0
            
            scoring_breakdown['security_analysis']['score'] = security_score
            
            # Trading activity scoring (0-10 points)
            if trading_activity:
                activity_score = trading_activity.get('recent_activity_score', 0)
                trading_score = min(10, activity_score / 10)  # Scale to max 10
                
                # Bonus for good buy/sell ratio
                buy_sell_ratio = trading_activity.get('buy_sell_ratio', 0)
                if buy_sell_ratio > 1.5:  # More buys than sells
                    trading_score += 3
                elif buy_sell_ratio > 1.0:
                    trading_score += 1
                
                trading_score = min(10, trading_score)  # Cap at 10
            
            scoring_breakdown['trading_activity']['score'] = trading_score
            
            # Calculate final score
            final_score = base_score + overview_score + whale_score + volume_score + community_score + security_score + trading_score
            
            # Cap at 100
            final_score = min(100, final_score)
            
            # Add final score summary to breakdown
            scoring_breakdown['final_score_summary'] = {
                'base_score': base_score,
                'overview_score': overview_score,
                'whale_score': whale_score,
                'volume_score': volume_score,
                'community_score': community_score,
                'security_score': security_score,
                'trading_score': trading_score,
                'total_before_cap': base_score + overview_score + whale_score + volume_score + community_score + security_score + trading_score,
                'final_score': final_score,
                'scoring_breakdown': scoring_breakdown,
                'max_possible_score': 100
            }
            
            self.logger.debug(f"📊 Final score calculation breakdown:")
            self.logger.debug(f"  • Base score: {base_score:.1f}")
            self.logger.debug(f"  • Overview: {overview_score:.1f}")
            self.logger.debug(f"  • Whale: {whale_score:.1f}")
            self.logger.debug(f"  • Volume: {volume_score:.1f}")
            self.logger.debug(f"  • Community: {community_score:.1f}")
            self.logger.debug(f"  • Security: {security_score:.1f}")
            self.logger.debug(f"  • Trading: {trading_score:.1f}")
            self.logger.debug(f"  • FINAL: {final_score:.1f}")
            
            return final_score, scoring_breakdown
            
        except Exception as e:
            self.logger.error(f"❌ Error calculating final score: {e}")
            # Return base score as fallback with minimal breakdown
            fallback_breakdown = {
                'base_score': candidate.get('score', 0),
                'error': str(e),
                'final_score_summary': {
                    'final_score': candidate.get('score', 0)
                }
            }
            return candidate.get('score', 0), fallback_breakdown

    async def _send_detailed_alert(self, detailed_analysis: Dict[str, Any], scan_id: str) -> bool:
        """Send detailed Telegram alert for high-conviction token"""
        if not self.telegram_alerter:
            self.logger.warning("⚠️ Telegram alerter not available")
            return False
            
        try:
            candidate = detailed_analysis['candidate']
            metrics = MinimalTokenMetrics(
                symbol=candidate['symbol'],
                address=candidate['address'],
                name=candidate.get('name', ''),
                price=detailed_analysis['overview_data'].get('price', candidate.get('price', 0)),
                market_cap=detailed_analysis['overview_data'].get('market_cap', candidate.get('market_cap', 0)),
                liquidity=detailed_analysis['overview_data'].get('liquidity', candidate.get('liquidity', 0)),
                volume_24h=detailed_analysis['overview_data'].get('volume_24h', candidate.get('volume_24h', 0)),
                holders=detailed_analysis['overview_data'].get('holders', 0),
                price_change_24h=detailed_analysis['overview_data'].get('price_change_24h', 0),
                score=detailed_analysis['final_score']
            )
            
            enhanced_data = {
                'cross_platform_analysis': {
                    'platforms': candidate.get('platforms', []),
                    'cross_platform_score': candidate.get('cross_platform_score', 0),
                    'boost_data': candidate.get('boost_data', {}),
                    'community_data': candidate.get('community_data', {})
                },
                'whale_analysis': detailed_analysis['whale_analysis'],
                'volume_price_analysis': detailed_analysis['volume_price_analysis'],
                'community_boost_analysis': detailed_analysis['community_boost_analysis'],
                'security_analysis': detailed_analysis['security_analysis'],
                'trading_activity': detailed_analysis['trading_activity']
            }
            
            self.telegram_alerter.send_gem_alert(
                metrics=metrics,
                score=detailed_analysis['final_score'],
                enhanced_data=enhanced_data,
                scan_id=scan_id
            )
            
            self.logger.info(f"📱 Sent detailed alert for {candidate['symbol']} (score: {detailed_analysis['final_score']:.1f})")
            return True
        except Exception as e:
            self.logger.error(f"❌ Error sending detailed alert: {e}")
            return False
        
    async def run_continuous(self, interval_minutes: int = 15):
        """Run continuous detection with specified interval and enhanced reporting"""
        self.logger.info(f"🔄 Starting continuous detection with enhanced reporting (every {interval_minutes} minutes)")
        
        while True:
            try:
                result = await self.run_detection_cycle()
                status = result.get('status', 'unknown')
                if status == 'completed':
                    alerts_sent = result.get('alerts_sent', 0)
                    if alerts_sent > 0:
                        self.logger.info(f"✅ Cycle completed: {alerts_sent} alerts sent")
                    else:
                        self.logger.info("✅ Cycle completed: No new high-conviction tokens found")
                else:
                    self.logger.info(f"✅ Cycle completed with status: {status}")
                    
                self.logger.info(f"⏰ Waiting {interval_minutes} minutes until next cycle...")
                await asyncio.sleep(interval_minutes * 60)
                
            except KeyboardInterrupt:
                self.logger.info("🛑 Received stop signal, shutting down...")
                break
            except Exception as e:
                self.logger.error(f"❌ Error in continuous detection: {e}")
                self._record_error('continuous_detection', str(e), 'detector', 'run_continuous')
                self.logger.info("⏰ Waiting 5 minutes before retry...")
                await asyncio.sleep(300)
                
        self._save_session_results()
        
    async def cleanup(self):
        """Cleanup resources and save final session report"""
        try:
            # Display comprehensive session summary before cleanup
            self._display_session_token_summary()
            
            # Update final health monitoring
            self._update_health_monitoring()
            
            self._save_session_results()
            
            if self.cross_platform_analyzer and hasattr(self.cross_platform_analyzer, 'birdeye'):
                await self.cross_platform_analyzer.birdeye.close()
                
            if self.birdeye_api:
                await self.birdeye_api.close()
                
            if self.telegram_alerter:
                await self.telegram_alerter.close()
                
            self.logger.info("✅ Enhanced cleanup completed with comprehensive session reporting")
        except Exception as e:
            self.logger.error(f"❌ Error during cleanup: {e}")
    
    # ==================== ENHANCED TOKEN REGISTRY METHODS ====================
    
    def _record_scan_tokens(self, result: Dict[str, Any], scan_number: int):
        """Record all tokens discovered and analyzed in this scan with enhanced tracking"""
        try:
            scan_tokens = []
            
            # PRIORITY 1: Use updated candidates from detailed analysis if available
            detailed_analyses = result.get('detailed_analyses_data', [])
            analyzed_addresses = set()
            
            for detailed_analysis in detailed_analyses:
                if detailed_analysis and 'candidate' in detailed_analysis:
                    candidate = detailed_analysis['candidate']
                    address = candidate.get('address')
                    
                    if address and address not in analyzed_addresses:
                        analyzed_addresses.add(address)
                        
                        # FIX: Use the updated symbol and name from the candidate (which was updated in detailed analysis)
                        token_info = {
                            'address': address,
                            'symbol': candidate.get('symbol', 'Unknown'),  # This should now have the updated Birdeye symbol
                            'name': candidate.get('name', ''),  # This should now have the updated Birdeye name
                            'score': detailed_analysis.get('final_score', candidate.get('cross_platform_score', 0)),
                            'platforms': candidate.get('platforms', []),
                            'price': candidate.get('price', 0),
                            'volume_24h': candidate.get('volume_24h', 0),
                            'market_cap': candidate.get('market_cap', 0),
                            'liquidity': candidate.get('liquidity', 0),
                            'scan_number': scan_number,
                            'timestamp': datetime.now().isoformat(),
                            'source_breakdown': self._analyze_token_sources(candidate),
                            'detailed_analyzed': True  # Mark as having detailed analysis
                        }
                        
                        scan_tokens.append(token_info)
                        self._update_session_registry(token_info)
            
            # PRIORITY 2: Add high conviction candidates that weren't detailed analyzed
            high_conviction_candidates = result.get('high_conviction_candidates_data', [])
            for candidate in high_conviction_candidates:
                if isinstance(candidate, dict) and 'address' in candidate:
                    address = candidate['address']
                    
                    # Skip if already added from detailed analysis
                    if address not in analyzed_addresses:
                        token_info = {
                            'address': address,
                            'symbol': candidate.get('symbol', 'Unknown'),
                            'name': candidate.get('name', ''),
                            'score': candidate.get('cross_platform_score', 0),
                            'platforms': candidate.get('platforms', []),
                            'price': candidate.get('price', 0),
                            'volume_24h': candidate.get('volume_24h', 0),
                            'market_cap': candidate.get('market_cap', 0),
                            'liquidity': candidate.get('liquidity', 0),
                            'scan_number': scan_number,
                            'timestamp': datetime.now().isoformat(),
                            'source_breakdown': {'platforms': candidate.get('platforms', [])},
                            'detailed_analyzed': False
                        }
                        scan_tokens.append(token_info)
                        self._update_session_registry(token_info)
            
            # PRIORITY 3: Extract remaining tokens from cross-platform analysis results if needed
            if hasattr(self, 'cross_platform_analyzer'):
                try:
                    # Get cross-platform results from the detector's last run
                    cross_platform_data = getattr(self.cross_platform_analyzer, '_last_analysis_results', None)
                    
                    if not cross_platform_data:
                        # If no cached results, we'll extract from result structure
                        cross_platform_data = result.get('cross_platform_results', {})
                    
                    if cross_platform_data and 'correlations' in cross_platform_data:
                        # Extract tokens from normalized data
                        all_tokens = cross_platform_data['correlations'].get('all_tokens', {})
                        
                        for address, token_data in all_tokens.items():
                            if not address or address in ['', 'unknown'] or address in analyzed_addresses:
                                continue
                                
                            token_info = {
                                'address': address,
                                'symbol': token_data.get('symbol', 'Unknown'),
                                'name': token_data.get('name', ''),
                                'score': token_data.get('score', 0),
                                'platforms': token_data.get('platforms', []),
                                'price': token_data.get('price', 0),
                                'volume_24h': token_data.get('volume_24h', 0),
                                'market_cap': token_data.get('market_cap', 0),
                                'liquidity': token_data.get('liquidity', 0),
                                'scan_number': scan_number,
                                'timestamp': datetime.now().isoformat(),
                                'source_breakdown': self._analyze_token_sources(token_data),
                                'detailed_analyzed': False
                            }
                            
                            scan_tokens.append(token_info)
                            
                            # Update session registry
                            self._update_session_registry(token_info)
                            
                except Exception as e:
                    if self.is_debug_enabled():  # Enable debug output
                        self.logger.debug(f"🔍 DEBUG: Could not extract cross-platform tokens: {e}")
            
            # Store tokens for this scan
            self.session_token_registry['all_tokens_by_scan'][scan_number] = scan_tokens
            
            # Update session summary
            self._update_session_summary()
            
            if self.is_debug_enabled():  # Enable debug output
                self.logger.info(f"📊 SCAN #{scan_number} TOKEN REGISTRY:")
                self.logger.info(f"  🎯 Tokens Discovered: {len(scan_tokens)}")
                if scan_tokens:
                    self.logger.info(f"  📈 Score Range: {min(t['score'] for t in scan_tokens):.1f} - {max(t['score'] for t in scan_tokens):.1f}")
                    
                    # Show top 3 tokens by score
                    top_tokens = sorted(scan_tokens, key=lambda x: x['score'], reverse=True)[:3]
                    for i, token in enumerate(top_tokens, 1):
                        platforms_str = ', '.join(token['platforms']) if token['platforms'] else 'Unknown'
                        detailed_marker = " ✨" if token.get('detailed_analyzed', False) else ""
                        self.logger.info(f"  {i}. {token['symbol']} ({token['address'][:8]}...) - Score: {token['score']:.1f} - Platforms: {platforms_str}{detailed_marker}")
                else:
                    self.logger.info(f"  ℹ️  No tokens discovered in this scan")
                    
        except Exception as e:
            self.logger.error(f"❌ Error recording scan tokens: {e}")
            if self.is_debug_enabled():  # Enable debug output
                import traceback
                self.logger.debug(f"🔍 DEBUG: {traceback.format_exc()}")
    
    def _analyze_token_sources(self, token_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze which sources contributed to token discovery"""
        sources = {
            'platforms': token_data.get('platforms', []),
            'rugcheck_data': 'rugcheck' in token_data.get('platforms', []),
            'dexscreener_data': 'dexscreener' in token_data.get('platforms', []),
            'birdeye_data': 'birdeye' in token_data.get('platforms', []),
            'multi_platform': len(token_data.get('platforms', [])) > 1
        }
        return sources
    
    def _update_session_registry(self, token_info: Dict[str, Any]):
        """Update the session-wide token registry with enhanced tracking"""
        address = token_info['address']
        
        # Update unique tokens discovered
        if address not in self.session_token_registry['unique_tokens_discovered']:
            self.session_token_registry['unique_tokens_discovered'][address] = token_info
            
            # Track sources
            self.session_token_registry['token_sources'][address] = token_info['platforms']
            
            # Update source counts
            for platform in token_info['platforms']:
                if platform in self.session_token_registry['session_summary']['tokens_by_source']:
                    self.session_token_registry['session_summary']['tokens_by_source'][platform] += 1
        
        # Update score tracking (always update to track progression)
        if address not in self.session_token_registry['token_scores']:
            self.session_token_registry['token_scores'][address] = []
        
        self.session_token_registry['token_scores'][address].append({
            'score': token_info['score'],
            'scan_number': token_info['scan_number'],
            'timestamp': token_info['timestamp']
        })
        
        # Track high conviction tokens
        if token_info['score'] >= self.high_conviction_threshold:
            self.session_token_registry['high_conviction_tokens'][address] = token_info
        
        # Track cross-platform validated tokens
        if len(token_info['platforms']) > 1:
            self.session_token_registry['cross_platform_validated_tokens'][address] = token_info
    
    def _update_session_summary(self):
        """Update session-wide summary statistics with enhanced metrics"""
        summary = self.session_token_registry['session_summary']
        
        # Update total unique tokens
        summary['total_unique_tokens'] = len(self.session_token_registry['unique_tokens_discovered'])
        
        # Update multi-platform tokens count
        summary['multi_platform_tokens'] = len(self.session_token_registry['cross_platform_validated_tokens'])
        
        # Update score distribution
        score_dist = {'0-2': 0, '2-4': 0, '4-6': 0, '6-8': 0, '8-10': 0}
        
        for token_data in self.session_token_registry['unique_tokens_discovered'].values():
            score = token_data['score']
            if score < 2:
                score_dist['0-2'] += 1
            elif score < 4:
                score_dist['2-4'] += 1
            elif score < 6:
                score_dist['4-6'] += 1
            elif score < 8:
                score_dist['6-8'] += 1
            else:
                score_dist['8-10'] += 1
        
        summary['score_distribution'] = score_dist
        
        # Analyze score progression
        progression_analysis = {}
        for address, score_history in self.session_token_registry['token_scores'].items():
            if len(score_history) > 1:
                first_score = score_history[0]['score']
                last_score = score_history[-1]['score']
                improvement = last_score - first_score
                
                if improvement > 0.5:
                    progression_analysis[address] = {
                        'type': 'improving',
                        'improvement': improvement,
                        'scans': len(score_history)
                    }
                elif improvement < -0.5:
                    progression_analysis[address] = {
                        'type': 'declining',
                        'decline': abs(improvement),
                        'scans': len(score_history)
                    }
                else:
                    progression_analysis[address] = {
                        'type': 'stable',
                        'variance': improvement,
                        'scans': len(score_history)
                    }
        
        summary['score_progression_analysis'] = progression_analysis
    
    def _display_session_token_summary(self):
        """Display comprehensive session token registry summary using formatted tables"""
        try:
            # Import prettytable here if not available globally
            try:
                from prettytable import PrettyTable
            except ImportError:
                # Fallback to original formatting if prettytable not available
                self._display_session_token_summary_fallback()
                return
            
            self.logger.info("\n" + "-" * 60)
            self.logger.info("COMPREHENSIVE TOKEN REGISTRY - SESSION SUMMARY")
            self.logger.info("-" * 60)
            
            registry = self.session_token_registry
            summary = registry['session_summary']
            
            # Overall statistics table
            self.logger.info(f"\n📊 OVERALL TOKEN STATISTICS:")
            
            overview_table = PrettyTable()
            overview_table.field_names = ["Metric", "Count", "Percentage"]
            overview_table.align = "l"
            
            total_tokens = summary['total_unique_tokens']
            multi_platform = summary['multi_platform_tokens']
            
            overview_table.add_row([
                "🎯 Total Unique Tokens", 
                str(total_tokens), 
                "100.0%"
            ])
            
            overview_table.add_row([
                "🔗 Cross-Platform Validated", 
                str(multi_platform), 
                f"{(multi_platform/max(total_tokens, 1)*100):.1f}%"
            ])
            
            # Add source breakdown
            for source, count in summary['tokens_by_source'].items():
                if count > 0:
                    overview_table.add_row([
                        f"📡 {source.title()} Source", 
                        str(count), 
                        f"{(count/max(total_tokens, 1)*100):.1f}%"
                    ])
            
            # Print overview table
            self.logger.info(f"\n{overview_table}")
            
            # Score distribution table
            self.logger.info(f"\n📈 SCORE DISTRIBUTION:")
            
            score_table = PrettyTable()
            score_table.field_names = ["Score Range", "Token Count", "Percentage"]
            score_table.align = "l"
            
            for score_range, count in summary['score_distribution'].items():
                if count > 0:
                    percentage = (count / max(total_tokens, 1)) * 100
                    score_table.add_row([
                        f"📊 {score_range}",
                        str(count),
                        f"{percentage:.1f}%"
                    ])
            
            # Print score table
            self.logger.info(f"\n{score_table}")
            
            # High conviction tokens table
            high_conviction_count = len(registry['high_conviction_tokens'])
            if high_conviction_count > 0:
                self.logger.info(f"\n🎯 HIGH CONVICTION TOKENS ({high_conviction_count}):")
                
                hc_table = PrettyTable()
                hc_table.field_names = ["Rank", "Symbol", "Score", "Platforms", "Address"]
                hc_table.align = "l"
                
                sorted_hc_tokens = sorted(
                    registry['high_conviction_tokens'].items(),
                    key=lambda x: x[1]['score'],
                    reverse=True
                )
                
                for i, (address, token) in enumerate(sorted_hc_tokens[:10], 1):  # Top 10
                    platforms_str = ', '.join(token['platforms']) if token['platforms'] else 'Unknown'
                    if len(platforms_str) > 20:
                        platforms_str = platforms_str[:17] + "..."
                    
                    hc_table.add_row([
                        f"{i}.",
                        token['symbol'][:12],  # Truncate long symbols
                        f"{token['score']:.1f}",
                        platforms_str,
                        f"{address[:8]}..."
                    ])
                
                # Print high conviction table
                self.logger.info(f"\n{hc_table}")
            else:
                self.logger.info(f"\n🎯 HIGH CONVICTION TOKENS: None found")
            
            # Cross-platform validated tokens table
            cross_platform_tokens = list(registry['cross_platform_validated_tokens'].values())
            if cross_platform_tokens:
                self.logger.info(f"\n🔗 CROSS-PLATFORM VALIDATED TOKENS ({len(cross_platform_tokens)}):")
                
                cp_table = PrettyTable()
                cp_table.field_names = ["Rank", "Symbol", "Score", "Platforms", "Address"]
                cp_table.align = "l"
                
                cross_platform_tokens.sort(key=lambda x: x['score'], reverse=True)
                
                for i, token in enumerate(cross_platform_tokens[:10], 1):  # Top 10
                    platforms_str = ', '.join(token['platforms'])
                    if len(platforms_str) > 20:
                        platforms_str = platforms_str[:17] + "..."
                    
                    cp_table.add_row([
                        f"{i}.",
                        token['symbol'][:12],  # Truncate long symbols
                        f"{token['score']:.1f}",
                        platforms_str,
                        f"{token['address'][:8]}..."
                    ])
                
                # Print cross-platform table
                self.logger.info(f"\n{cp_table}")
            else:
                self.logger.info(f"\n🔗 CROSS-PLATFORM VALIDATED TOKENS: None found")
            
            # Score progression summary
            progression = summary.get('score_progression_analysis', {})
            if progression:
                improving = [addr for addr, data in progression.items() if data['type'] == 'improving']
                declining = [addr for addr, data in progression.items() if data['type'] == 'declining']
                stable = [addr for addr, data in progression.items() if data['type'] == 'stable']
                
                self.logger.info(f"\n📈 SCORE PROGRESSION ANALYSIS:")
                
                prog_table = PrettyTable()
                prog_table.field_names = ["Progression Type", "Token Count", "Best Example"]
                prog_table.align = "l"
                
                # Show best improving token
                best_example = "None"
                if improving:
                    best_improvement = max(progression.items(), 
                                         key=lambda x: x[1].get('improvement', 0) if x[1]['type'] == 'improving' else 0)
                    if best_improvement[1]['type'] == 'improving':
                        token = registry['unique_tokens_discovered'].get(best_improvement[0], {})
                        improvement = best_improvement[1]['improvement']
                        best_example = f"{token.get('symbol', 'Unknown')} (+{improvement:.1f})"
                
                prog_table.add_row(["⬆️ Improving", str(len(improving)), best_example])
                prog_table.add_row(["⬇️ Declining", str(len(declining)), ""])
                prog_table.add_row(["➡️ Stable", str(len(stable)), ""])
                
                # Print progression table
                self.logger.info(f"\n{prog_table}")
            
            # Save detailed token registry
            token_registry_file = f"data/token_registry_{int(time.time())}.json"
            try:
                with open(token_registry_file, 'w') as f:
                    json.dump(registry, f, indent=2, default=str)
                self.logger.info(f"\n💾 Complete token registry saved to: {token_registry_file}")
            except Exception as e:
                self.logger.error(f"\n❌ Error saving token registry: {e}")
            
            self.logger.info("-" * 60)
            
        except Exception as e:
            self.logger.error(f"❌ Error displaying session token summary: {e}")
            # Fallback to original method
            self._display_session_token_summary_fallback()
    
    def _display_session_token_summary_fallback(self):
        """Fallback session token summary without prettytable"""
        self.logger.info("\n" + "-" * 60)
        self.logger.info("COMPREHENSIVE TOKEN REGISTRY - SESSION SUMMARY")
        self.logger.info("-" * 60)
        
        registry = self.session_token_registry
        summary = registry['session_summary']
        
        # Overall statistics
        self.logger.info(f"\n📊 OVERALL TOKEN STATISTICS:")
        self.logger.info(f"  🎯 Total Unique Tokens Discovered: {summary['total_unique_tokens']}")
        self.logger.info(f"  🔗 Cross-Platform Validated Tokens: {summary['multi_platform_tokens']}")
        self.logger.info(f"  📈 Tokens by Source:")
        for source, count in summary['tokens_by_source'].items():
            if count > 0:
                self.logger.info(f"    • {source.title()}: {count} tokens")
        
        # Score distribution
        self.logger.info(f"\n📈 SCORE DISTRIBUTION:")
        for score_range, count in summary['score_distribution'].items():
            if count > 0:
                self.logger.info(f"  📊 Score {score_range}: {count} tokens")
        
        # High conviction tokens
        high_conviction_count = len(registry['high_conviction_tokens'])
        if high_conviction_count > 0:
            self.logger.info(f"\n🎯 HIGH CONVICTION TOKENS ({high_conviction_count}):")
            for address, token in list(registry['high_conviction_tokens'].items())[:10]:  # Show top 10
                platforms_str = ', '.join(token['platforms']) if token['platforms'] else 'Unknown'
                self.logger.info(f"  🚀 {token['symbol']} ({address[:8]}...) - Score: {token['score']:.1f} - Platforms: {platforms_str}")
        else:
            self.logger.info(f"\n🎯 HIGH CONVICTION TOKENS: None found")
        
        # Cross-platform validated tokens
        cross_platform_tokens = list(registry['cross_platform_validated_tokens'].values())
        if cross_platform_tokens:
            self.logger.info(f"\n🔗 CROSS-PLATFORM VALIDATED TOKENS ({len(cross_platform_tokens)}):")
            cross_platform_tokens.sort(key=lambda x: x['score'], reverse=True)
            for token in cross_platform_tokens[:10]:  # Show top 10
                platforms_str = ', '.join(token['platforms'])
                self.logger.info(f"  ✅ {token['symbol']} ({token['address'][:8]}...) - Score: {token['score']:.1f} - Platforms: {platforms_str}")
        else:
            self.logger.info(f"\n🔗 CROSS-PLATFORM VALIDATED TOKENS: None found")
        
        # Score progression analysis
        progression = summary.get('score_progression_analysis', {})
        if progression:
            improving = [addr for addr, data in progression.items() if data['type'] == 'improving']
            declining = [addr for addr, data in progression.items() if data['type'] == 'declining']
            stable = [addr for addr, data in progression.items() if data['type'] == 'stable']
            
            self.logger.info(f"\n📈 SCORE PROGRESSION ANALYSIS:")
            self.logger.info(f"  ⬆️  Improving Tokens: {len(improving)}")
            self.logger.info(f"  ⬇️  Declining Tokens: {len(declining)}")
            self.logger.info(f"  ➡️  Stable Tokens: {len(stable)}")
            
            # Show best improving token
            if improving:
                best_improvement = max(progression.items(), key=lambda x: x[1].get('improvement', 0) if x[1]['type'] == 'improving' else 0)
                if best_improvement[1]['type'] == 'improving':
                    addr = best_improvement[0]
                    token = registry['unique_tokens_discovered'].get(addr, {})
                    improvement = best_improvement[1]['improvement']
                    self.logger.info(f"    🏆 Best Improving: {token.get('symbol', 'Unknown')} (+{improvement:.1f} points)")
        
        # Save detailed token registry
        token_registry_file = f"data/token_registry_{int(time.time())}.json"
        try:
            with open(token_registry_file, 'w') as f:
                json.dump(registry, f, indent=2, default=str)
            self.logger.info(f"\n💾 Complete token registry saved to: {token_registry_file}")
        except Exception as e:
            self.logger.error(f"\n❌ Error saving token registry: {e}")
        
        self.logger.info("-" * 60)
    
    # ==================== ENHANCED HEALTH MONITORING METHODS ====================
    
    def _update_health_monitoring(self):
        """Update real-time health monitoring and system stability metrics"""
        try:
            health_monitoring = self.session_stats['health_monitoring']
            
            # Calculate service health scores
            service_health_scores = {}
            overall_health_score = 0
            active_services = 0
            
            for service_name, service_stats in self.session_stats['api_usage_by_service'].items():
                total_calls = service_stats.get('total_calls', 0)
                if total_calls > 0:
                    successful_calls = service_stats.get('successful_calls', 0)
                    failed_calls = service_stats.get('failed_calls', 0)
                    consecutive_failures = service_stats.get('consecutive_failures', 0)
                    
                    # Calculate health score (0-100)
                    success_rate = (successful_calls / total_calls) * 100
                    failure_penalty = min(consecutive_failures * 10, 50)  # Max 50 point penalty
                    health_score = max(0, success_rate - failure_penalty)
                    
                    service_health_scores[service_name] = health_score
                    overall_health_score += health_score
                    active_services += 1
                    
                    # Update service health status
                    if health_score >= 90:
                        service_stats['health_status'] = 'excellent'
                    elif health_score >= 75:
                        service_stats['health_status'] = 'good'
                    elif health_score >= 50:
                        service_stats['health_status'] = 'fair'
                    elif health_score >= 25:
                        service_stats['health_status'] = 'poor'
                    else:
                        service_stats['health_status'] = 'critical'
            
            # Calculate overall health status
            if active_services > 0:
                avg_health_score = overall_health_score / active_services
                health_monitoring['api_reliability_score'] = avg_health_score
                
                if avg_health_score >= 90:
                    health_monitoring['overall_health_status'] = 'excellent'
                elif avg_health_score >= 75:
                    health_monitoring['overall_health_status'] = 'good'
                elif avg_health_score >= 50:
                    health_monitoring['overall_health_status'] = 'fair'
                elif avg_health_score >= 25:
                    health_monitoring['overall_health_status'] = 'poor'
                else:
                    health_monitoring['overall_health_status'] = 'critical'
            else:
                health_monitoring['overall_health_status'] = 'no_activity'
                health_monitoring['api_reliability_score'] = 0
            
            health_monitoring['service_health_scores'] = service_health_scores
            
            # Calculate system stability score
            error_rate = 0
            total_cycles = self.session_stats['performance_metrics']['total_cycles']
            if total_cycles > 0:
                total_errors = self.session_stats['error_analysis']['total_errors']
                error_rate = (total_errors / total_cycles) * 100
            
            stability_score = max(0, 100 - error_rate)
            health_monitoring['system_stability_score'] = stability_score
            
            # Generate performance alerts
            self._generate_performance_alerts()
            
            # Identify optimization opportunities
            self._identify_optimization_opportunities()
            
        except Exception as e:
            self.logger.error(f"❌ Error updating health monitoring: {e}")
            self._record_error('health_monitoring_update', str(e), 'system')
    
    def _generate_performance_alerts(self):
        """Generate performance alerts based on current metrics"""
        alerts = []
        health_monitoring = self.session_stats['health_monitoring']
        
        # Check API reliability
        api_reliability = health_monitoring.get('api_reliability_score', 0)
        if api_reliability < 50:
            alerts.append({
                'type': 'critical',
                'message': f'API reliability critically low: {api_reliability:.1f}%',
                'timestamp': datetime.now().isoformat(),
                'recommendation': 'Check API configurations and network connectivity'
            })
        elif api_reliability < 75:
            alerts.append({
                'type': 'warning',
                'message': f'API reliability below optimal: {api_reliability:.1f}%',
                'timestamp': datetime.now().isoformat(),
                'recommendation': 'Monitor API error patterns and consider rate limit adjustments'
            })
        
        # Check system stability
        stability_score = health_monitoring.get('system_stability_score', 0)
        if stability_score < 50:
            alerts.append({
                'type': 'critical',
                'message': f'System stability critically low: {stability_score:.1f}%',
                'timestamp': datetime.now().isoformat(),
                'recommendation': 'Review error logs and implement error handling improvements'
            })
        
        # Check for consecutive failures
        for service_name, service_stats in self.session_stats['api_usage_by_service'].items():
            consecutive_failures = service_stats.get('consecutive_failures', 0)
            if consecutive_failures >= 5:
                alerts.append({
                    'type': 'warning',
                    'message': f'{service_name} has {consecutive_failures} consecutive failures',
                    'timestamp': datetime.now().isoformat(),
                    'recommendation': f'Check {service_name} API status and credentials'
                })
        
        health_monitoring['performance_alerts'] = alerts
        
        # Log critical alerts
        for alert in alerts:
            if alert['type'] == 'critical':
                self.logger.error(f"🚨 CRITICAL ALERT: {alert['message']}")
            elif alert['type'] == 'warning':
                self.logger.warning(f"⚠️  WARNING: {alert['message']}")
    
    def _identify_optimization_opportunities(self):
        """Identify optimization opportunities based on performance data"""
        opportunities = []
        
        # Check cache performance
        for service_name, service_stats in self.session_stats['api_usage_by_service'].items():
            total_calls = service_stats.get('total_calls', 0)
            if total_calls > 10:  # Only analyze services with significant usage
                # Check response times
                avg_response_time = service_stats.get('avg_response_time_ms', 0)
                if avg_response_time > 2000:  # Slower than 2 seconds
                    opportunities.append({
                        'type': 'performance',
                        'service': service_name,
                        'issue': 'slow_response_times',
                        'message': f'{service_name} average response time is {avg_response_time:.0f}ms',
                        'recommendation': 'Consider implementing request batching or optimizing API calls'
                    })
                
                # Check failure rates
                success_rate = (service_stats.get('successful_calls', 0) / total_calls) * 100
                if success_rate < 95:
                    opportunities.append({
                        'type': 'reliability',
                        'service': service_name,
                        'issue': 'high_failure_rate',
                        'message': f'{service_name} success rate is {success_rate:.1f}%',
                        'recommendation': 'Implement retry logic and better error handling'
                    })
        
        # Check token discovery efficiency
        total_cycles = self.session_stats['performance_metrics']['total_cycles']
        unique_tokens = self.session_stats['performance_metrics']['unique_tokens']
        if total_cycles > 5 and unique_tokens > 0:
            discovery_rate = unique_tokens / total_cycles
            if discovery_rate < 1:  # Less than 1 unique token per cycle
                opportunities.append({
                    'type': 'efficiency',
                    'service': 'token_discovery',
                    'issue': 'low_discovery_rate',
                    'message': f'Token discovery rate is {discovery_rate:.2f} tokens per cycle',
                    'recommendation': 'Consider adjusting cross-platform score thresholds or expanding data sources'
                })
        
        self.session_stats['health_monitoring']['optimization_opportunities'] = opportunities

    def _print_comprehensive_scan_summary(self, cycle_results: Dict[str, Any]):
        """Print comprehensive scan summary with enhanced tracking information"""
        try:
            cycle_number = self.session_stats['performance_metrics']['total_cycles']
            scan_id = cycle_results.get('scan_id', 'unknown')
            
            # Header with clean ASCII border
            self.logger.info("\n" + "=" * 80)
            self.logger.info(f"COMPREHENSIVE SCAN SUMMARY - CYCLE #{cycle_number}")
            self.logger.info(f"Scan ID: {scan_id}")
            self.logger.info(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            self.logger.info("=" * 80)
            
            # Cycle Performance Metrics
            duration = cycle_results.get('cycle_duration_seconds', 0)
            status = cycle_results.get('status', 'unknown')
            
            self.logger.info(f"\n📊 CYCLE PERFORMANCE:")
            self.logger.info(f"  ⏱️  Duration: {duration:.1f}s")
            self.logger.info(f"  ✅ Status: {status.upper()}")
            self.logger.info(f"  🎯 Tokens Analyzed: {cycle_results.get('total_analyzed', 0)}")
            self.logger.info(f"  🚀 High Conviction Candidates: {cycle_results.get('high_conviction_candidates', 0)}")
            self.logger.info(f"  🆕 New Candidates: {cycle_results.get('new_candidates', 0)}")
            self.logger.info(f"  📋 Detailed Analyses: {cycle_results.get('detailed_analyses', 0)}")
            self.logger.info(f"  📢 Alerts Sent: {cycle_results.get('alerts_sent', 0)}")
            
            # Token Discovery Summary
            self._print_token_discovery_summary()
            
            # API Health Status
            self._print_api_health_summary()
            
            # Session Statistics
            self._print_session_statistics_summary()
            
            # Performance Alerts (if any)
            self._print_performance_alerts()
            
            # Enhanced Debug Information (if debug mode)
            if self.is_debug_enabled():  # Enable debug output
                self._print_debug_summary(cycle_results)
            
            # Footer with clean ASCII border
            self.logger.info("=" * 80)
            self.logger.info(f"END SCAN #{cycle_number} SUMMARY")
            self.logger.info("=" * 80 + "\n")
            
        except Exception as e:
            self.logger.error(f"❌ Error printing comprehensive scan summary: {e}")
    
    def _print_token_discovery_summary(self):
        """Print token discovery summary from session stats using formatted tables"""
        try:
            # Import prettytable here if not available globally
            try:
                from prettytable import PrettyTable
            except ImportError:
                # Fallback to original formatting if prettytable not available
                self._print_token_discovery_summary_fallback()
                return
            
            tokens_discovered = self.session_stats['tokens_discovered']
            total_unique = len(tokens_discovered)
            
            self.logger.info(f"\n📊 TOKEN DISCOVERY SUMMARY:")
            self.logger.info(f"  📊 Total Unique Tokens: {total_unique}")
            
            if total_unique > 0:
                # High conviction tokens
                high_conviction = [t for t in tokens_discovered.values() 
                                 if t.get('best_conviction_score', 0) >= self.high_conviction_threshold]
                self.logger.info(f"  🎯 High Conviction Tokens: {len(high_conviction)}")
                
                # Cross-platform validated tokens
                cross_platform = [t for t in tokens_discovered.values() 
                                if len(t.get('platforms', [])) > 1]
                self.logger.info(f"  🔗 Cross-Platform Validated: {len(cross_platform)}")
                
                # Recent discoveries (last 3 cycles)
                recent_discoveries = [t for t in tokens_discovered.values() 
                                    if t.get('times_detected', 0) <= 3]
                self.logger.info(f"  🆕 Recent Discoveries: {len(recent_discoveries)}")
                
                # Show top tokens in a formatted table
                top_tokens = sorted(tokens_discovered.values(), 
                                  key=lambda x: x.get('best_conviction_score', 0), 
                                  reverse=True)[:10]  # Show top 10
                
                if top_tokens:
                    self.logger.info(f"  🏆 TOP TOKENS BY CONVICTION SCORE:")
                    
                    # Create formatted table
                    table = PrettyTable()
                    table.field_names = ["Rank", "Symbol", "Score", "Platforms", "Detected"]
                    table.align = "l"
                    
                    for i, token in enumerate(top_tokens, 1):
                        symbol = token.get('symbol', 'Unknown')
                        score = token.get('best_conviction_score', 0)
                        platforms = ', '.join(token.get('platforms', []))
                        times_detected = token.get('times_detected', 0)
                        
                        # Truncate long platform lists
                        if len(platforms) > 25:
                            platforms = platforms[:22] + "..."
                        
                        table.add_row([
                            f"{i}.",
                            symbol[:15],  # Limit symbol length
                            f"{score:.1f}",
                            platforms,
                            f"{times_detected}x"
                        ])
                    
                    # Print complete table as single string
                    self.logger.info(f"\n{table}")
            else:
                self.logger.info(f"  ℹ️  No tokens discovered yet")
                
        except Exception as e:
            self.logger.error(f"❌ Error printing token discovery summary: {e}")
            # Fallback to original method
            self._print_token_discovery_summary_fallback()
    
    def _print_token_discovery_summary_fallback(self):
        """Fallback token discovery summary without prettytable"""
        try:
            tokens_discovered = self.session_stats['tokens_discovered']
            total_unique = len(tokens_discovered)
            
            self.logger.info(f"\n📊 TOKEN DISCOVERY SUMMARY:")
            self.logger.info(f"  📊 Total Unique Tokens: {total_unique}")
            
            if total_unique > 0:
                # High conviction tokens
                high_conviction = [t for t in tokens_discovered.values() 
                                 if t.get('best_conviction_score', 0) >= self.high_conviction_threshold]
                self.logger.info(f"  🎯 High Conviction Tokens: {len(high_conviction)}")
                
                # Cross-platform validated tokens
                cross_platform = [t for t in tokens_discovered.values() 
                                if len(t.get('platforms', [])) > 1]
                self.logger.info(f"  🔗 Cross-Platform Validated: {len(cross_platform)}")
                
                # Recent discoveries (last 3 cycles)
                recent_discoveries = [t for t in tokens_discovered.values() 
                                    if t.get('times_detected', 0) <= 3]
                self.logger.info(f"  🆕 Recent Discoveries: {len(recent_discoveries)}")
                
                # Show top 3 tokens by conviction score
                top_tokens = sorted(tokens_discovered.values(), 
                                  key=lambda x: x.get('best_conviction_score', 0), 
                                  reverse=True)[:3]
                
                if top_tokens:
                    self.logger.info(f"  🏆 TOP TOKENS BY CONVICTION SCORE:")
                    for i, token in enumerate(top_tokens, 1):
                        symbol = token.get('symbol', 'Unknown')
                        score = token.get('best_conviction_score', 0)
                        platforms = ', '.join(token.get('platforms', []))
                        times_detected = token.get('times_detected', 0)
                        self.logger.info(f"    {i}. {symbol} - Score: {score:.1f} - Platforms: {platforms} - Detected: {times_detected}x")
            else:
                self.logger.info(f"  ℹ️  No tokens discovered yet")
                
        except Exception as e:
            self.logger.error(f"❌ Error printing fallback token discovery summary: {e}")
    
    def _print_api_health_summary(self):
        """Print API health and performance summary using formatted tables"""
        try:
            # Import prettytable here if not available globally
            try:
                from prettytable import PrettyTable
            except ImportError:
                # Fallback to original formatting if prettytable not available
                self._print_api_health_summary_fallback()
                return
            
            api_usage = self.session_stats['api_usage_by_service']
            health_monitoring = self.session_stats['health_monitoring']
            
            self.logger.info(f"\n🏥 API HEALTH & PERFORMANCE:")
            
            # Overall health status
            overall_health = health_monitoring.get('overall_health_status', 'unknown')
            api_reliability = health_monitoring.get('api_reliability_score', 0)
            system_stability = health_monitoring.get('system_stability_score', 0)
            
            self.logger.info(f"  🎯 Overall Health: {overall_health.upper()} ({api_reliability:.1f}%)")
            self.logger.info(f"  🔧 System Stability: {system_stability:.1f}%")
            
            # Create service health table
            active_services = [
                (service_name, service_stats) 
                for service_name, service_stats in api_usage.items() 
                if service_stats.get('total_calls', 0) > 0
            ]
            
            if active_services:
                self.logger.info(f"\n  📡 SERVICE HEALTH STATUS:")
                
                table = PrettyTable()
                table.field_names = ["Service", "Health", "Success Rate", "Avg Response", "Failures"]
                table.align = "l"
                
                for service_name, service_stats in active_services:
                    total_calls = service_stats.get('total_calls', 0)
                    successful_calls = service_stats.get('successful_calls', 0)
                    success_rate = (successful_calls / total_calls) * 100 if total_calls > 0 else 0
                    avg_response_time = service_stats.get('avg_response_time_ms', 0)
                    health_status = service_stats.get('health_status', 'unknown')
                    consecutive_failures = service_stats.get('consecutive_failures', 0)
                    
                    # Format health status with emoji
                    health_emoji = {
                        'excellent': '🟢',
                        'good': '🟡', 
                        'fair': '🟠',
                        'poor': '🔴',
                        'critical': '💀'
                    }.get(health_status, '❓')
                    
                    table.add_row([
                        service_name.title(),
                        f"{health_emoji} {health_status.upper()}",
                        f"{success_rate:.1f}% ({successful_calls}/{total_calls})",
                        f"{avg_response_time:.0f}ms",
                        f"{consecutive_failures}" if consecutive_failures > 0 else "0"
                    ])
                
                # Print complete table as single string
                table_lines = str(table).split('\n')
                for line in table_lines:
                    self.logger.info(f"    {line}")
            else:
                self.logger.info(f"  ℹ️  No active API services to display")
                        
        except Exception as e:
            self.logger.error(f"❌ Error printing API health summary: {e}")
            # Fallback to original method
            self._print_api_health_summary_fallback()
    
    def _print_api_health_summary_fallback(self):
        """Fallback API health summary without prettytable"""
        try:
            api_usage = self.session_stats['api_usage_by_service']
            health_monitoring = self.session_stats['health_monitoring']
            
            self.logger.info(f"\n🏥 API HEALTH & PERFORMANCE:")
            
            # Overall health status
            overall_health = health_monitoring.get('overall_health_status', 'unknown')
            api_reliability = health_monitoring.get('api_reliability_score', 0)
            system_stability = health_monitoring.get('system_stability_score', 0)
            
            self.logger.info(f"  🎯 Overall Health: {overall_health.upper()} ({api_reliability:.1f}%)")
            self.logger.info(f"  🔧 System Stability: {system_stability:.1f}%")
            
            # Service-specific health
            for service_name, service_stats in api_usage.items():
                total_calls = service_stats.get('total_calls', 0)
                if total_calls > 0:
                    success_rate = (service_stats.get('successful_calls', 0) / total_calls) * 100
                    avg_response_time = service_stats.get('avg_response_time_ms', 0)
                    health_status = service_stats.get('health_status', 'unknown')
                    consecutive_failures = service_stats.get('consecutive_failures', 0)
                    
                    self.logger.info(f"  📡 {service_name.title()}:")
                    self.logger.info(f"    • Health: {health_status.upper()}")
                    self.logger.info(f"    • Success Rate: {success_rate:.1f}% ({service_stats.get('successful_calls', 0)}/{total_calls})")
                    self.logger.info(f"    • Avg Response: {avg_response_time:.0f}ms")
                    if consecutive_failures > 0:
                        self.logger.info(f"    • ⚠️  Consecutive Failures: {consecutive_failures}")
                        
        except Exception as e:
            self.logger.error(f"❌ Error printing fallback API health summary: {e}")
    
    def _print_session_statistics_summary(self):
        """Print session-wide statistics summary"""
        try:
            perf_metrics = self.session_stats['performance_metrics']
            cost_analysis = self.session_stats['cost_analysis']
            
            self.logger.info(f"\n📈 SESSION STATISTICS:")
            self.logger.info(f"  🔄 Total Cycles: {perf_metrics['total_cycles']}")
            self.logger.info(f"  ✅ Successful Cycles: {perf_metrics['successful_cycles']}")
            
            if perf_metrics['total_cycles'] > 0:
                success_rate = (perf_metrics['successful_cycles'] / perf_metrics['total_cycles']) * 100
                self.logger.info(f"  📊 Success Rate: {success_rate:.1f}%")
            
            self.logger.info(f"  🎯 Unique Tokens Found: {perf_metrics['unique_tokens']}")
            self.logger.info(f"  🚀 High Conviction Tokens: {perf_metrics['high_conviction_tokens']}")
            self.logger.info(f"  📢 Total Alerts Sent: {perf_metrics['total_alerts_sent']}")
            
            # Cost information
            total_cost = cost_analysis.get('total_estimated_cost_usd', 0)
            if total_cost > 0:
                self.logger.info(f"  💰 Estimated Cost: ${total_cost:.4f}")
                cost_per_token = cost_analysis.get('cost_per_token_discovered', 0)
                if cost_per_token > 0:
                    self.logger.info(f"  💸 Cost per Token: ${cost_per_token:.4f}")
            
            # Token discovery rate
            session_duration = (datetime.now() - self.session_start_time).total_seconds() / 3600  # hours
            if session_duration > 0:
                tokens_per_hour = perf_metrics['unique_tokens'] / session_duration
                self.logger.info(f"  ⏱️  Discovery Rate: {tokens_per_hour:.1f} tokens/hour")
                
        except Exception as e:
            self.logger.error(f"❌ Error printing session statistics summary: {e}")
    
    def _print_performance_alerts(self):
        """Print any performance alerts from health monitoring"""
        try:
            alerts = self.session_stats['health_monitoring'].get('performance_alerts', [])
            
            if alerts:
                self.logger.info(f"\n⚠️  PERFORMANCE ALERTS:")
                for alert in alerts[-3:]:  # Show last 3 alerts
                    alert_type = alert.get('type', 'info').upper()
                    message = alert.get('message', 'No message')
                    recommendation = alert.get('recommendation', 'No recommendation')
                    
                    if alert_type == 'CRITICAL':
                        self.logger.info(f"  🚨 CRITICAL: {message}")
                    elif alert_type == 'WARNING':
                        self.logger.info(f"  ⚠️  WARNING: {message}")
                    else:
                        self.logger.info(f"  ℹ️  INFO: {message}")
                    
                    self.logger.info(f"    💡 Recommendation: {recommendation}")
            
            # Optimization opportunities
            opportunities = self.session_stats['health_monitoring'].get('optimization_opportunities', [])
            if opportunities:
                self.logger.info(f"\n💡 OPTIMIZATION OPPORTUNITIES:")
                for opp in opportunities[-2:]:  # Show last 2 opportunities
                    opp_type = opp.get('type', 'general').upper()
                    message = opp.get('message', 'No message')
                    recommendation = opp.get('recommendation', 'No recommendation')
                    
                    self.logger.info(f"  🔧 {opp_type}: {message}")
                    self.logger.info(f"    💡 {recommendation}")
                    
        except Exception as e:
            self.logger.error(f"❌ Error printing performance alerts: {e}")
    
    def _print_debug_summary(self, cycle_results: Dict[str, Any]):
        """Print enhanced debug information if debug mode is enabled"""
        try:
            debug_analysis = self.session_stats['debug_analysis']
            error_analysis = self.session_stats['error_analysis']
            
            self.logger.info(f"\n🐛 DEBUG INFORMATION:")
            self.logger.info(f"  🔍 API Errors Detected: {debug_analysis.get('api_errors_detected', 0)}")
            self.logger.info(f"  ❌ None Type Errors: {debug_analysis.get('none_type_errors_detected', 0)}")
            self.logger.info(f"  ✅ Successful API Calls: {debug_analysis.get('successful_api_calls', 0)}")
            self.logger.info(f"  🔄 Recovery Events: {len(debug_analysis.get('recovery_events', []))}")
            
            # Recent error patterns
            error_patterns = debug_analysis.get('error_patterns', [])
            if error_patterns:
                self.logger.info(f"  🔍 Recent Error Patterns:")
                for pattern in error_patterns[-2:]:  # Show last 2 patterns
                    self.logger.info(f"    • {pattern}")
            
            # Performance warnings
            perf_warnings = debug_analysis.get('performance_warnings', [])
            if perf_warnings:
                self.logger.info(f"  ⚠️  Performance Warnings:")
                for warning in perf_warnings[-2:]:  # Show last 2 warnings
                    self.logger.info(f"    • {warning}")
                    
        except Exception as e:
            self.logger.error(f"❌ Error printing debug summary: {e}")

    def _update_session_registry_with_detailed_analysis(self, detailed_analysis: Dict[str, Any]):
        """Update session registry with corrected token metadata from detailed analysis"""
        try:
            candidate = detailed_analysis['candidate']
            address = candidate.get('address')
            
            if not address:
                return
                
            # Update tokens_discovered with correct metadata
            if address in self.session_stats['tokens_discovered']:
                token_record = self.session_stats['tokens_discovered'][address]
                
                # Update with corrected symbol and name from Birdeye
                updated_symbol = candidate.get('symbol', 'Unknown')
                updated_name = candidate.get('name', '')
                
                # Only update if we have better data than 'Unknown'
                if updated_symbol != 'Unknown' and updated_symbol != token_record.get('symbol', ''):
                    token_record['symbol'] = updated_symbol
                    self.logger.debug(f"🏷️ Updated symbol for {address}: {updated_symbol}")
                    
                if updated_name and updated_name != token_record.get('name', ''):
                    token_record['name'] = updated_name
                    self.logger.debug(f"🏷️ Updated name for {address}: {updated_name}")
                
                # Update best analysis data
                token_record['best_analysis_data'] = detailed_analysis
                
                # Update conviction score if better
                final_score = detailed_analysis.get('final_score', 0)
                if final_score > token_record.get('best_conviction_score', 0):
                    token_record['best_conviction_score'] = final_score
                    
        except Exception as e:
            self.logger.error(f"❌ Error updating session registry with detailed analysis: {e}")

    # ===== OPTIMIZED FORMATTING METHODS =====
    
    def _colorize(self, text: str, color: str) -> str:
        """Apply ANSI color to text if colors are enabled"""
        if not self.use_colors:
            return text
        return f"{self.colors.get(color, '')}{text}{self.colors['RESET']}"
    
    def _get_status_indicator(self, success_rate: float) -> str:
        """Get colored status indicator based on success rate"""
        if success_rate >= 95:
            return self._colorize("●", "GREEN")
        elif success_rate >= 80:
            return self._colorize("●", "YELLOW")
        else:
            return self._colorize("●", "RED")
    
    def _print_optimized_scan_summary(self, cycle_results: Dict[str, Any]):
        """Print clean, optimized scan summary"""
        try:
            if self.compact_mode:
                self._print_compact_summary(cycle_results)
            else:
                self._print_standard_optimized_summary(cycle_results)
        except Exception as e:
            self.logger.error(f"❌ Error printing optimized summary: {e}")
            # Fallback to existing method
            self._print_comprehensive_scan_summary(cycle_results)
    
    def _print_compact_summary(self, cycle_results: Dict[str, Any]):
        """Ultra-compact summary for production monitoring"""
        try:
            scan_id = cycle_results.get('scan_id', 'unknown')
            duration = cycle_results.get('cycle_duration_seconds', 0)
            new_candidates = cycle_results.get('new_candidates', 0)
            alerts_sent = cycle_results.get('alerts_sent', 0)
            
            # One-line summary
            status = self._colorize("✓", "GREEN") if alerts_sent == 0 else self._colorize("!", "YELLOW")
            self.logger.info(
                f"{status} {self._colorize('SCAN', 'BOLD')} #{self.cycle_count} | "
                f"{duration:.1f}s | "
                f"{new_candidates} tokens | "
                f"{alerts_sent} alerts"
            )
            
            # Only show details if there are findings or issues
            if alerts_sent > 0 or new_candidates > 5:
                self._print_findings_summary(cycle_results)
                
        except Exception as e:
            self.logger.error(f"❌ Error in compact summary: {e}")
    
    def _print_standard_optimized_summary(self, cycle_results: Dict[str, Any]):
        """Standard optimized summary with key metrics"""
        try:
            # Clean header
            scan_id = cycle_results.get('scan_id', 'unknown')
            self.logger.info(f"\n{self._colorize('═' * 60, 'CYAN')}")
            self.logger.info(f"{self._colorize('SCAN SUMMARY', 'BOLD')} #{self.cycle_count} - {scan_id}")
            self.logger.info(f"{self._colorize('═' * 60, 'CYAN')}")
            
            # Key metrics in a clean format
            duration = cycle_results.get('cycle_duration_seconds', 0)
            total_analyzed = cycle_results.get('total_analyzed', 0)
            new_candidates = cycle_results.get('new_candidates', 0)
            alerts_sent = cycle_results.get('alerts_sent', 0)
            
            self.logger.info(f"⏱️  Duration: {self._colorize(f'{duration:.1f}s', 'WHITE')}")
            self.logger.info(f"📊 Analyzed: {self._colorize(str(total_analyzed), 'WHITE')} tokens")
            self.logger.info(f"🆕 Discovered: {self._colorize(str(new_candidates), 'GREEN' if new_candidates > 0 else 'WHITE')} new candidates")
            self.logger.info(f"🚨 Alerts: {self._colorize(str(alerts_sent), 'RED' if alerts_sent > 0 else 'WHITE')}")
            
            # Show token discoveries if any
            if new_candidates > 0:
                self._print_optimized_token_summary()
            
            # Show API health if there are issues
            if self._has_api_issues():
                self._print_optimized_api_status()
            
            self.logger.info(f"{self._colorize('═' * 60, 'CYAN')}\n")
            
        except Exception as e:
            self.logger.error(f"❌ Error in standard optimized summary: {e}")
    
    def _print_findings_summary(self, cycle_results: Dict[str, Any]):
        """Print summary of significant findings"""
        try:
            # Get top tokens from session registry
            unique_tokens = self.session_stats.get('tokens_discovered', {})
            if unique_tokens:
                # Sort by best conviction score
                sorted_tokens = sorted(
                    unique_tokens.items(),
                    key=lambda x: x[1].get('best_conviction_score', 0),
                    reverse=True
                )[:3]  # Top 3
                
                self.logger.info(f"  🏆 Top Tokens:")
                for address, token_data in sorted_tokens:
                    symbol = token_data.get('symbol', 'Unknown')
                    score = token_data.get('best_conviction_score', 0)
                    score_color = 'GREEN' if score >= 70 else 'YELLOW' if score >= 50 else 'WHITE'
                    self.logger.info(f"    • {symbol}: {self._colorize(f'{score:.1f}', score_color)}")
                    
        except Exception as e:
            self.logger.error(f"❌ Error in findings summary: {e}")
    
    def _print_optimized_token_summary(self):
        """Print optimized token discovery summary"""
        try:
            unique_tokens = self.session_stats.get('tokens_discovered', {})
            if not unique_tokens:
                return
                
            high_conviction_count = sum(1 for token in unique_tokens.values() 
                                      if token.get('best_conviction_score', 0) >= self.high_conviction_threshold)
            
            self.logger.info(f"\n{self._colorize('TOKEN DISCOVERIES', 'BOLD')}")
            self.logger.info(f"📊 Total: {len(unique_tokens)} | High Conviction: {self._colorize(str(high_conviction_count), 'GREEN' if high_conviction_count > 0 else 'WHITE')}")
            
            # Show top 3 tokens
            sorted_tokens = sorted(
                unique_tokens.items(),
                key=lambda x: x[1].get('best_conviction_score', 0),
                reverse=True
            )[:3]
            
            for i, (address, token_data) in enumerate(sorted_tokens, 1):
                symbol = token_data.get('symbol', 'Unknown')
                score = token_data.get('best_conviction_score', 0)
                platforms = len(token_data.get('platforms', []))
                
                score_color = 'GREEN' if score >= 70 else 'YELLOW' if score >= 50 else 'WHITE'
                self.logger.info(f"  {i}. {symbol} - {self._colorize(f'{score:.1f}', score_color)} ({platforms} platforms)")
                
        except Exception as e:
            self.logger.error(f"❌ Error in optimized token summary: {e}")
    
    def _print_optimized_api_status(self):
        """Print optimized API status summary"""
        try:
            api_usage = self.session_stats.get('api_usage_by_service', {})
            
            self.logger.info(f"\n{self._colorize('API STATUS', 'BOLD')}")
            
            for service_name, stats in api_usage.items():
                total_calls = stats.get('total_calls', 0)
                if total_calls == 0:
                    continue
                    
                successful_calls = stats.get('successful_calls', 0)
                success_rate = (successful_calls / total_calls) * 100
                avg_response = stats.get('avg_response_time_ms', 0)
                
                status_indicator = self._get_status_indicator(success_rate)
                service_display = service_name.replace('_', ' ').title()
                
                self.logger.info(f"  {status_indicator} {service_display}: {success_rate:.1f}% ({avg_response:.0f}ms)")
                
        except Exception as e:
            self.logger.error(f"❌ Error in optimized API status: {e}")
    
    def _has_api_issues(self) -> bool:
        """Check if there are any API issues worth reporting"""
        try:
            api_usage = self.session_stats.get('api_usage_by_service', {})
            
            for stats in api_usage.values():
                total_calls = stats.get('total_calls', 0)
                if total_calls == 0:
                    continue
                    
                successful_calls = stats.get('successful_calls', 0)
                success_rate = (successful_calls / total_calls) * 100
                
                if success_rate < 95:  # Less than 95% success rate
                    return True
                    
                if stats.get('consecutive_failures', 0) > 0:
                    return True
                    
            return False
            
        except Exception as e:
            self.logger.error(f"❌ Error checking API issues: {e}")
            return False
    
    def set_formatting_mode(self, compact: bool = False, use_colors: bool = True):
        """Set formatting preferences"""
        self.compact_mode = compact
        self.use_colors = use_colors
        
        # Update color codes
        self.colors = {
            'RED': '\033[91m',
            'GREEN': '\033[92m',
            'YELLOW': '\033[93m',
            'BLUE': '\033[94m',
            'MAGENTA': '\033[95m',
            'CYAN': '\033[96m',
            'WHITE': '\033[97m',
            'BOLD': '\033[1m',
            'RESET': '\033[0m'
        } if self.use_colors else {k: '' for k in ['RED', 'GREEN', 'YELLOW', 'BLUE', 'MAGENTA', 'CYAN', 'WHITE', 'BOLD', 'RESET']}

async def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description="High Conviction Token Detector")
    parser.add_argument("--config", default="config/config.yaml", help="Configuration file path")
    parser.add_argument("--interval", type=int, default=15, help="Detection interval in minutes")
    parser.add_argument("--single-run", action="store_true", help="Run once and exit")
    parser.add_argument("--debug", action="store_true", help="Enable enhanced debug mode with comprehensive logging")
    parser.add_argument("--compact", action="store_true", help="Use compact formatting for output")
    parser.add_argument("--no-colors", action="store_true", help="Disable ANSI colors in output")
    parser.add_argument("--verbose", action="store_true", help="Use verbose (original) formatting instead of optimized")
    
    args = parser.parse_args()
    
    detector = None
    try:
        # Initialize detector with debug mode
        detector = HighConvictionTokenDetector(config_path=args.config, debug_mode=args.debug)
        
        # Set formatting preferences
        detector.set_formatting_mode(
            compact=args.compact,
            use_colors=not args.no_colors
        )
        
        # Override to use verbose formatting if requested
        if args.verbose:
            detector.compact_mode = False
            # Replace optimized method with comprehensive method
            detector._print_optimized_scan_summary = detector._print_comprehensive_scan_summary
        
        if args.single_run:
            # Run single detection cycle
            result = await detector.run_detection_cycle()
            print(json.dumps(result, indent=2))
        else:
            # Run continuous detection
            await detector.run_continuous(interval_minutes=args.interval)
            
    except KeyboardInterrupt:
        print("\n🛑 Received stop signal")
    except Exception as e:
        print(f"❌ Fatal error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if detector:
            await detector.cleanup()

if __name__ == "__main__":
    asyncio.run(main()) 