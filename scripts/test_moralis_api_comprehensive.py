#!/usr/bin/env python3
"""
üîç COMPREHENSIVE MORALIS API EVALUATION
Test Moralis API capabilities for Early Gem Detector data requirements

This script evaluates whether Moralis can replace Birdeye/other APIs for:
‚úÖ Token discovery and metadata (87+ data points needed)
‚úÖ Real-time price and market data
‚úÖ Trading analytics and volume tracking
‚úÖ Holder distribution and whale analysis
‚úÖ Liquidity and DEX data
‚úÖ Performance and rate limits
"""

import asyncio
import aiohttp
import logging
import time
import json
import sys
import os
from datetime import datetime
from typing import Dict, List, Any, Optional

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class MoralisAPITester:
    """
    üîç MORALIS API COMPREHENSIVE TESTER
    
    Evaluates Moralis API for Early Gem Detector requirements:
    - Data completeness (87+ data points analysis)
    - Performance vs current APIs (Birdeye, etc.)
    - Solana blockchain support
    - Rate limits and costs
    - Real-time data quality
    """
    
    def __init__(self, api_key: str):
        """Initialize Moralis API tester"""
        self.api_key = api_key
        self.base_url = "https://deep-index.moralis.io/api/v2.2"
        self.logger = self._setup_logger()
        
        # Test configuration
        self.test_tokens = [
            # Test with known Solana tokens
            "So11111111111111111111111111111111111111112",  # Wrapped SOL
            "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",  # USDC
            "Es9vMFrzaCERmJfrF4H2FYD4KCoNkY11McCe8BenwNYB",  # USDT
            # Pump.fun token from user's example
            "9BB6NFEcjBCtnNLFko2FqVQBq8HHM13kCyYcdQbgpump",  # Example pump.fun token
        ]
        
        # Performance tracking
        self.test_results = {
            'api_connectivity': {},
            'data_completeness': {},
            'performance_metrics': {},
            'solana_support': {},
            'comparison_analysis': {},
            'recommendations': {}
        }
        
        # Required data points for Early Gem Detector
        self.required_data_points = {
            'token_metadata': [
                'address', 'symbol', 'name', 'creator_address', 'creation_timestamp',
                'total_supply', 'decimals', 'metadata_uri', 'update_authority'
            ],
            'market_data': [
                'price', 'price_sol', 'market_cap', 'market_cap_sol', 'ath_market_cap',
                'price_change_5m', 'price_change_30m', 'price_change_1h', 'velocity_usd_per_hour'
            ],
            'bonding_curve': [
                'graduation_threshold_usd', 'graduation_progress_pct', 'bonding_curve_stage',
                'sol_in_bonding_curve', 'graduation_eta_hours', 'bonding_curve_velocity'
            ],
            'trading_analytics': [
                'volume_24h', 'volume_1h', 'volume_30m', 'trades_24h', 'trades_1h',
                'unique_traders_24h', 'buy_sell_ratio', 'avg_trade_size_usd', 'trade_frequency_per_minute'
            ],
            'holder_analytics': [
                'total_unique_holders', 'whale_holders_5sol_plus', 'whale_holders_10sol_plus',
                'dev_current_holdings_pct', 'top_10_holders_pct', 'whale_concentration_score',
                'gini_coefficient', 'holders_growth_24h', 'retention_rate_24h'
            ],
            'first_100_buyers': [
                'first_100_retention_pct', 'first_100_holding_time_avg', 'first_100_total_bought_usd',
                'first_100_avg_entry_price', 'diamond_hands_score', 'first_100_still_holding_count'
            ],
            'liquidity_metrics': [
                'liquidity', 'liquidity_to_mcap_ratio', 'liquidity_to_volume_ratio',
                'bid_ask_spread_bps', 'market_depth_1pct', 'liquidity_quality_score'
            ],
            'security_analysis': [
                'dev_tokens_sold', 'dev_usd_realized', 'update_authority',
                'risk_factors', 'security_score'
            ]
        }
        
        self.session = None
        
        self.logger.info("üîç Moralis API Comprehensive Tester initialized")
        self.logger.info(f"   üìä Testing {len(self.test_tokens)} tokens")
        self.logger.info(f"   üéØ Evaluating {sum(len(v) for v in self.required_data_points.values())} data points")
    
    def _setup_logger(self) -> logging.Logger:
        """Setup comprehensive logging"""
        logger = logging.getLogger('MoralisAPITester')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # Console handler
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter(
                '%(asctime)s - üîç MORALIS_TEST - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
            
            # File handler for detailed logs
            file_handler = logging.FileHandler(f'logs/moralis_api_test_{int(time.time())}.log')
            file_formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(
            headers={
                'X-API-Key': self.api_key,
                'Accept': 'application/json',
                'Content-Type': 'application/json'
            },
            timeout=aiohttp.ClientTimeout(total=30)
        )
        
        # Initialize the MoralisAPI connector
        from api.moralis_connector import MoralisAPI
        self.moralis = MoralisAPI(self.api_key, self.logger)
        await self.moralis.__aenter__()
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if hasattr(self, 'moralis') and self.moralis:
            await self.moralis.__aexit__(exc_type, exc_val, exc_tb)
        if self.session:
            await self.session.close()
    
    async def _make_request(self, endpoint: str, params: Optional[Dict] = None) -> Optional[Dict]:
        """Make authenticated request to Moralis API"""
        start_time = time.time()
        
        try:
            url = f"{self.base_url}{endpoint}"
            self.logger.debug(f"üîç Testing endpoint: {endpoint}")
            
            async with self.session.get(url, params=params) as response:
                response_time = time.time() - start_time
                
                if response.status == 200:
                    data = await response.json()
                    self.logger.debug(f"‚úÖ Success: {endpoint} ({response_time:.2f}s)")
                    return {'data': data, 'response_time': response_time, 'status': 200}
                elif response.status == 429:
                    self.logger.warning(f"‚ö†Ô∏è Rate limit: {endpoint}")
                    return {'error': 'rate_limit', 'response_time': response_time, 'status': 429}
                else:
                    error_text = await response.text()
                    self.logger.error(f"‚ùå Error {response.status}: {endpoint} - {error_text}")
                    return {'error': error_text, 'response_time': response_time, 'status': response.status}
                    
        except Exception as e:
            response_time = time.time() - start_time
            self.logger.error(f"‚ùå Request failed: {endpoint} - {e}")
            return {'error': str(e), 'response_time': response_time, 'status': 0}
    
    async def test_api_connectivity(self) -> Dict:
        """Test basic API connectivity and authentication"""
        self.logger.info("üîç Testing Moralis API connectivity...")
        
        connectivity_results = {
            'authentication_valid': False,
            'api_version': None,
            'endpoint_weights': None,
            'supported_chains': [],
            'base_response_time': 0
        }
        
        try:
            # Test API version
            version_result = await self._make_request("/web3/version")
            if version_result and version_result.get('status') == 200:
                connectivity_results['authentication_valid'] = True
                connectivity_results['api_version'] = version_result['data']
                connectivity_results['base_response_time'] = version_result['response_time']
                self.logger.info("‚úÖ Authentication successful")
            else:
                self.logger.error("‚ùå Authentication failed")
                return connectivity_results
            
            # Test endpoint weights
            weights_result = await self._make_request("/info/endpointWeights")
            if weights_result and weights_result.get('status') == 200:
                connectivity_results['endpoint_weights'] = weights_result['data']
                self.logger.info("‚úÖ Endpoint weights retrieved")
            
            self.test_results['api_connectivity'] = connectivity_results
            return connectivity_results
            
        except Exception as e:
            self.logger.error(f"API connectivity test failed: {e}")
            connectivity_results['error'] = str(e)
            return connectivity_results
    
    async def test_solana_support(self) -> Dict:
        """Test Solana blockchain support specifically"""
        self.logger.info("üîç Testing Solana blockchain support...")
        
        solana_results = {
            'chain_supported': False,
            'token_endpoints_work': False,
            'price_endpoints_work': False,
            'transfer_endpoints_work': False,
            'wallet_endpoints_work': False,
            'tested_endpoints': {}
        }
        
        try:
            # Test with wrapped SOL (known token)
            sol_address = "So11111111111111111111111111111111111111112"
            
            # Test CORRECT Solana Gateway endpoints that actually work
            # Use the working pump.fun endpoint via Solana Gateway
            bonding_tokens_result = await self.moralis.get_bonding_tokens_by_exchange('pumpfun', limit=5)
            if bonding_tokens_result:
                self.logger.info("‚úÖ bonding_tokens_pumpfun works for Solana")
                solana_results['token_endpoints_work'] = True
            else:
                self.logger.warning("‚ö†Ô∏è bonding_tokens_pumpfun failed for Solana")
            
            # Test other endpoints (these may still fail)
            endpoints_to_test = {
                'trending_tokens': "/solana/tokens/trending",
                'account_balance': f"/solana/account/{sol_address}/balance",
                'wallet_tokens': f"/solana/account/{sol_address}/tokens"
            }
            
            for endpoint_name, endpoint in endpoints_to_test.items():
                params = {'network': 'mainnet', 'limit': 10}
                result = await self._make_request(endpoint, params)
                
                solana_results['tested_endpoints'][endpoint_name] = {
                    'endpoint': endpoint,
                    'status': result.get('status', 0) if result else 0,
                    'response_time': result.get('response_time', 0) if result else 0,
                    'has_data': bool(result and result.get('data')),
                    'error': result.get('error') if result else 'No response'
                }
                
                if result and result.get('status') == 200:
                    self.logger.info(f"‚úÖ {endpoint_name} works for Solana")
                    if endpoint_name == 'bonding_tokens_pumpfun':
                        solana_results['token_endpoints_work'] = True
                    elif endpoint_name == 'trending_tokens':
                        solana_results['price_endpoints_work'] = True
                    elif endpoint_name == 'account_balance':
                        solana_results['transfer_endpoints_work'] = True
                    elif endpoint_name == 'wallet_tokens':
                        solana_results['wallet_endpoints_work'] = True
                else:
                    self.logger.warning(f"‚ö†Ô∏è {endpoint_name} failed for Solana: {result.get('error') if result else 'No response'}")
            
            # Overall Solana support assessment
            working_endpoints = sum([
                solana_results['token_endpoints_work'],
                solana_results['price_endpoints_work'], 
                solana_results['transfer_endpoints_work'],
                solana_results['wallet_endpoints_work']
            ])
            
            solana_results['chain_supported'] = working_endpoints >= 2
            
            if solana_results['chain_supported']:
                self.logger.info(f"‚úÖ Solana support confirmed ({working_endpoints}/4 endpoint types working)")
            else:
                self.logger.warning(f"‚ö†Ô∏è Limited Solana support ({working_endpoints}/4 endpoint types working)")
            
            self.test_results['solana_support'] = solana_results
            return solana_results
            
        except Exception as e:
            self.logger.error(f"Solana support test failed: {e}")
            solana_results['error'] = str(e)
            return solana_results
    
    async def test_data_completeness(self) -> Dict:
        """Test how many of the required 87+ data points Moralis can provide"""
        self.logger.info("üîç Testing data completeness for Early Gem Detector requirements...")
        
        completeness_results = {
            'total_required_points': sum(len(v) for v in self.required_data_points.values()),
            'available_points': 0,
            'coverage_percentage': 0,
            'category_coverage': {},
            'missing_critical_data': [],
            'data_quality_assessment': {}
        }
        
        try:
            # Test data availability for each category
            for category, required_fields in self.required_data_points.items():
                category_results = {
                    'required_fields': required_fields,
                    'available_fields': [],
                    'partially_available': [],
                    'unavailable_fields': [],
                    'coverage_pct': 0
                }
                
                self.logger.info(f"   üìä Testing {category} ({len(required_fields)} fields)")
                
                # Test with multiple tokens to assess data availability
                field_availability = {field: 0 for field in required_fields}
                
                for token_address in self.test_tokens[:2]:  # Test with 2 tokens
                    token_data = await self._get_comprehensive_token_data(token_address)
                    
                    if token_data:
                        for field in required_fields:
                            if self._field_available_in_data(field, token_data):
                                field_availability[field] += 1
                
                # Classify fields based on availability across test tokens
                for field, availability_count in field_availability.items():
                    if availability_count >= 2:  # Available in both test tokens
                        category_results['available_fields'].append(field)
                    elif availability_count == 1:  # Available in some tokens
                        category_results['partially_available'].append(field)
                    else:  # Not available
                        category_results['unavailable_fields'].append(field)
                
                # Calculate coverage
                available_count = len(category_results['available_fields'])
                total_count = len(required_fields)
                category_results['coverage_pct'] = (available_count / total_count * 100) if total_count > 0 else 0
                
                completeness_results['category_coverage'][category] = category_results
                completeness_results['available_points'] += available_count
                
                self.logger.info(f"     ‚úÖ {available_count}/{total_count} fields available ({category_results['coverage_pct']:.1f}%)")
                
                # Identify critical missing data
                if category in ['token_metadata', 'market_data', 'trading_analytics']:
                    critical_missing = category_results['unavailable_fields']
                    if critical_missing:
                        completeness_results['missing_critical_data'].extend([f"{category}.{field}" for field in critical_missing])
            
            # Calculate overall coverage
            total_required = completeness_results['total_required_points']
            total_available = completeness_results['available_points']
            completeness_results['coverage_percentage'] = (total_available / total_required * 100) if total_required > 0 else 0
            
            self.logger.info(f"üìä Overall data coverage: {total_available}/{total_required} ({completeness_results['coverage_percentage']:.1f}%)")
            
            self.test_results['data_completeness'] = completeness_results
            return completeness_results
            
        except Exception as e:
            self.logger.error(f"Data completeness test failed: {e}")
            completeness_results['error'] = str(e)
            return completeness_results
    
    async def _get_comprehensive_token_data(self, token_address: str) -> Optional[Dict]:
        """Get all available token data from Moralis"""
        token_data = {}
        
        try:
            # Get bonding tokens data (this actually works!)
            bonding_result = await self.moralis.get_bonding_tokens_by_exchange('pumpfun', limit=50)
            if bonding_result:
                # Look for the specific token in the bonding tokens list
                target_token = None
                for token in bonding_result:
                    if token.get('token_address') == token_address:
                        target_token = token
                        break
                
                # If not found, use the first token as a sample
                if not target_token and bonding_result:
                    target_token = bonding_result[0]
                
                if target_token:
                    token_data['bonding_data'] = target_token
            
            # Test other endpoints (may still fail but that's OK)
            endpoints_to_test = {
                'trending_tokens': "/solana/tokens/trending", 
                'top_gainers': "/solana/tokens/top-gainers",
                'account_balance': f"/solana/account/{token_address}/balance"
            }
            
            for data_type, endpoint in endpoints_to_test.items():
                params = {'network': 'mainnet', 'limit': 10}
                result = await self._make_request(endpoint, params)
                
                if result and result.get('status') == 200 and result.get('data'):
                    token_data[data_type] = result['data']
                    
            return token_data if token_data else None
            
        except Exception as e:
            self.logger.error(f"Error getting comprehensive token data: {e}")
            return None
    
    def _field_available_in_data(self, field: str, token_data: Dict) -> bool:
        """Check if a required field is available in the token data"""
        # Map required fields to possible Moralis response fields (based on actual bonding tokens data)
        field_mappings = {
            # Token metadata
            'address': ['address', 'token_address', 'tokenAddress'],
            'symbol': ['symbol'],
            'name': ['name'],
            'creator_address': ['creator', 'deployer', 'owner'],
            'total_supply': ['total_supply', 'totalSupply'],
            'decimals': ['decimals'],
            'logo': ['logo'],
            
            # Market data
            'price': ['usdPrice', 'price', 'usd_price', 'priceUsd'],
            'market_cap': ['marketCap', 'market_cap', 'mc', 'fullyDilutedValuation'],
            'price_change_24h': ['24hrPercentChange', 'price_change_24h'],
            'price_native': ['priceNative', 'price_native'],
            
            # Bonding curve data (pump.fun specific)
            'bonding_curve_progress': ['bondingCurveProgress', 'bonding_curve_progress'],
            'liquidity': ['liquidity'],
            
            # Trading data
            'volume_24h': ['volume24h', 'volume_24h'],
            'transfers_24h': ['transfers24h', 'transfers_24h'],
            
            # Holder data
            'holders_count': ['holdersCount', 'holders_count', 'unique_holders']
        }
        
        possible_fields = field_mappings.get(field, [field])
        
        # Check all data sources for the field
        for data_source in token_data.values():
            if isinstance(data_source, dict):
                for possible_field in possible_fields:
                    if possible_field in data_source and data_source[possible_field] is not None:
                        return True
        
        return False
    
    async def test_performance_metrics(self) -> Dict:
        """Test API performance and rate limits"""
        self.logger.info("üîç Testing performance metrics...")
        
        performance_results = {
            'average_response_time': 0,
            'requests_per_second_limit': 0,
            'rate_limit_behavior': '',
            'concurrent_request_performance': {},
            'large_dataset_performance': {},
            'reliability_score': 0
        }
        
        try:
            # Test response times with multiple requests
            response_times = []
            successful_requests = 0
            
            self.logger.info("   üìä Testing sequential request performance...")
            for i in range(10):
                start_time = time.time()
                result = await self._make_request("/web3/version")
                response_time = time.time() - start_time
                
                if result and result.get('status') == 200:
                    response_times.append(response_time)
                    successful_requests += 1
                    
                await asyncio.sleep(0.1)  # Small delay between requests
            
            if response_times:
                performance_results['average_response_time'] = sum(response_times) / len(response_times)
                performance_results['reliability_score'] = (successful_requests / 10) * 100
                
                self.logger.info(f"     ‚è±Ô∏è Average response time: {performance_results['average_response_time']:.3f}s")
                self.logger.info(f"     ‚úÖ Reliability: {performance_results['reliability_score']:.1f}%")
            
            # Test concurrent requests
            self.logger.info("   üìä Testing concurrent request performance...")
            concurrent_start = time.time()
            
            concurrent_tasks = []
            for i in range(5):
                task = self._make_request("/web3/version")
                concurrent_tasks.append(task)
            
            concurrent_results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)
            concurrent_time = time.time() - concurrent_start
            
            successful_concurrent = sum(1 for r in concurrent_results if isinstance(r, dict) and r.get('status') == 200)
            
            performance_results['concurrent_request_performance'] = {
                'total_time': concurrent_time,
                'successful_requests': successful_concurrent,
                'total_requests': 5,
                'requests_per_second': 5 / concurrent_time if concurrent_time > 0 else 0
            }
            
            self.logger.info(f"     üîÑ Concurrent performance: {successful_concurrent}/5 successful in {concurrent_time:.2f}s")
            
            self.test_results['performance_metrics'] = performance_results
            return performance_results
            
        except Exception as e:
            self.logger.error(f"Performance test failed: {e}")
            performance_results['error'] = str(e)
            return performance_results
    
    async def generate_comparison_analysis(self) -> Dict:
        """Generate comparison analysis vs current APIs (Birdeye, etc.)"""
        self.logger.info("üîç Generating comparison analysis...")
        
        comparison = {
            'data_completeness_vs_birdeye': {},
            'performance_comparison': {},
            'cost_analysis': {},
            'feature_gaps': [],
            'advantages': [],
            'disadvantages': [],
            'migration_feasibility': ''
        }
        
        try:
            # Analyze data completeness
            completeness = self.test_results.get('data_completeness', {})
            coverage_pct = completeness.get('coverage_percentage', 0)
            
            if coverage_pct >= 80:
                comparison['data_completeness_vs_birdeye']['assessment'] = 'EXCELLENT'
                comparison['advantages'].append('High data completeness (>80%)')
            elif coverage_pct >= 60:
                comparison['data_completeness_vs_birdeye']['assessment'] = 'GOOD'
                comparison['advantages'].append('Adequate data completeness')
            else:
                comparison['data_completeness_vs_birdeye']['assessment'] = 'POOR'
                comparison['disadvantages'].append(f'Low data completeness ({coverage_pct:.1f}%)')
            
            # Analyze performance
            performance = self.test_results.get('performance_metrics', {})
            avg_response_time = performance.get('average_response_time', 0)
            
            if avg_response_time < 0.5:
                comparison['performance_comparison']['response_time'] = 'EXCELLENT'
                comparison['advantages'].append('Fast response times (<0.5s)')
            elif avg_response_time < 1.0:
                comparison['performance_comparison']['response_time'] = 'GOOD'
            else:
                comparison['performance_comparison']['response_time'] = 'SLOW'
                comparison['disadvantages'].append(f'Slow response times ({avg_response_time:.2f}s)')
            
            # Identify feature gaps
            missing_critical = completeness.get('missing_critical_data', [])
            for missing_item in missing_critical:
                if any(critical in missing_item for critical in ['bonding_curve', 'first_100', 'whale_', 'velocity']):
                    comparison['feature_gaps'].append(f"Missing critical early gem data: {missing_item}")
            
            # Solana support assessment
            solana_support = self.test_results.get('solana_support', {})
            if solana_support.get('chain_supported'):
                comparison['advantages'].append('Solana blockchain support confirmed')
            else:
                comparison['disadvantages'].append('Limited or no Solana support')
                comparison['feature_gaps'].append('Solana blockchain support')
            
            # Migration feasibility
            if coverage_pct >= 70 and solana_support.get('chain_supported') and avg_response_time < 1.0:
                comparison['migration_feasibility'] = 'FEASIBLE'
            elif coverage_pct >= 50:
                comparison['migration_feasibility'] = 'PARTIAL'
            else:
                comparison['migration_feasibility'] = 'NOT_RECOMMENDED'
            
            self.test_results['comparison_analysis'] = comparison
            return comparison
            
        except Exception as e:
            self.logger.error(f"Comparison analysis failed: {e}")
            comparison['error'] = str(e)
            return comparison
    
    async def generate_recommendations(self) -> Dict:
        """Generate final recommendations"""
        self.logger.info("üîç Generating recommendations...")
        
        recommendations = {
            'overall_recommendation': '',
            'migration_strategy': '',
            'hybrid_approach': '',
            'action_items': [],
            'risks_and_considerations': [],
            'timeline_estimate': ''
        }
        
        try:
            comparison = self.test_results.get('comparison_analysis', {})
            completeness = self.test_results.get('data_completeness', {})
            
            migration_feasibility = comparison.get('migration_feasibility', '')
            coverage_pct = completeness.get('coverage_percentage', 0)
            
            # Generate overall recommendation
            if migration_feasibility == 'FEASIBLE' and coverage_pct >= 80:
                recommendations['overall_recommendation'] = 'RECOMMENDED'
                recommendations['migration_strategy'] = 'Full migration to Moralis API'
                recommendations['timeline_estimate'] = '2-3 weeks'
                recommendations['action_items'] = [
                    'Begin gradual migration starting with token metadata',
                    'Implement Moralis connector parallel to existing APIs',
                    'Run A/B testing for 1 week',
                    'Complete migration and remove old API dependencies'
                ]
                
            elif migration_feasibility == 'PARTIAL' and coverage_pct >= 50:
                recommendations['overall_recommendation'] = 'HYBRID_APPROACH'
                recommendations['hybrid_approach'] = 'Use Moralis for available data, keep existing APIs for gaps'
                recommendations['timeline_estimate'] = '3-4 weeks'
                recommendations['action_items'] = [
                    'Identify which data points Moralis can reliably provide',
                    'Implement hybrid connector using Moralis + existing APIs',
                    'Focus Moralis usage on metadata and basic trading data',
                    'Keep Birdeye/others for specialized early gem metrics'
                ]
                
            else:
                recommendations['overall_recommendation'] = 'NOT_RECOMMENDED'
                recommendations['migration_strategy'] = 'Continue with existing API stack'
                recommendations['action_items'] = [
                    'Monitor Moralis API improvements',
                    'Re-evaluate in 6 months',
                    'Focus on optimizing current Birdeye integration'
                ]
            
            # Identify risks
            if coverage_pct < 70:
                recommendations['risks_and_considerations'].append(
                    'Significant data gaps may impact early gem detection accuracy'
                )
            
            solana_support = self.test_results.get('solana_support', {})
            if not solana_support.get('chain_supported'):
                recommendations['risks_and_considerations'].append(
                    'Limited Solana support may cause reliability issues'
                )
            
            missing_critical = completeness.get('missing_critical_data', [])
            if any('bonding_curve' in item for item in missing_critical):
                recommendations['risks_and_considerations'].append(
                    'Missing bonding curve data critical for pump.fun detection'
                )
            
            self.test_results['recommendations'] = recommendations
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Recommendations generation failed: {e}")
            recommendations['error'] = str(e)
            return recommendations
    
    async def run_comprehensive_test(self) -> Dict:
        """Run complete test suite"""
        self.logger.info("üöÄ Starting comprehensive Moralis API evaluation...")
        self.logger.info("=" * 80)
        
        start_time = time.time()
        
        try:
            # Test 1: API Connectivity
            self.logger.info("üîç PHASE 1: API Connectivity Testing")
            await self.test_api_connectivity()
            
            # Test 2: Solana Support
            self.logger.info("üîç PHASE 2: Solana Blockchain Support")
            await self.test_solana_support()
            
            # Test 3: Data Completeness
            self.logger.info("üîç PHASE 3: Data Completeness Analysis")
            await self.test_data_completeness()
            
            # Test 4: Performance Metrics
            self.logger.info("üîç PHASE 4: Performance Testing")
            await self.test_performance_metrics()
            
            # Test 5: Comparison Analysis
            self.logger.info("üîç PHASE 5: Comparison Analysis")
            await self.generate_comparison_analysis()
            
            # Test 6: Generate Recommendations
            self.logger.info("üîç PHASE 6: Recommendations")
            await self.generate_recommendations()
            
            total_time = time.time() - start_time
            
            # Generate final summary
            self._generate_test_summary(total_time)
            
            return self.test_results
            
        except Exception as e:
            self.logger.error(f"Comprehensive test failed: {e}")
            self.test_results['error'] = str(e)
            return self.test_results
    
    def _generate_test_summary(self, total_time: float):
        """Generate and log test summary"""
        self.logger.info("=" * 80)
        self.logger.info("üìä MORALIS API EVALUATION SUMMARY")
        self.logger.info("=" * 80)
        
        # Connectivity Summary
        connectivity = self.test_results.get('api_connectivity', {})
        if connectivity.get('authentication_valid'):
            self.logger.info("‚úÖ API Authentication: VALID")
        else:
            self.logger.info("‚ùå API Authentication: FAILED")
        
        # Solana Support Summary  
        solana = self.test_results.get('solana_support', {})
        if solana.get('chain_supported'):
            self.logger.info("‚úÖ Solana Support: CONFIRMED")
        else:
            self.logger.info("‚ö†Ô∏è Solana Support: LIMITED")
        
        # Data Completeness Summary
        completeness = self.test_results.get('data_completeness', {})
        coverage_pct = completeness.get('coverage_percentage', 0)
        available_points = completeness.get('available_points', 0)
        total_points = completeness.get('total_required_points', 0)
        
        self.logger.info(f"üìä Data Coverage: {available_points}/{total_points} ({coverage_pct:.1f}%)")
        
        # Performance Summary
        performance = self.test_results.get('performance_metrics', {})
        avg_response_time = performance.get('average_response_time', 0)
        reliability = performance.get('reliability_score', 0)
        
        self.logger.info(f"‚ö° Performance: {avg_response_time:.3f}s avg, {reliability:.1f}% reliable")
        
        # Final Recommendation
        recommendations = self.test_results.get('recommendations', {})
        overall_rec = recommendations.get('overall_recommendation', 'UNKNOWN')
        
        if overall_rec == 'RECOMMENDED':
            self.logger.info("üéØ RECOMMENDATION: ‚úÖ MIGRATE TO MORALIS")
        elif overall_rec == 'HYBRID_APPROACH':
            self.logger.info("üéØ RECOMMENDATION: ‚öñÔ∏è HYBRID APPROACH")
        else:
            self.logger.info("üéØ RECOMMENDATION: ‚ùå KEEP CURRENT APIS")
        
        self.logger.info(f"‚è±Ô∏è Total test time: {total_time:.2f}s")
        self.logger.info("=" * 80)
    
    def save_results_to_file(self, filename: Optional[str] = None):
        """Save test results to JSON file"""
        if not filename:
            timestamp = int(time.time())
            filename = f"results/moralis_api_evaluation_{timestamp}.json"
        
        try:
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            
            with open(filename, 'w') as f:
                json.dump(self.test_results, f, indent=2, default=str)
            
            self.logger.info(f"üìÅ Results saved to: {filename}")
            
        except Exception as e:
            self.logger.error(f"Failed to save results: {e}")


async def main():
    """Main test execution"""
    # Moralis API key provided by user
    MORALIS_API_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJub25jZSI6IjM4NTlhNzQyLTEwNjctNDkyMy05YTU2LWQ5YjQxMGZmYmI5NiIsIm9yZ0lkIjoiNDU0OTY2IiwidXNlcklkIjoiNDY4MTAwIiwidHlwZUlkIjoiNjI1YTg1ZDEtM2Q0OC00YTUxLWEyOWEtMDM5YTU0Zjk2NzkwIiwidHlwZSI6IlBST0pFQ1QiLCJpYXQiOjE3NTA0MjgwMzcsImV4cCI6NDkwNjE4ODAzN30.kXspLCawOP0iaF3NxCnKMvN7prb6X_Na5xXjPRU7Lb4"
    
    print("üîç MORALIS API COMPREHENSIVE EVALUATION")
    print("=" * 60)
    print("   üéØ Testing suitability for Early Gem Detector")
    print("   üìä Evaluating 87+ required data points") 
    print("   ‚ö° Performance vs current APIs (Birdeye, etc.)")
    print("   üíé Solana blockchain support assessment")
    print()
    
    # Ensure directories exist
    os.makedirs('logs', exist_ok=True)
    os.makedirs('results', exist_ok=True)
    
    async with MoralisAPITester(MORALIS_API_KEY) as tester:
        results = await tester.run_comprehensive_test()
        
        # Save results
        tester.save_results_to_file()
        
        print("\nüìÅ Detailed results saved to results/ directory")
        print("üìä Check logs/ directory for comprehensive logs")
        
        return results


if __name__ == "__main__":
    asyncio.run(main()) 