#!/usr/bin/env python3
"""
ğŸš€ PUMP.FUN INTEGRATION TEST SUITE
Tests the complete Stage 0 detection pipeline integration with existing Stage 2 systems
"""

import asyncio
import logging
import time
import json
from datetime import datetime
from typing import Dict, List, Any, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class PumpFunIntegrationTest:
    """Comprehensive test suite for pump.fun Stage 0 integration"""
    
    def __init__(self):
        self.logger = logging.getLogger('PumpFunTest')
        self.test_results = {}
        self.start_time = time.time()
        
        # Test configuration
        self.test_token_addresses = [
            "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",  # USDC (for testing only)
            "So11111111111111111111111111111111111111112",   # WSOL (for testing only)
            "4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R",   # RAY (for testing only)
        ]
        
        self.logger.info("ğŸ§ª Pump.fun Integration Test Suite Initialized")
    
    async def run_comprehensive_test(self):
        """Run complete test suite"""
        print("\n" + "="*80)
        print("ğŸš€ PUMP.FUN STAGE 0 INTEGRATION TEST SUITE")
        print("="*80)
        print("Testing the complete pipeline from launch detection to graduation monitoring...")
        print()
        
        # Test 1: Component Initialization
        await self._test_component_initialization()
        
        # Test 2: Mock Launch Detection
        await self._test_launch_detection()
        
        # Test 3: Stage 0 Priority Processing
        await self._test_stage0_priority_processing()
        
        # Test 4: Integration with Existing Pipeline
        await self._test_existing_pipeline_integration()
        
        # Test 5: Graduation Monitoring
        await self._test_graduation_monitoring()
        
        # Test 6: Wallet Coordination Integration
        await self._test_wallet_coordination()
        
        # Test 7: Performance Validation
        await self._test_performance_metrics()
        
        # Generate final report
        await self._generate_test_report()
    
    async def _test_component_initialization(self):
        """Test 1: Verify all components initialize properly"""
        print("ğŸ§ª Test 1: Component Initialization")
        print("-" * 40)
        
        test_name = "component_initialization"
        self.test_results[test_name] = {"status": "RUNNING", "details": []}
        
        try:
            # Test pump.fun monitor initialization
            print("ğŸ” Testing PumpFunMonitor initialization...")
            try:
                from services.pump_fun_monitor import PumpFunMonitor
                monitor = PumpFunMonitor()
                self.test_results[test_name]["details"].append("âœ… PumpFunMonitor initialized successfully")
                print("   âœ… PumpFunMonitor: OK")
            except Exception as e:
                self.test_results[test_name]["details"].append(f"âŒ PumpFunMonitor failed: {e}")
                print(f"   âŒ PumpFunMonitor: {e}")
            
            # Test integration layer initialization
            print("ğŸ”— Testing PumpFunStage0Integration initialization...")
            try:
                from services.pump_fun_integration import PumpFunStage0Integration
                integration = PumpFunStage0Integration()
                self.test_results[test_name]["details"].append("âœ… Integration layer initialized successfully")
                print("   âœ… Integration Layer: OK")
            except Exception as e:
                self.test_results[test_name]["details"].append(f"âŒ Integration layer failed: {e}")
                print(f"   âŒ Integration Layer: {e}")
            
            # Test early token detection enhancement
            print("ğŸ“Š Testing enhanced EarlyTokenDetector...")
            try:
                from services.early_token_detection import EarlyTokenDetector
                detector = EarlyTokenDetector()
                
                # Check if pump.fun methods exist
                if hasattr(detector, '_process_stage0_priority_tokens'):
                    self.test_results[test_name]["details"].append("âœ… Stage 0 methods present")
                    print("   âœ… Stage 0 Methods: Present")
                else:
                    self.test_results[test_name]["details"].append("âš ï¸ Stage 0 methods missing")
                    print("   âš ï¸ Stage 0 Methods: Missing (will use fallback)")
                
            except Exception as e:
                self.test_results[test_name]["details"].append(f"âŒ EarlyTokenDetector failed: {e}")
                print(f"   âŒ EarlyTokenDetector: {e}")
            
            self.test_results[test_name]["status"] = "PASSED"
            print("âœ… Test 1 PASSED: All components initialized\n")
            
        except Exception as e:
            self.test_results[test_name]["status"] = "FAILED"
            self.test_results[test_name]["details"].append(f"Critical failure: {e}")
            print(f"âŒ Test 1 FAILED: {e}\n")
    
    async def _test_launch_detection(self):
        """Test 2: Mock pump.fun launch detection"""
        print("ğŸ§ª Test 2: Launch Detection")
        print("-" * 40)
        
        test_name = "launch_detection"
        self.test_results[test_name] = {"status": "RUNNING", "details": []}
        
        try:
            from services.pump_fun_monitor import PumpFunMonitor
            from services.pump_fun_integration import PumpFunStage0Integration
            
            # Initialize components
            monitor = PumpFunMonitor()
            integration = PumpFunStage0Integration()
            
            # Mock launch event data
            mock_launch_events = [
                {
                    'token_address': 'MOCK1234567890abcdef',
                    'type': 'pump_fun_launch',
                    'stage': 'stage_0',
                    'priority': 'ultra_high',
                    'timestamp': time.time(),
                    'symbol': 'TESTTOKEN1',
                    'estimated_age_minutes': 0
                },
                {
                    'token_address': 'MOCK0987654321fedcba',
                    'type': 'pump_fun_launch',
                    'stage': 'stage_0',
                    'priority': 'ultra_high',
                    'timestamp': time.time(),
                    'symbol': 'TESTTOKEN2',
                    'estimated_age_minutes': 0
                }
            ]
            
            print(f"ğŸ”¥ Testing {len(mock_launch_events)} mock launch events...")
            
            # Test launch event processing
            launches_processed = 0
            for event in mock_launch_events:
                try:
                    await integration.handle_pump_fun_launch(event)
                    launches_processed += 1
                    print(f"   âœ… Processed launch: {event['token_address'][:12]}...")
                    
                except Exception as e:
                    print(f"   âŒ Failed launch: {event['token_address'][:12]}... - {e}")
            
            # Validate priority queue
            priority_queue = integration.get_stage0_priority_queue()
            queue_size = len(priority_queue)
            
            print(f"ğŸ“‹ Priority queue size: {queue_size}")
            
            if queue_size == launches_processed:
                self.test_results[test_name]["details"].append(f"âœ… All {launches_processed} launches added to priority queue")
                print(f"   âœ… Priority queue correct: {queue_size} items")
            else:
                self.test_results[test_name]["details"].append(f"âš ï¸ Priority queue mismatch: {queue_size} vs {launches_processed}")
                print(f"   âš ï¸ Priority queue mismatch: {queue_size} vs {launches_processed}")
            
            # Test integration stats
            stats = integration.get_integration_stats()
            print(f"ğŸ“Š Integration stats: {stats}")
            
            self.test_results[test_name]["status"] = "PASSED"
            self.test_results[test_name]["details"].append(f"Successfully processed {launches_processed} mock launches")
            print("âœ… Test 2 PASSED: Launch detection working\n")
            
        except Exception as e:
            self.test_results[test_name]["status"] = "FAILED"
            self.test_results[test_name]["details"].append(f"Launch detection failed: {e}")
            print(f"âŒ Test 2 FAILED: {e}\n")
    
    async def _test_stage0_priority_processing(self):
        """Test 3: Stage 0 priority processing and scoring"""
        print("ğŸ§ª Test 3: Stage 0 Priority Processing")
        print("-" * 40)
        
        test_name = "stage0_priority"
        self.test_results[test_name] = {"status": "RUNNING", "details": []}
        
        try:
            from services.pump_fun_integration import PumpFunStage0Integration
            
            integration = PumpFunStage0Integration()
            
            # Create Stage 0 test token
            stage0_token = {
                'address': 'STAGE0TESTTOKEN123',
                'symbol': 'S0TEST',
                'source': 'pump_fun_launch',
                'stage': 'stage_0',
                'priority_score': 100,
                'pump_fun_bonus': 25,
                'estimated_age_minutes': 0,
                'launch_detected_at': datetime.now().isoformat()
            }
            
            print("ğŸ¯ Testing Stage 0 scoring calculation...")
            
            # Test Stage 0 scoring
            enhanced_analysis = await integration._calculate_stage0_scoring(stage0_token)
            
            print(f"   ğŸ“Š Stage 0 Score: {enhanced_analysis.get('stage0_score', 0)}")
            print(f"   ğŸ† Bonuses Applied: {enhanced_analysis.get('bonuses_applied', {})}")
            print(f"   ğŸš€ Recommended Action: {enhanced_analysis.get('recommended_action', 'UNKNOWN')}")
            
            # Validate scoring
            expected_min_score = 100  # High score for Stage 0
            actual_score = enhanced_analysis.get('stage0_score', 0)
            
            if actual_score >= expected_min_score:
                self.test_results[test_name]["details"].append(f"âœ… Stage 0 scoring correct: {actual_score}")
                print(f"   âœ… Scoring validation passed: {actual_score} >= {expected_min_score}")
            else:
                self.test_results[test_name]["details"].append(f"âš ï¸ Stage 0 scoring low: {actual_score}")
                print(f"   âš ï¸ Scoring validation warning: {actual_score} < {expected_min_score}")
            
            # Test processing priority
            if enhanced_analysis.get('recommended_action') == 'IMMEDIATE_ANALYSIS':
                self.test_results[test_name]["details"].append("âœ… Stage 0 triggers immediate analysis")
                print("   âœ… Priority processing: IMMEDIATE_ANALYSIS triggered")
            else:
                self.test_results[test_name]["details"].append("âš ï¸ Stage 0 doesn't trigger immediate analysis")
                print("   âš ï¸ Priority processing: IMMEDIATE_ANALYSIS not triggered")
            
            self.test_results[test_name]["status"] = "PASSED"
            print("âœ… Test 3 PASSED: Stage 0 priority processing working\n")
            
        except Exception as e:
            self.test_results[test_name]["status"] = "FAILED"
            self.test_results[test_name]["details"].append(f"Stage 0 processing failed: {e}")
            print(f"âŒ Test 3 FAILED: {e}\n")
    
    async def _test_existing_pipeline_integration(self):
        """Test 4: Integration with existing Early Token Detection pipeline"""
        print("ğŸ§ª Test 4: Existing Pipeline Integration")
        print("-" * 40)
        
        test_name = "pipeline_integration"
        self.test_results[test_name] = {"status": "RUNNING", "details": []}
        
        try:
            from services.early_token_detection import EarlyTokenDetector
            from services.pump_fun_integration import PumpFunStage0Integration
            
            # Initialize components
            detector = EarlyTokenDetector()
            integration = PumpFunStage0Integration(early_detector=detector)
            
            print("ğŸ”— Testing integration with existing pipeline...")
            
            # Test Stage 0 token feeding
            stage0_enhanced_token = {
                'token_address': 'PIPELINE_TEST_TOKEN',
                'stage0_score': 125,
                'bonuses_applied': {
                    'pump_fun_launch_bonus': 25,
                    'stage0_priority_bonus': 15,
                    'ultra_early_bonus': 10
                },
                'recommended_action': 'IMMEDIATE_ANALYSIS',
                'stage': 'stage_0_enhanced',
                'processing_priority': 'ULTRA_HIGH',
                'symbol': 'PIPTEST',
                'pump_fun_origin': True
            }
            
            # Test feeding to pipeline
            await integration._feed_to_existing_pipeline(stage0_enhanced_token)
            
            # Check if early detector has the method
            if hasattr(detector, '_process_stage0_priority_tokens'):
                print("   âœ… Early detector has Stage 0 processing methods")
                self.test_results[test_name]["details"].append("âœ… Stage 0 methods present in early detector")
                
                # Test Stage 0 priority processing
                stage0_tokens = await detector._process_stage0_priority_tokens()
                print(f"   ğŸ“‹ Stage 0 tokens processed: {len(stage0_tokens)}")
                
            else:
                print("   âš ï¸ Early detector missing Stage 0 methods (using fallback)")
                self.test_results[test_name]["details"].append("âš ï¸ Stage 0 methods missing (fallback mode)")
            
            # Test discovery pipeline enhancement
            print("ğŸš€ Testing enhanced discovery pipeline...")
            
            # Mock discovery run with Stage 0 tokens
            try:
                # This would normally run the full pipeline
                # For testing, we'll just validate the structure
                discovered_tokens = await detector._discover_and_analyze(max_tokens=10)
                
                if discovered_tokens is not None:
                    print(f"   âœ… Discovery pipeline runs successfully: {len(discovered_tokens)} tokens")
                    self.test_results[test_name]["details"].append(f"âœ… Pipeline processes {len(discovered_tokens)} tokens")
                else:
                    print("   âš ï¸ Discovery pipeline returned None")
                    self.test_results[test_name]["details"].append("âš ï¸ Pipeline returned None")
                    
            except Exception as e:
                print(f"   âš ï¸ Discovery pipeline test skipped: {e}")
                self.test_results[test_name]["details"].append(f"âš ï¸ Pipeline test skipped: {e}")
            
            self.test_results[test_name]["status"] = "PASSED"
            print("âœ… Test 4 PASSED: Pipeline integration working\n")
            
        except Exception as e:
            self.test_results[test_name]["status"] = "FAILED"
            self.test_results[test_name]["details"].append(f"Pipeline integration failed: {e}")
            print(f"âŒ Test 4 FAILED: {e}\n")
    
    async def _test_graduation_monitoring(self):
        """Test 5: Graduation monitoring and exit signals"""
        print("ğŸ§ª Test 5: Graduation Monitoring")
        print("-" * 40)
        
        test_name = "graduation_monitoring"
        self.test_results[test_name] = {"status": "RUNNING", "details": []}
        
        try:
            from services.pump_fun_integration import PumpFunStage0Integration
            
            integration = PumpFunStage0Integration()
            
            # Mock graduation event
            graduation_event = {
                'token_address': 'GRADUATION_TEST_TOKEN',
                'type': 'graduation_exit_signal',
                'action': 'TAKE_PROFITS',
                'urgency': 'HIGH',
                'stage': 'graduation_complete',
                'graduation_source': 'raydium_pool_creation',
                'tracking_duration_hours': 48.5,
                'market_cap': 75000,  # Above $69K graduation threshold
                'timestamp': time.time()
            }
            
            print("ğŸ“ Testing graduation signal processing...")
            
            # Test graduation signal handling
            await integration.handle_graduation_signal(graduation_event)
            
            # Validate graduation signal was processed
            stats = integration.get_integration_stats()
            graduation_signals = stats.get('graduation_signals_sent', 0)
            
            if graduation_signals > 0:
                print(f"   âœ… Graduation signal processed: {graduation_signals} signals sent")
                self.test_results[test_name]["details"].append(f"âœ… {graduation_signals} graduation signals processed")
            else:
                print("   âš ï¸ No graduation signals detected")
                self.test_results[test_name]["details"].append("âš ï¸ No graduation signals processed")
            
            # Test graduation exit signal structure
            expected_fields = ['token_address', 'action', 'urgency', 'recommended_exit_percentage']
            graduation_signal_valid = all(field in graduation_event for field in expected_fields)
            
            if graduation_signal_valid:
                print("   âœ… Graduation signal structure valid")
                self.test_results[test_name]["details"].append("âœ… Exit signal structure valid")
            else:
                print("   âŒ Graduation signal structure invalid")
                self.test_results[test_name]["details"].append("âŒ Exit signal structure invalid")
            
            self.test_results[test_name]["status"] = "PASSED"
            print("âœ… Test 5 PASSED: Graduation monitoring working\n")
            
        except Exception as e:
            self.test_results[test_name]["status"] = "FAILED"
            self.test_results[test_name]["details"].append(f"Graduation monitoring failed: {e}")
            print(f"âŒ Test 5 FAILED: {e}\n")
    
    async def _test_wallet_coordination(self):
        """Test 6: Wallet coordination integration"""
        print("ğŸ§ª Test 6: Wallet Coordination Integration")
        print("-" * 40)
        
        test_name = "wallet_coordination"
        self.test_results[test_name] = {"status": "RUNNING", "details": []}
        
        try:
            from services.pump_fun_integration import DiscoveryScoutPumpFunHandler
            
            # Test Discovery Scout handler
            handler = DiscoveryScoutPumpFunHandler()
            
            print("ğŸ¤– Testing Discovery Scout pump.fun handler...")
            
            # Mock Stage 0 token for auto-trading
            stage0_token = {
                'token_address': 'WALLET_TEST_TOKEN',
                'stage': 'stage_0',
                'priority': 'ULTRA_HIGH',
                'estimated_age_minutes': 2,
                'pump_fun_bonus': 25,
                'stage0_priority': True
            }
            
            # Test auto-trading score calculation
            auto_trade_score = await handler._calculate_auto_trade_score(stage0_token)
            
            print(f"   ğŸ“Š Auto-trade score: {auto_trade_score}")
            
            if auto_trade_score >= 85:
                print("   âœ… Stage 0 token qualifies for auto-trading")
                self.test_results[test_name]["details"].append(f"âœ… Auto-trade qualification: {auto_trade_score}")
            else:
                print(f"   âš ï¸ Stage 0 token below auto-trade threshold: {auto_trade_score}")
                self.test_results[test_name]["details"].append(f"âš ï¸ Auto-trade score low: {auto_trade_score}")
            
            # Test wallet coordination parameters
            print(f"   ğŸ’° Position size: {handler.pump_fun_position_size * 100}%")
            print(f"   ğŸ¯ Profit target: {handler.pump_fun_auto_take_profit}x")
            print(f"   ğŸ›‘ Stop loss: {handler.pump_fun_stop_loss * 100}%")
            
            self.test_results[test_name]["details"].append("âœ… Wallet coordination handler initialized")
            
            self.test_results[test_name]["status"] = "PASSED"
            print("âœ… Test 6 PASSED: Wallet coordination ready\n")
            
        except Exception as e:
            self.test_results[test_name]["status"] = "FAILED"
            self.test_results[test_name]["details"].append(f"Wallet coordination failed: {e}")
            print(f"âŒ Test 6 FAILED: {e}\n")
    
    async def _test_performance_metrics(self):
        """Test 7: Performance metrics and monitoring"""
        print("ğŸ§ª Test 7: Performance Metrics")
        print("-" * 40)
        
        test_name = "performance_metrics"
        self.test_results[test_name] = {"status": "RUNNING", "details": []}
        
        try:
            from services.pump_fun_monitor import PumpFunMonitor
            from services.pump_fun_integration import PumpFunStage0Integration
            
            # Initialize components
            monitor = PumpFunMonitor()
            integration = PumpFunStage0Integration()
            
            print("ğŸ“Š Testing performance metrics collection...")
            
            # Test monitor statistics
            monitor_stats = monitor.get_monitoring_stats()
            print(f"   ğŸ” Monitor stats: {monitor_stats}")
            
            # Test integration statistics
            integration_stats = integration.get_integration_stats()
            print(f"   ğŸ”— Integration stats: {integration_stats}")
            
            # Validate required metrics exist
            required_monitor_metrics = ['tokens_detected', 'graduations_detected', 'detection_rate']
            required_integration_metrics = ['stage0_tokens_processed', 'graduation_signals_sent', 'integration_status']
            
            monitor_metrics_valid = all(metric in monitor_stats for metric in required_monitor_metrics)
            integration_metrics_valid = all(metric in integration_stats for metric in required_integration_metrics)
            
            if monitor_metrics_valid:
                print("   âœ… Monitor metrics complete")
                self.test_results[test_name]["details"].append("âœ… Monitor metrics validated")
            else:
                print("   âŒ Monitor metrics incomplete")
                self.test_results[test_name]["details"].append("âŒ Monitor metrics missing")
            
            if integration_metrics_valid:
                print("   âœ… Integration metrics complete")
                self.test_results[test_name]["details"].append("âœ… Integration metrics validated")
            else:
                print("   âŒ Integration metrics incomplete")
                self.test_results[test_name]["details"].append("âŒ Integration metrics missing")
            
            # Test performance calculations
            total_runtime = time.time() - self.start_time
            print(f"   â±ï¸ Test runtime: {total_runtime:.2f} seconds")
            
            self.test_results[test_name]["status"] = "PASSED"
            print("âœ… Test 7 PASSED: Performance metrics working\n")
            
        except Exception as e:
            self.test_results[test_name]["status"] = "FAILED"
            self.test_results[test_name]["details"].append(f"Performance metrics failed: {e}")
            print(f"âŒ Test 7 FAILED: {e}\n")
    
    async def _generate_test_report(self):
        """Generate comprehensive test report"""
        print("="*80)
        print("ğŸ“‹ PUMP.FUN INTEGRATION TEST REPORT")
        print("="*80)
        
        # Calculate overall results
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result['status'] == 'PASSED')
        failed_tests = sum(1 for result in self.test_results.values() if result['status'] == 'FAILED')
        
        total_runtime = time.time() - self.start_time
        
        print(f"ğŸ•’ Test Duration: {total_runtime:.2f} seconds")
        print(f"ğŸ“Š Total Tests: {total_tests}")
        print(f"âœ… Passed: {passed_tests}")
        print(f"âŒ Failed: {failed_tests}")
        print(f"ğŸ“ˆ Success Rate: {(passed_tests/total_tests)*100:.1f}%")
        print()
        
        # Detailed results
        for test_name, result in self.test_results.items():
            status_emoji = "âœ…" if result['status'] == 'PASSED' else "âŒ"
            print(f"{status_emoji} {test_name.replace('_', ' ').title()}: {result['status']}")
            
            for detail in result['details']:
                print(f"   {detail}")
            print()
        
        # Integration readiness assessment
        print("ğŸš€ INTEGRATION READINESS ASSESSMENT")
        print("-" * 50)
        
        if passed_tests == total_tests:
            print("ğŸ‰ EXCELLENT: All tests passed - Integration ready for deployment!")
            print("âœ… Stage 0 detection pipeline is fully operational")
            print("âœ… Graduation monitoring is working correctly")
            print("âœ… Wallet coordination is ready")
        elif passed_tests >= total_tests * 0.8:
            print("ğŸ‘ GOOD: Most tests passed - Integration mostly ready")
            print("âš ï¸ Review failed tests before full deployment")
        else:
            print("âš ï¸ NEEDS WORK: Multiple test failures detected")
            print("âŒ Address issues before deployment")
        
        print()
        print("ğŸ¯ NEXT STEPS:")
        print("1. Review any failed tests and fix issues")
        print("2. Configure pump.fun monitoring endpoints")
        print("3. Set up WebSocket connections for real-time monitoring")
        print("4. Integrate with Discovery Scout wallet for automation")
        print("5. Deploy Stage 0 detection alongside existing Stage 2 system")
        
        # Save detailed report
        report_data = {
            'test_timestamp': datetime.now().isoformat(),
            'test_duration_seconds': total_runtime,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': failed_tests,
            'success_rate': (passed_tests/total_tests)*100,
            'detailed_results': self.test_results,
            'integration_status': 'READY' if passed_tests == total_tests else 'NEEDS_REVIEW'
        }
        
        report_filename = f"pump_fun_integration_test_report_{int(time.time())}.json"
        with open(report_filename, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)
        
        print(f"\nğŸ“„ Detailed report saved: {report_filename}")
        print("="*80)

async def main():
    """Run the pump.fun integration test suite"""
    print("ğŸš€ Starting Pump.fun Integration Test Suite...")
    
    # Initialize and run tests
    test_suite = PumpFunIntegrationTest()
    await test_suite.run_comprehensive_test()
    
    print("\nğŸ‰ Test suite completed!")

if __name__ == "__main__":
    asyncio.run(main()) 